\documentclass[11pt]{article}%
\usepackage{geometry}%
\geometry{a4paper,
  lmargin=2cm,rmargin=2cm,tmargin=2.5cm,bmargin=2.5cm}

\input{../../../../macros.tex}

\pagestyle{fancy} %
\lhead{ECE2 \hfill % 15 mai 2017 \\
  Mathématiques\\} %
\chead{\hrule} %
\rhead{} %
\lfoot{} %
\cfoot{} %
\rfoot{\thepage} %

\renewcommand{\headrulewidth}{0pt}% : Trace un trait de séparation
                                  % de largeur 0,4 point. Mettre 0pt
                                  % pour supprimer le trait.

\renewcommand{\footrulewidth}{0.4pt}% : Trace un trait de séparation
                                    % de largeur 0,4 point. Mettre 0pt
                                    % pour supprimer le trait.

\setlength{\headheight}{14pt}

\title{Oraux HEC - 2007 - 2016} %
\author{} %
\date{} %
\begin{document}

\maketitle %
\vspace{-1.6cm}\hrule %
\thispagestyle{fancy}

\vspace*{1cm}

% \noindent
% {\Large \bf Chaque exercice sera rédigé sur des copies séparées.}

\section{Annales 2014}

% \setcounter{exercice}{0}
\begin{exercice}{\it (Exercice avec préparation)}~
  \begin{noliste}{1.}
  \item Deux variables aléatoires discrètes $X$ et $y$ sont
    indépendantes si pour tut $i \in X ( \Omega)$ et tout $j \in Y (
    \Omega )$,
    \[
    P [ \Ev{X = i} \cap \Ev{Y= j } ] = P \Ev{ X = i } P \Ev{ Y = j } .
    \]    
    De plus si $X$ et $Y$ sont indépendantes, leur covariance est
    nulle, mais la réciproque est fausse. \\
    
    Soit $X$ et $Y$ deux variables aléatoires discrètes finies à
    valeurs dans $\N$, définies sur un espace probabilisé $(\Omega ,
    \mathcal{A} , P)$. On suppose que $ X ( \Omega ) \subset \llb 0 ;
    n \rrb$ et $Y ( \Omega ) \subset \llb 0 ; m \rrb$, où $n$ et $m$
    sont deux entiers de $\N^*$. \\
    
    Pour tout couple $(i,j) \in \llb 0 ; n \rrb \times \llb 0 ; m
    \rrb$, on pose : $p_{i,j} = P \big( \Ev{X=i} \cap \Ev{Y=j} \big)$. \\ 

    Soit $F_X$ et $F_Y$ les deux fonctions de $\R$ dans $\R$ définies
    par : $F_X ( x) = \Sum{i=0}{n} \Prob(\Ev{X=i}) x^i $ et $F_Y (x) =
    \Sum{j=0}{m} \Prob(\Ev{Y=l}) x^j$. \\ 

    Soit $Z = (X,Y)$ et $G_Z$ la fonction de $\R^2$ dans $\R$ définie
    par : $G_Z ( x , y ) = \Sum{i=0}{n} \Sum{j=0}{m} p_{i,j} x^i y^j$.
    
  \item $G_Z (1 , 1 ) = \Sum{i=0}{n} \Sum{j=0}{m} p_{i,j} = 1 $ avec
    le système complet d'évènement associé au couple $(X,Y)$. \\ 
    
    De plus les dérivées partielles de $G$ sont : 
    \[
    \partial_1 (G ) (x,y) = \Sum{i=0}{n} \Sum{j=0}{m} i p_{i,j}
    x^{i-1} y^j \ \ , \ \ \partial_2 (G ) (x,y) = \Sum{i=0}{n}
    \Sum{j=0}{m} j p_{i,j} x^i y^{j-1}
    \]
    puis
    \[
    \partial_{1,1}^2 (G ) (x,y) = \Sum{i=0}{n} \Sum{j=0}{m} i (i-1)
    p_{i,j} x^{i-2} y^j \ \ , \ \ \partial_{2,2}^2 (G ) (x,y) =
    \Sum{i=0}{n} \Sum{j=0}{m} j (j-1) p_{i,j} x^i y^{j-2}
    \]
    et
    \[
    \partial_{1,2}^2 (G ) (x,y) = \Sum{i=0}{n} \Sum{j=0}{m} i j
    p_{i,j} x^{i-1} y^{j-1}
    \]
    et au point (1,1), en reconnaissant le théorème de transfert à
    deux variables, on obtient :
    \[
    \partial_1 (G ) (1,1) = \Sum{i=0}{n} \Sum{j=0}{m} i p_{i,j} = E (
    X ) \ \ , \ \ \partial_2 (G ) (1,1) = \Sum{i=0}{n} \Sum{j=0}{m} j
    p_{i,j} = E ( Y )
    \]
    et
    \[
    \partial_{1,2}^2 (G ) (1,1) = \Sum{i=0}{n} \Sum{j=0}{m} i j
    p_{i,j} = E ( X Y )
    \]
    et enfin : 
    \[
    \Cov ( X , Y ) = E ( X Y ) - E ( X ) E ( Y ) = \partial_{1,2}^2
    (G ) (1,1) - \partial_1 (G ) (1,1) \partial_2 (G ) (1,1) .
    \]
    
  \item Soit $f$ une fonction polynômiale de deux variables définies
    sur $\R^2$ par : $f(x,y) = \Sum{i=0}{n} \Sum{j=0}{m} a_{i,j} x^i
    y^j$ avec $a_{i,j} \in \R$. \\
    On suppose que pour tout couple $(x,y) \in \R^2$, on a $f(x,y) =
    0$.
    \begin{noliste}{a)}
    \item Question difficile. Il faut penser à faire apparaître des
      polynômes un une variable. \\
      
      On fixe $y \in \R$, on sait que
      \[
      f ( x,  y) = \Sum{i=0}{n} \left( \Sum{j=0}{m} a_{i,j} y^j \right) x^i 
      \]
      qui est un polynôme nul en tout $x$, donc chacun de ses
      coefficients sont nuls. Donc pour tout $i \in \llb 0 ; n \rrb $
      et pour tout $y \in \R$, on sait que :
      \[
      \Sum{j=0}{m} a_{i,j} y^j = 0 
      \]
      et puisque c'est un polynôme, chaque coefficient est nul donc :
      \[
      \forall i \in \llb 0 ; n \rrb , \forall j \in \llb 0 ; m \rrb ,
      \ a_{i,j} = 0
      \]
      
    \item Si $X$ et $Y$ sont indépendantes, $p_{i,j} = p_i pj$ pour
      tout $(i,j)$ donc les deux fonctions sont égales. Si les deux
      fonctions sont égales, la fonction
      \[
      f(x) = G_Z ( x,y) - F_X ( x) F_Y (y) = \Sum{i=0}{n}
      \Sum{j=0}{m} (p_{i,j} - p_i p_j ) x^i y^j
      \]
      est nulle pour tout $(x,y)$, donc d'après la question a, pour
      tout $(i,j)$ on a :
      \[
      p_{i,j} - p_i p_j = 0 \Longleftrightarrow p_{i,j} = p_i p_j 
      \]
      et les variables sont indépendantes. 
    \end{noliste}
     
  \item Une urne contient des jetons portant chacun une des lettres
    $A$, $B$ ou $C$. La proportion des jetons portant la lettre $A$
    est $p$, celle des jetons portant la lettre $B$ est $q$, et celle
    des jetons portant la lettre $C$ est $r$, où $p$, $q$ et $r$ sont
    trois réels strictement positifs vérifiant $p+q+r=1$. \\
    Soit $n \in \N^*$. On effectue $n$ tirages d'un jeton avec remise
    dans cette urne. On note $X$ (resp. $Y$) la variable aléatoire
    égale au nombre de jetons tirés portant la lettre $A$ (resp. $B$)
    à l'issue de ces $n$ tirages.
     
    \begin{noliste}{a)}
    \item $x$ et $Y$ suivent des lois binomiales de paramètre $n$ et
      $p$ (resp. $q$). On calcule alors :
      \[
      F_X ( x) = \Sum{i=0}{n} \binom{n}{i} p^i (1-p)^{n-i} x^i =
      \Sum{i=0}{n} \binom{n}{i} (px)^i (1-p)^{n-i} = (px + 1 - p)^n =
      (px + q + r)^n
      \]
      et par symétrie des rôles de $(X,p)$ et $(Y, q)$ :
      \[
      F_Y (y) = ( q y+p + r )^n . 
      \]
      
    \item Les supports de $X$ et $Y$ sont déjà connus, et $[ \Ev{X=i}
      \cap \Ev{Y=j} ]$ signifie qu'on a obtenu $i$ jetons $A$, avec
      $i$ places à choisir parmi $n$, $j$ jetons $B$ avec $j$ places à
      choisir parmi $n-i$ restantes, et enfin $(n-i-j)$ jetons $C$
      dont les places sont les places sont imposées par les choix
      précédents donc :
      \[
      p_{i,j} = \binom{n}{i} \binom{n-i}{j} p^i q^j r^{n-i-j} =
      \frac{ n! }{ i! j! (n-i-j)! } p^i q^j r^{n-i-j} .
      \]
      Ceci est valable tant que $i + j \leq n$, et $p_{i,j}$ est nul
      dès que $i + j > n$ donc $j > n-i$ car l'évènement est alors
      impossible. On obtient alors
      \begin{eqnarray*}
        G_Z ( x,y) & = & \Sum{i=0}{n} \Sum{j=0}{n-j} \binom{n}{i}
        \binom{n-i}{j} p^i q^j r^{n-i-j} x^i y^j \\ \\ 
        & = & \Sum{i=0}{n} \left( \binom{n}{i} ( p x )^i
          \Sum{j=0}{n-i}  \binom{n-i}{j}  (q y )^j r^{n-i-j} \right)
        \\ \\ 
        & = & \Sum{i=0}{n} \binom{n}{i} ( p x )^i ( r + q y )^{n-i} \\ \\
        & = & ( p x + q y + r )^n 
      \end{eqnarray*}

    \item On calcule le produit de $F_X$ et $F_Y$ : 
      \[
      F_X ( x) F_Y (y) = \left( \rule{0cm}{0.4cm} (px + q + r) (qy + p
        + r ) \right)^n = \left( p q x y + p^2 x + p r x + q^2 y + p q
        + q r + q r y + p q + r^2 \rule{0cm}{0.4cm} \right)^n
      \]
      
      Si $F_X F_Y = G_Z$ pour tout $(x,y)$ alors en composant par la
      racine $n$-ème on obtiendrait pour tout $(x,y) \geq 0$ :
      \[
      p q x y + p^2 x + p r x + q^2 y + p q + q r + q r y + p q + r^2
      = p x + q y + r
      \]
      donc
      \[
      p q x y + (p^2 + pr ) x + (q^2 + qr ) y + p q + q r + + p q +
      r^2 = p x + q y + r
      \]
      et par question 3a, les coefficients devant $xy$, $x$, $y$ et
      les constantes sont égales, donc avec les coefficients en $xy$
      :
      \[
      p q = 0 
      \]
      qui est absurde car les deux sont non nuls. Les variables $X$
      et $Y$ ne sont donc pas indépendantes.
      
    \item On se sert des dérivées partielles de $G_Z$ : 
      \[
      \partial_1 (G_Z ) (x,y) = n p ( px + q y + r)^{n-1} \ \ , \
      \ \partial_2 (G_Z ) (x,y) = n q ( px + q y + r )^{ n-1 }
      \]
      puis
      \[
      \partial_{1,2} (G_Z) (x,y) = n (n-1) p q (px + q y + r )^{ n - 2 } 
      \]
      donc
      \[
      \Cov ( X , Y ) = n (n-1) pq (p+q+r)^{n-2} - n p ( p + q +
      r)^{n-1} n q ( p + q + r)^{n-1} = [ n(n-1) - n^2 ] p q = - n p
      q
      \]
      car $p+q+r=1$. Ce signe est prévisible car lorsque $X$
      augmente, $Y$ aura tendance à baisser car le nombre de tirages
      restants pouvant donner des jetons B diminuent.       
    \end{noliste}
  \end{noliste}
\end{exercice}

\addtocounter{exercice}{-1}
\begin{exercice}{\it (Exercice sans préparation)}~\\
  Soit $n \in \N^*$ et $A$ une matrice de $\mathcal{M}_n (\R)$ telle
  que $A {}^t A A {}^t A A = I$, où $I$ est la matrice identité de
  $\mathcal{M}_n (\R)$.
  \begin{noliste}{1.}
  \item On remarque qu'on a une forme $A B = I$, donc $A$ est
    inversible et :
    \[
    A^{-1} = {}^tA A {}^tA A 
    \]
    qui est symétrique car : 
    \[
    {}^t(A^{-1}) = {}^t( {}^tA A {}^tA A ) ={}^t A {}^t ( {}^t A )
    {}^t A {}^t ({}^t A ) = {}^t A A {}^t A A = A^{-1} .
    \]
    Or on sait que l'inverse et la transposée sont commutatives donc : 
    \[
    ( {}^tA )^{-1} = A^{-1} 
    \]
    et en passant à l'inverse, ${}^tA = A$ et $A$ est symétrique. \\
    
  \item On en déduit que $A^5 = I$, et que $A$ est symétrique donc
    diagonalisable. Il existe donc $P$ inversible et $D$ diagonale
    telles que
    \[
    A = P D P^{-1}
    \]
    avec les valeurs diagonales de $D$ qui sont les valeurs propres de
    $A$, donc racines du polynômes annulateurs. Elles vérifient donc
    $\lambda^5 - 1 = 0$, donc $\lambda^5 = 1$, donc l'unique solution
    (la fonction $f(x) = x^5$ est bijective) est $\lambda =
    1$.\\
    On en déduit que $D = I$, donc
    \[
    A = P I P^{-1} = I . 
    \]
  \end{noliste}
\end{exercice}


\newpage

\begin{exercice}{\it (Exercice avec préparation)}~
  \begin{noliste}{1.}
  \item La partie entière d'un nombre réel $x$ est l'unique entier $k$
    vérifiant :
    \[
    k = \lfloor x \rfloor \leq x < k+1 = \lfloor x \rfloor + 1 . 
    \]
    
    Sa représentation graphique est constituée de multiples segment
    horizontaux, avec le point à gauche du segment inclus et le point
    à droite exclus.
     
    On note $E$ l'espace vectoriel des applications de $\R$ dans $\R$
    et $F$ le sous-espace vectoriel de $E$ engendré par les quatre
    fonctions $f_0$, $f_1$, $f_2$ et $f_3$ définies par :
    \[
    \forall x \in \R , \ f_0 (x) = 1 , \ f_1 (x) = x , \ f_2 (x) = e^x
    , \ f_3 (x) = x e^x 
    \]

  \item On note : $\mathcal{B} = (f_0 , f_1 , f_2 , f_3)$.
    \begin{noliste}{a)}
    \item La famille est génératrice de $e$ par définition, testons sa
      liberté. On suppose que $a f_0 + b f_1 + c f_2 + d f_3 = 0$,
      alors la limite en $+\infty$ donne :
      \[
      \dlim{ x \rightarrow + \infty } x e^x \left( d + \frac{ c }{ x }
        + b e^{-x} + \frac{ a }{ x } e^{ - x } \right) = \dlim{ x
        \rightarrow + \infty } 0 = 0
      \]
      et comme le facteur entre parenthèses tend vers $d$ et l'autre
      vers $+\infty$, ce n'est possible que si $d=0$. Mais on a alors
      a $ a f_0 + b f_1 + c f_2$ et la limite donne encore :
      \[
      \dlim{ x \rightarrow + \infty } e^x \left( c+ b x e^{-x} + a e^{
          - x } \right) = \dlim{ x \rightarrow + \infty } 0 = 0
      \]
      et de la même manière, ce n'est possible que si $c=0$. On a
      alors $0 f_0 + b f_1 = 0$, puis
      \[
      \dlim{ x \rightarrow + \infty } x \left( b+ \frac{a}{x} \right)
      = \dlim{ x \rightarrow + \infty } 0 = 0
      \]
      possible seulement si $b=0$, et enfin $a f_0 = 0$, avec $f_0 =
      1$ donc $a=0$.\\
      La famille est donc libre, c'est une base de $F$. 

    \item Toutes les fonctions de $f$ sont des combinaisons linéaires
      des $f_i$ qui sont toutes de classe $C^{ \infty }$ sur $\R$,
      donc les élément de $f$ sont tous de classe $C^{ \infty }$, et
      en particulier continues et dérivables sur $\R$.

    \end{noliste}

  \item Soit $\Phi$ l'application définie par : pour tout $f \in F$,
    $\Phi (f) = f'$, où $f'$ est la dérivée de $f$. 
    \begin{noliste}{a)}
    \item On calcule $\Phi (f)$ pour chaque élément de la base :
      \[
      \Phi ( f_0 ) = 0 \ \ , \ \ \Phi ( f_1 ) =f_0 \ \ , \ \ \Phi (
      f_2 ) = f_2 \ \ , \ \ \Phi ( f_3 ) = f_2 + f_3
      \]      
      donc ils sont tous éléments de $F$. Par stabilité de $f$ par
      combinaison linéaire, pour tout $f = a f_0 + b f_1 + c f_2 + d
      f_3 \in F$, on obtient :
      \[
      \Phi (f) = a \Phi ( f_0 ) + b \Phi ( f_1 ) + c \Phi ( f_2 ) + d
      \Phi (f_3 ) \in F
      \]
      et $\Phi$ est à valeurs de $F$. De plus par linéarité de la
      dérivation $\Phi$ est linéaire, c'est un endomorphisme de
      $F$. Les images trouvées juste avant donnent alors :
      \[
      M = 
      \begin{smatrix} 
        0 & 1 & 0 & 0 \\ 
        0 & 0 & 0 & 0 \\ 
        0 & 0 & 1 & 1 \\ 
        0 & 0 & 0 & 1 \\ 
      \end{smatrix}
      \]
      
    \item $M$ est triangulaire, ses valeurs propres sont sur la
      diagonale : $\spc ( M ) = \{ 0 ; 1 \}$.
      
      De plus on trouve sans problème $E_0 (M ) = \Vect{
        \begin{smatrix} 
          1 \\ 
          0 \\ 
          0 \\ 
          0 
        \end{smatrix} }$ et $E_1 (M) = \Vect { 
        \begin{smatrix} 
          0 \\ 
          0 \\ 
          1 \\ 
          0 
        \end{smatrix} }$ donc la somme des dimensions des sous-espaces
      propres vaut 2 et $P$ est d'ordre 4, elle n'est pas
      diagonalisable, et $\Phi$ ne l'est donc pas non plus.

    \item On remarque que
      \[
      \Im \Phi = \Vect{ f_0 , f_2 , f_2 + f_3 } = \Vect{ f_0 , f_2 , f_3 } 
      \]      
      avec le pivot $C_3 \leftarrow C_3 - C_2$, donc $f_3 \in \Im
      \Phi$. De plus on résout :
      \[
      M X = 
      \begin{smatrix} 
        0 \\ 0 \\ 0 \\ 1 
      \end{smatrix}
      \Longleftrightarrow 
      \begin{smatrix} 
        y \\ 0 \\ z + t \\ t
      \end{smatrix} = 
      \begin{smatrix} 
        0 \\ 0 \\ 0 \\ 1
      \end{smatrix}
      \Longleftrightarrow X = 
      \begin{smatrix} 
        x \\ 0 \\ -1 \\ 1 
      \end{smatrix}
      \]      
      donc
      \[
      \Phi (f) = f_3 \Longleftrightarrow f = x f_0 + f_3 - f_2 .
      \]
    \end{noliste}

  \item On note $G$ l'ensemble des fonctions $g$ de $E$ telles que :
    \[
    \forall x \in \R , \ g(x+1) - g(x) = 0 . 
    \]    
    \begin{noliste}{a)}       
    \item $G$ est inclus dans $E$ et contient la fonction nulle
      puisque pour tout $x \in \R$, $0(x+1) - 0(x) = 0 - 0 = 0$ et si
      $g_1$ et $g_2$ sont dans $G$ et $\lambda$ est un réel :
      \[
      \forall x \in \R , \ ( \lambda g_1 + g_2 ) (x+1) - (\lambda g_1
      + g_2 ) (x) = \lambda ( g_1 (x+1) - g(x) ) + g_2 (x+1) - g_2 (x)
      = \lambda 0 + 0 = 0
      \]      
      donc $(\lambda g_1 + g_2 ) \in G$, qui est stable par
      combinaison linéaire : c'est un sous-espace vectoriel de $E$.
      
      Les éléments de $F \cap G$ sont les fonctions $f$ combinaisons
      linéaires des $f_i$ et vérifiant $f(x+1) - f(x) = 0$ pour tout
      $x$, donc :
      \[
      \forall x \in \R , \ f(x) = a + b x + c e^x + d x e^x \ \
      \text{ et } \ \ f(x+1) - f(x) = b (x+1-x) + c ( e^{x+1} - e^x )
      + d ( (x+1) e^{x+1} - x e^x ) ) = 0
      \]
      donc : 
      \[
      b + c (e-1) e^x + d ( e x e^x + e e^x - x e^x ) = b + [ c (e-1)
      + d e ) e^x + d (e - 1 ) x e^x = 0
      \]
      Par liberté de la famille $(f_i)_{ 0 \leq i \leq 3} $, on
      obtient $b = c ( e-1) + d e = d ( e-1) = 0 $ (avec $e = e^1$),
      donc $b=c=d=0$, et enfin :
      \[
      F \cap G = \{ f = a f_0 , a \in \R \} = \Vect{ f_0 } . 
      \]
      
    \item On cherche une fonction vérifiant $f ( x + 1 ) = f(x)$ pour
      tout $x$, c'est-à-dire périodique de période 1. Il suffit de
      créer une fonction sur $[0;1[$, puis de la reporter par
      périodicité. Par exemple la fonction $f(x) = x - \lfloor x
      \rfloor$ (pour utiliser la question de cours) fonctionne, et
      n'est pas élément de $F$.      
    \end{noliste}

  \item On pose $u$ l'application linéaire :
    \[
    \forall x \in \R , \ u (f) (x) = f(x+1) - f(x) - (e-1) f'(x) 
    \]    
    et on cherche sa matrice dans la base $\mathcal{B}$. On obtient :
    \[
    u (f_0) = 0 \ \ , \ \ u (f_1 ) = (2-e) f_0 \ \ , \ \ u (f_2) = 0
    \ \ , \ \ u (f_3 ) = (2-e) f_2
    \]
    donc
    \[
    Mat_{ \mathcal{B} } (u) = \begin{smatrix} 0 & 2-e & 0 & 0 \\ 0 &
      0 & 0 & 0 \\ 0 & 0 & 0 & 2-e \\ 0 & 0 & 0 & 0 \\ \end{smatrix}=
    N
    \]
    donc on trouve sans difficulté que $\ker N = \Vect
    { \begin{smatrix} 1 \\ 0 \\ 0 \\ 0 \\ \end{smatrix}
      , \begin{smatrix} 0 \\ 0 \\ 1 \\ 0 \\ \end{smatrix} }$, puis
    \[
    \ker ( u ) = \{ f \in F , \forall x \in \R , f(x+1) - f(x) =
    (e-1) f'(x) \} = \Vect [ f_0 , f_2 ] .
    \]
  \end{noliste}
\end{exercice}

\begin{exercice}{\it (Exercice sans préparation)}~\\
  Soit $p$ un réel de $]0;1[$ et $q=1-p$. Soit $(X_n)_{ n \in \N^* }$
  une suite de variables aléatoires indépendantes définies sur un
  espace probabilisé $(\Omega , \mathcal{A} , P)$, de même loi de
  Bernouilli telle que : \\
  $ \forall k \in \N^* , \ P ( [ X_k = 1 ] ) = p$ et $\Prob( [ X_k = 0
  ] ) = q$. Pour $n$ entier de $\N^*$, on définit pour tout $k \in
  \llb 1 ; n \rrb$, la variable aléatoire $Y_k = X_k + X_{k+1}$.
  \begin{noliste}{1.}
  \item 
    \begin{enumerate}
      
    \item Par bilinéarité de la covariance,
       \[
       \Cov ( Y_k , Y_{k+1} ) = \Cov ( X_k , X_{k+1} ) + \Cov ( X_k , X_{k+2} ) + \Cov ( X_{k+1} , X_{k+1} ) + \Cov ( X_{k+1} , X_{k+2} ) 
       \]

       et comme les $X_i$ sont indépendantes, on obtient : 
       \[
       \Cov ( Y_k , Y_{k+1} ) = 0 + 0 + V (X_{k+1} ) + 0 = p (1-p) . 
       \]

     \item On étudie la fonction $ f(p ) = p (1-p) = p - p^2$ sur $[0,1]$ : elle est dérivable et vérifie
       \[
       f'(p) = 1 - 2 p 
       \]

       qui s'annule en $\frac{1}{2}$, est positive avant et négative avant donc $f$ admet un maximum en $\frac{1}{2}$ égal à $\frac{1}{4}$, et comme $f(0) = f(1) = 0$, elle admet un minimum égal à 0 atteint seulement en 0 et 1, donc est strictement supérieure ailleurs et : 
       \[
       0 < \Cov ( Y_k , Y_{k+1} ) = p (1-p ) \leq \frac{ 1 }{ 4 } . 
       \]

     \end{enumerate}

   \item Si $l = k+1$, on a déjà vu que $\Cov ( Y_k , Y_l ) = 0$. Sinon avec la bilinéarité on fait apparaître 4 covariances nulles avec l'indépendance des $X_i$, et $\Cov ( Y_k , Y_l ) = 0$. \\

   \item Par linéarité de l'espérance, $E ( Y_k ) = 2p$ puis $E \left( \frac{1}{n} \Sum{k=1}{n} Y_k \right) = 2 p $ donc l'inégalité de B-T donne : 
     \[
     P \left( \left\vert \frac{1}{n} \Sum{k=1}{n} Y_k - 2 p \right\vert > \varepsilon \right) \leq \frac{ V \left( \frac{1}{n} \Sum{k=1}{n} Y_k \right) }{ \varepsilon^2 } = \frac{ V \left( \Sum{k=1}{n} Y_k \right) }{ n^2 \varepsilon^2 } . 
     \]

     On calcule cette variance par bilinéarité de la covariance : 
     \begin{eqnarray*}
       V \left( \Sum{k=1}{n} Y_k \right) & = & \Cov \left( \Sum{k=1}{n} Y_k   , \Sum{j=1}{n} Y_j \right) = \Sum{k=1}{n} \Sum{j=1}{n} \Cov ( Y_k , Y_j ) \\ \\
       & = & \Sum{k=1}{n} \Cov ( Y_k , Y_k ) + \Sum{k=1}{n-1} \Cov ( Y_k , Y_{k+1} ) + \Sum{k=2}{n} \Cov ( Y_{k-1} , Y_k ) + 0 
     \end{eqnarray*}

     et avec les questions précédentes : 
     \[
     V \left( \Sum{k=1}{n} Y_k \right) = n p (1-p) + (n-1) p (1-p) + (n-1) p (1-p) = (3n-2) p (1-p ) 
     \]

     et enfin on peut conclure que : 
     \[
     0 \leq P \left( \left\vert \frac{1}{n} \Sum{k=1}{n} Y_k - 2 p \right\vert > \varepsilon \right) \leq \frac{ (3n-2) p (1-p) }{ n^2 \varepsilon^2 } \xrightarrow[ n \rightarrow + \infty ]{} 0 
     \]

     donc par encadrement la probabilité tend vers 0 lorsque $n$ tend vers $+\infty$. \\

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Deux matrices carrées d'ordre $n$ $A$ et $B$ sont dites semblables s'il existe une matrice $P$ carrée d'ordre $n$, inversible, telle que : 
     \[
     A = P B P^{-1} 
     \]

     On sait de plus que deux matrice sont semblables si et seulement si elles sont deux matrices d'un même endomorphisme dans des bases différentes. \\

     Soit $E$ un espace vectoriel sur $\R$ de dimension 2. On note $\mathcal{L} ( E )$ l'ensemble des endomorphismes de $E$. \\
     Pour toute matrice $A = \begin{smatrix} a & b \\ c & d \\ \end{smatrix} \in \mathcal{M}_2 (\R) $, on note $D$ et $T$ les deux applications suivantes : 
     \[
     D : \mathcal{M}_2 (\R) \rightarrow \R , \ A \mapsto a d - b c \ \ \ \text{ et } \ \ \ T : \mathcal{M}_2 (\R) \rightarrow \R , \ A \mapsto a + d . 
     \]

   \item Soit $A$ et $B$ deux matrices de $\mathcal{M}_2 (\R)$. \begin{enumerate}

     \item On pose $A = \begin{smatrix} a & b \\ c & d \\ \end{smatrix}$ et $B = \begin{smatrix} x & y \\ z & t \\ \end{smatrix}$, on a 
       \[
       D ( A ) = a d - b c \ \ , \ \ D ( B ) = x t - y z \ \ , \ \ A B = \begin{smatrix} a x + b z & a y + b t \\ c x + d z & c y + d t \\ \end{smatrix} 
       \]

       puis
       \begin{eqnarray*}
         D ( AB ) & = & (ax + bz ) (cy + dt) - (ay+bt) (cx + d z ) = a c x y + a d x t + b c y z + b d z t - acxy - a d y z - b c x t - b d z t \\ \\
         &  & a d x t + b c y z - a d y z - b c x t = ad ( xt - yz ) + bc ( yz - xt ) = (xt - y z ) (ad - bc ) = D ( A ) D ( B ) . 
       \end{eqnarray*}

       D'autre part on a
       \[
       B A = \begin{smatrix} a x + c y & b x + d y \\ a z + c t & b z + d t \\ \end{smatrix} 
       \]

       donc
       \[
       T ( A B ) = a x + b z + c y + dt = (a x + c y ) + (bz + dt) = T ( B A ) . 
       \]

     \item Si $a$ et $b$ sont semblables, il existe $P$ inversible tel que $B = P A P^{-1}$ puis : 
       \[
       D ( B ) = D ( P ) D ( A ) D ( P^{-1} ) = D ( A ) D ( P P^{-1} ) = D ( A ) D ( I ) = D ( A ) (1 \times 1 - 0 \times 0 ) = D ( A ) . 
       \]

       D'autre part on a 
       \[
       T ( B ) = T ( [PA]P^{-1} ) = T ( P^{-1} [PA] ) = T ( P^{-1} P A ) = T ( A ) . 
       \]

     \end{enumerate}

   \item $D ( A ) = 0$ est vrai si et seulement si $a d = bc $, ce qui est vrai si les produits sont nuls ou si tous les coefficients sont non nuls et $ d = \frac{ b c }{ a }$ donc
     \[
     \ker D = \left\{ \begin{smatrix} 0 & b \\ 0 & d \\ \end{smatrix} \right\} \cup \left\{ \begin{smatrix} a & b \\ 0 & 0 \\ \end{smatrix} \right\} \cup \left\{ \begin{smatrix} 0 & 0 \\ c & d \\ \end{smatrix} \right\} \cup \left\{ \begin{smatrix} a & 0 \\ c & 0 \\ \end{smatrix} \right\} \cup \left\{ \begin{smatrix} a & b \\ c & \frac{ c }{ a } b \\ \end{smatrix} \right\} 
     \]

     et on remarque que toutes les matrices concernées ne sont pas inversible. Réciproquement si $A$ n'est pas inversible, soit la première colonne est nulle et $D (A) = 0$, soit la deuxième est colinéaire à la première ce qui signifie qu'il existe $\lambda$ tel que $ b = \lambda a$ et $d = \lambda c$, et on a alors
     \[
     D ( A ) = a d - b c = \lambda a c - \lambda a c = 0 
     \]

     donc on obtient la double-inclusion et finalement : 
     \[
     \ker ( D ) = \{ A \in \mathcal{M}_2 ( \R ) \text{ tq } A \text{ n'est pas inversible } \} . 
     \]

     Pour $T$ c'est plus simple, car on reconnaît une équation linéaire : 
     \[
     A \in \ker ( T ) \Longleftrightarrow a + d = 0 \Longleftrightarrow a = - d \Longleftrightarrow A = \begin{smatrix} - d & b \\ c & d \\ \end{smatrix} 
     \]

     donc
     \[
     \ker T = \Vect { \begin{smatrix} -1 & 0 \\ 0 & 1 \\ \end{smatrix} , \begin{smatrix} 0 & 1 \\ 0 & 0 \\ \end{smatrix} , \begin{smatrix} 0 & 0 \\ 1 & 0 \\ \end{smatrix} } 
     \]

     et la famille est libre (système facile) donc $\ker T$ est un espace vectoriel de dimension 3. \\

     Dorénavant, si $u \in \mathcal{L} (E)$ de matrice $A$ dans une base $\mathcal{B}$ de $E$, on note : $D(u) = D(A)$ et $T(u) = T(A)$. \\

   \item On exprime $A^2$ en fonction de $A$ et $I$ : 
     \[
     A^2 = \begin{smatrix} a^2 + bc & a b + b d \\ a c + cd & d^2 + bc \\ \end{smatrix} = \lambda \begin{smatrix} a & b \\ c & d \\ \end{smatrix} + \mu \begin{smatrix} 1 & 0 \\ 0 & 1 \\ \end{smatrix} 
     \]

     si et seulement si
     \[
     \left\{ \begin{array}{c} a^2 + bc = \lambda a + \mu \\ b ( a + d )= \lambda b \\ c ( a+d) = \lambda c \\ d^2 + bc = \lambda d + b\mu \\ \end{array} \right. 
     \]

     On remarque que $\lambda = a + d = T ( A )$ résout les deux équations du milieu, et on obtient alors sur les deux autres : 
     \[
     \left\{ \begin{array}{c} a^2 + bc = a^2 + ad  + \mu \\ d^2 + bc = a d + d^2 + \mu \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{c}  bc - ad =   \mu \\  bc - a d = \mu \\ \end{array} \right. \Longleftrightarrow \mu = b c - ad = - D ( A ) 
     \]

     donc pour toute matrice $A$ on a 
     \[
     A^2 = T(A) A - D ( A ) I \ \ \text{ donc } \ \ u^2 = T ( A ) u - D ( A ) \id = T (u) u - D ( u ) \id . 
     \]

   \item $\mathcal{S}_0$ est inclus dans l'espace vectoriel $\mathcal{L} (E)$ et contient l'endomorphisme nul car $u \circ 0 = 0 \circ u = 0$. De plus pour tous $v_1 , v_2$ dans $\mathcal{S}_0$ et tout réel $\lambda$ on a : 
     \[
     u \circ ( \lambda v_1 + v_2 ) - ( \lambda v_1 + v_2 ) \circ u = \lambda ( u \circ v_1 - v_1  \circ u ) + u \circ v_2 - v_2 \circ u = \lambda 0 + 0 = 0 
     \]

     donc $(\lambda v_1 + v_2) \in \mathcal{S}_0$ qui est donc stable par combinaison linéaire, c'est un sous-espace vectoriel de $\mathcal{L} (E)$. \\

     De plus tout polynôme en $u$ commute avec $u$ donc $\{ P(u) , P(u) \in \R [X] \} \subset \mathcal{S}_0$. \\

   \item Soit $u \in \mathcal{L} (E)$ avec $u \neq 0$. On pose : $\mathcal{S} = \{ v \in \mathcal{L} (E) \vert u \circ v - v \circ u = u \}$. \begin{enumerate}

     \item Si l'ensemble est non vide, il existe un endomorphisme $v$ tel que
       \[
       u \circ v - v \circ u = u . 
       \]

       On suppose que $u$ est bijectif. Alors en passant aux matrices $A$ et $B$ de $u$ dans la base $\mathcal{B}$ de $E$ et en multipliant à gauche par $A^{-1}$ on obtient :
       \[
       A B - B A = A \ \ \ \text{ puis } \ \ \ B - A^{-1} B A = I 
       \]

       On compose par $T$ linéaire et comme $A^{-1} B A$ est semblable à $b$, on obtient : 
       \[
       T ( B ) - T ( B) = T ( I ) \Longleftrightarrow 0 = 2 
       \]

       qui est absurde. On en déduit que si $\mathcal{S}$ est non vide, $u$ n'est pas bijectif. \\

       On peut alors appliquer l'application linéaire $T$ à la relation de $\mathcal{S}$ : 
       \[
       T ( u \circ v ) - T ( v \circ u ) = T ( u ) \ \ \text{ donc } \ \ T ( u ) = 0 
       \]

       puisque $T ( u \circ v ) = T ( A B ) = T ( B A ) = T ( v \circ u )$ avec $A$ et $B$ les matrices de $u$ dans la base $\mathcal{B}$ de $E$. \\

       On en déduit que $u^2 = T ( u) - D ( u ) \id = - D(u) \id = 0 $, puisque $u$ n'est pas bijectif, donc $A$ pas inversible, et enfin $D (u) = D ( A ) = 0$. \\

       On en déduit que si $\mathcal{S}$ est non vide, alors $u^2 = 0$. \\

     \item Comme $u \neq 0$, il existe $x \in E$ tel que $u ( x ) \neq 0$, et la famille $(x , u(x) )$ est alors libre. En effet si $a x + b u(x) = 0$, alors
       \[
       a u(x) + b u^2 (x) = 0 \ \ \text{ donc } \ \ a u(x) = 0 
       \]

       avec $u(x) \neq 0$, donc $a = 0$. On obtient alors $ b u(x) = 0$, avec $u(x) \neq 0$ donc $b=0$. Dans la base $( u(x) , x )$ de $E$, on obtient alors
       \[
       M_u = Mat_{ (u(x) , x) } (u) = \begin{smatrix} 0 & 1 \\ 0 & 0 \\ \end{smatrix} 
       \]

       Dans cette base, la matrice $B = \begin{smatrix} x & y \\ z & t \\ \end{smatrix} $ de $v$ vérifie : 
       \[
       M_u B - B M_u = M_u \Longleftrightarrow \begin{smatrix} z & t \\ 0 & 0 \\ \end{smatrix} - \begin{smatrix} 0 & x \\ 0 & z \\ \end{smatrix} = \begin{smatrix} 0 & 1 \\ 0 & 0 \\ \end{smatrix} \Longleftrightarrow \begin{smatrix} z & t - x \\ 0 & -z \\ \end{smatrix} = \begin{smatrix} 0 & 1 \\ 0 & 0 \\ \end{smatrix} 
       \]

       donc $z = 0$ et $t = x + 1$, et enfin
       \[
       B = \begin{smatrix} x & y \\ 0 & x + 1 \\ \end{smatrix} = x I + y M_u + \begin{smatrix} 0 & 0 \\ 0 & 1 \\ \end{smatrix} 
       \]

     \item On déduit de la forme de $B$ ci-dessus que
       \[
       v = x \id + y u + v_0 
       \]

       où $v_0$ est l'endomorphisme dont la matrice dans la base $( u(x) , x )$ est $\begin{smatrix} 0 & 0 \\ 0 & 1 \\ \end{smatrix}$, donc vérifiant $v_0 ( u(x) ) = 0$ et $v_0 (x) = x$, et on obtient bien
       \[
       \mathcal{S} = \{ v_0 + \alpha \id + \beta u , \alpha , \beta \in \R \} . 
       \]

       Bilan : un exercice totalement inadapté à l'ECE (même en voie S, il serait très difficile...) mais l'introduction de la trace et du déterminant est très intéressante, notamment dans l'optique de l'utilisation sur des matrices Hessiennes. \\

     \end{enumerate}

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $k$ et $\lambda$ deux réels et soit $f$ la fonction définie sur $\R$ à valeurs réelles donnée par : 
   \[
   f(t) = \left\{ \begin{array}{cl} k t e^{ - \lambda t } & \text{ si } t \geq 0 \\ 0 & \text{ sinon } \\ \end{array} \right. 
   \]
   \begin{enumerate}
   \item $f$ est continue sur $\R$ sauf peut-être en 0, et positive sur $\R$ à condition que $k$ soit positif. Enfin on a : 
     \[
     \int_{ - \infty }^{ + \infty } f(t) \ dt = \int_{- \infty}^0 0 \ dt + \frac{ k }{ \lambda } \int_0^{+\infty} t \times \lambda e^{ -  \lambda t } \ dt . 
     \]

     La première intégrale converge et vau 0 (fonction nulle), la seconde converge et vaut $ \frac{1}{\lambda}$ (espérance de la loi exponentielle), donc l'intégrale de $f$ sur $\R$ converge et
     \[
     \int_{ - \infty }^{ + \infty } f(t) \ dt = \frac{ k }{ \lambda^2 } 
     \]

     qui vaut 1 si et seulement si $k = \lambda^2$, qui est bien positif, et pour cette valeur $f$ est bien une densité de probabilité. \\

     On note $X$ une variable aléatoire réelle ayant $f$ pour densité. \\

   \item On comparaison avec $\frac{1}{t^2}$ donne par croissances comparées :
     \[
     t^n f(t) = o_{ + \infty } \left( \frac{ 1 }{ t^2 } \right) 
     \]

     avec les fonctions qui sont positives et l'intégrale de $1/t^2$ qui converge en $+\infty$ (Riemann) donc par théorème de comparaison $X^n$ admet une espérance et $X$ admet un moment d'ordre $n$, pour tout $n \in \N^*$. De plus on effectue une IPP sur l'intégrale partielle avec
     \[
     u = \lambda t^{n+1} \ \ \ \text{ et } \ \ \ v = - e^{ - \lambda t } 
     \]

     qui sont de classe $C^1$ avec
     \[
     u' = (n+1) \lambda t^n \ \ \ \text{ et } \ \ \ v' = \lambda e^{ - \lambda t } 
     \]

     donc
     \[
     \int_0^A \lambda^2 t^{n+1} e^{ - \lambda t } \ dt = \left[ - \lambda t^{n+1} e^{ - \lambda t } \right]_0^A + \frac{ (n+1) }{ \lambda } \int_0^A \lambda^2 t^n e^{ - \lambda t } \ dt 
     \]

     puis en faisant tendre $A$ vers $+\infty$ (avec une croissance comparée) on obtient : 
     \[
     E ( X^n ) = \frac{ n+1 }{ \lambda } E ( X^{n-1} ) 
     \]

     donc par itération de la relation ou par récurrence on obtient : 
     \[
     E ( X^n ) = \frac{ (n+1) \times \dots \times 2 }{ \lambda^n } E ( X^0 ) = \frac{ (n+1)! }{ \lambda^n } \times 1 = \frac{ (n+1)! }{ \lambda^n } . 
     \]


   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item La loi d'un couple $(X,Y)$ de variables discrètes est caractérisée par la donnée des supports de $X$ et $Y$, et pour tout $i \in X ( \Omega)$ et tout $j \in Y ( \Omega)$, de la probabilité : 
     \[
     P ( [X=i] \cap [Y=j] ) 
     \]

     On appelle lois marginales les lois des variables $X$ et $Y$, et loi conditionnelle de $Y$ sachant $[X=i]$, pour $i$ fixé dans $Y ( \Omega )$, la donnée du support de $Y$ sachant $[X=i]$ (car cet évènement peut rendre impossibles certaines valeurs de $Y$) et pour tout $j \in Y ( \Omega)$, de la probabilité : 
     \[
     P_{ [X = i] } \Ev{ Y = j } = \frac{ P ( [X=i] \cap [Y=j] ) }{ P \Ev{X=i } } 
     \]

     et de même pour la loi conditionnelle de $X$ sachant $[Y=j]$. \\

     Soit $c$ un réel strictement positif et soit $X$ et $Y$ deux variables aléatoires à valeurs dans $\N$ définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$, telles que : 
     \[
     \forall (i,j) \in \N^2 , \ \Prob( [ X = i ] \cap [ Y=j ] ) = c \frac{ i + j }{ i! j! } . 
     \]

   \item \begin{enumerate}

     \item Avec le système complet d'évènements  $[Y=j]_{ j \in \N}$, les probabilités totales donnent : pour tout $i \in \N$,
       \begin{eqnarray*}
         P \Ev{ X = i } & = & \Sum{j=0}{+\infty} P ( [X=i ] \cap [Y=j] ) = \Sum{j=0}{+\infty} c \frac{ i + j }{ i! j! } = \frac{ c }{ i! } \left( i \Sum{j=0}{+\infty} \frac{ 1 }{ j! } + \Sum{j=0}{+\infty} \frac{ j }{ j! } \right) \\ \\
         & = & \frac{ c }{ i! } \left( i e^1+ 0 + \Sum{j=1}{+\infty} \frac{ 1 }{ (j-1)! } \right) =  \frac{ c }{ i! } \left( i e^1+ 0 + \Sum{j=0}{+\infty} \frac{ 1 }{ j! } \right) = \frac{ c }{ i! } \left( i e^1+ 0 + e^1 \right) \\ \\
         & = & c \frac{ i+1 }{ i! } e . 
       \end{eqnarray*}

       De plus on sait que la somme des $P \Ev{X=i}$ doit faire 1 (système complet d'évènements) on la calcule : 
       \begin{eqnarray*}
         \Sum{i=0}{+\infty} P \Ev{X=i} & = & c e \left( \Sum{i=0}{+\infty} \frac{ i }{ i! } + \Sum{i=0}{+\infty}\frac{ 1 }{ i! } \right) = c e \left( 0 + \Sum{i=1}{+\infty} \frac{ 1 }{ (i-1)! } + e^1 \right) = c e \left( \Sum{i=0}{+\infty} \frac{ 1 }{ i! } + e \right) \\ \\
         & = & c e ( 2 e ) = 2 c e^2 
       \end{eqnarray*}

       donc on en déduit que
       \[
       2 c e^2 = 1 \Longleftrightarrow c = \frac{ 1 }{ 2 e^2 } . 
       \]

     \item Pour l'espérance, on s'intéresse à la convergence absolue et à la valeur de 
       \[
       \Sum{i=0}{+\infty} i P \Ev{x=i } = \frac{ 1 }{ 2 e } \Sum{i=0}{+\infty} \frac{ i (i+1 ) }{ i! } 
       \]

       Après transformations, on reconnaît une série exponentielle, elle converge et elle est à terme positifs, donc elle converge absolument et $X$ admet une espérance, qui vaut : 
       \begin{eqnarray*}
         E ( X ) & = & \frac{ 1 }{ 2 e } \left( 0 +  \Sum{i=1}{+\infty} \frac{ i+1 }{ (i-1)! } \right) = \frac{ 1 }{ 2 e }   \Sum{i=1}{+\infty} \frac{ i - 1 +2 }{ (i-1)! }  = \frac{ 1 }{ 2 e } \left(  \Sum{i=1}{+\infty} \frac{ i - 1 }{ (i-1)! } +  \Sum{i=1}{+\infty} \frac{2 }{ (i-1)! } \right) \\ \\
         & = & \frac{ 1 }{ 2 e } \left( 0 + \Sum{i=2}{+\infty} \frac{ 1 }{ (i-2)! } + 2 \Sum{i=1}{+\infty} \frac{1 }{ (i-1)! } \right) = \frac{ 1 }{ 2 e } \left( \Sum{i=0}{+\infty} \frac{ 1 }{ i! } + 2 \Sum{i=0}{+\infty} \frac{1 }{ i! } \right) \\ \\
         & = & \frac{ 1 }{ 2 e } ( e + 2e ) = \frac{ 3 e }{ 2e } = \frac{ 3 }{ 2 } .
       \end{eqnarray*}

       En faisant de même apparaitre des séries exponentielles et avec le théorème de transfert, on obtiendra une série absolument convergente et $E ( X^2 )$ existe donc $V ( X)$ aussi. De plus : 
       \begin{eqnarray*}
         E ( X^2 ) & = & \Sum{i=0}{+\infty} \frac{ i^2 (i+1 ) }{ i! } = \frac{ 1 }{ 2 e } \left( 0 +  \Sum{i=1}{+\infty} \frac{ i ( i+1 ) }{ (i-1)! } \right) = \frac{ 1 }{ 2 e }   \Sum{i=1}{+\infty} \frac{ (i - 1 + 1 ) (i+1) }{ (i-1)! }  \\ \\
         & = & \frac{ 1 }{ 2 e } \left(  \Sum{i=1}{+\infty} \frac{ (i - 1) (i+1) }{ (i-1)! } +  \Sum{i=1}{+\infty} \frac{i+1 }{ (i-1)! } \right) = \frac{ 1 }{ 2 e } \left( 0 + \Sum{i=2}{+\infty} \frac{ i+1 }{ (i-2)! } +  \Sum{i=1}{+\infty} \frac{ i - 1 + 2 }{ (i-1)! } \right) \\ \\
         & = & \frac{ 1 }{ 2 e } \left( \Sum{i=2}{+\infty} \frac{ i - 2+3 }{ (i-2)! } +  \Sum{i=1}{+\infty} \frac{ i - 1  }{ (i-1)! } +  \Sum{i=1}{+\infty} \frac{ 2 }{ (i-1)! } \right) \\ \\
         & = & \frac{ 1 }{ 2 e } \left( \Sum{i=2}{+\infty} \frac{ i - 2 }{ (i-2)! } + \Sum{i=2}{+\infty} \frac{ 3 }{ (i-2)! }  + 0 + \Sum{i=2}{+\infty} \frac{ 1  }{ (i-2)! } + 2 \Sum{i=1}{+\infty} \frac{ 1 }{ (i-1)! } \right) \\ \\
         & = & \frac{ 1 }{ 2 e } \left( 0 + \Sum{i=3}{+\infty} \frac{ 1 }{ (i-3)! } + 4 \Sum{i=2}{+\infty} \frac{ 1 }{ (i-2)! }  + 2 \Sum{i=1}{+\infty} \frac{ 1 }{ (i-1)! } \right) = \frac{ 1 }{ 2 e } \left(  \Sum{i=0}{+\infty} \frac{ 1 }{ i! } + 4 \Sum{i=0}{+\infty} \frac{ 1 }{ i! }  + 2 \Sum{i=0}{+\infty} \frac{ 1 }{ i! } \right) \\ \\
         & = & \frac{ 7 }{ 2 e } \Sum{i=0}{+\infty} \frac{ 1 }{ i! } = \frac{ 7 }{ 2 e } \times e = \frac{ 7 }{ 2 }
       \end{eqnarray*}

       et enfin
       \[
       V ( X ) = \frac{ 7 }{ 2 } - \frac{ 9 }{ 4 } = \frac{ 14 - 9 }{ 4 } = \frac{ 5 }{ 4 } . 
       \]

     \item On sait que pour tous $i$ et $j$ dans $\N$ on a : 
       \[
       P ( [X=i] \cap [Y=j] ) = \frac{ 1 }{ i + j }{ 2 e^2 i! j! } = \frac{ 1 }{ 2 e^2 i! j! } \times (i+j) 
       \]

       et comme les rôles de $i$ et $j$ sont symétriques dans la formule, la loi de $Y$ est la même que celle de $X$ donc : 
       \[
       P \Ev{X = i } P \Ev{ Y = j } = \frac{ (i+1) ( j+1) }{ 4 e^2 i! j! }  = \frac{ 1 }{ 2 e^2 i! j! } \times \frac{ (i+1 ) (j+1) }{ 2 } 
       \]

       et par exemple pour $i=j=0$ on a : 
       \[
       \frac{ 1 \times 1 }{ 2 } \neq (0+0) \ \ \text{ donc } \ \ P ( [X=0] \cap [Y=0] ) \neq P \Ev{X=0} P \Ev{ Y = 0 } 
       \]

       donc $X$ et $Y$ ne sont pas indépendantes. \\

     \end{enumerate}

   \item \begin{enumerate}

     \item Cette variable prend pour valeur minimale $(0+0-1)$ mais avec une probabilité nulle car $P [ \Ev{X=0 } \cap \Ev{Y=0} ] = 0$, donc la plus petite valeur de probabilité non nulle est $(0+1-1 = 0)$ et toutes les valeurs entières supérieure à 0 sont possibles donc : 
       \[
       (X + Y - 1) ( \Omega ) = \N 
       \]

       De plus pour tout $k \in \N$, avec le système complet $\Ev{X=i}_{i \in \N}$, on a : 
       \[
       (X + Y - 1 = k ) = \bigcup\limits_{i=0}^{+\infty} [ \Ev{X=i} \cap (X+Y-1 = k ) ] = \bigcup\limits_{i=0}^{+\infty} [ \Ev{X=i} \cap ( Y = k + 1 - i  ) ] = \bigcup\limits_{i=0}^{k+1} [ \Ev{X=i} \cap ( Y = k + 1 - i ) ] 
       \]

       car pour $k + 1 - i < 0 \Longleftrightarrow i > k +1$, l'évènement $(Y = k+1-i)$ est impossible. On en déduit (union incompatible) que : 
       \begin{eqnarray*}
         P ( X + Y - 1 = k ) & = & \frac{ 1 }{ 2e^2 } \Sum{i=0}{k+1} \frac{ i+ k+1 - i }{ i! (k+1-i)! } = \frac{ 1 }{ 2e^2 } \Sum{i=0}{k+1} \frac{ k+1 }{ i! (k+1-i)! } \\ \\
         & = & \frac{ 1 }{ 2e^2 k!  } \Sum{i=0}{k+1} \frac{ (k+1) k! }{ i! (k+1-i)! } = \frac{ 1 }{ 2e^2 k!  } \Sum{i=0}{k+1} \frac{ (k+1)! }{ i! (k+1-i)! } \\ \\
         & = & \frac{ 1 }{ 2e^2 k!  } \Sum{i=0}{k+1} \binom{k+1}{i} = \frac{ 1 }{ 2e^2 k!  } \Sum{i=0}{k+1} \binom{k+1}{i} 1^i 1^{k+1-i} \\ \\
         & = & \frac{ 1 }{ 2e^2 k!  } \times (1+1)^{k+1} = \frac{ 2^{ k+1 } }{ 2 e^2 k! } = \frac{ 2^k e^{ - 2 } }{ k! } 
       \end{eqnarray*}

       donc $ X + Y - 1 \suit \mathcal{P} (2)$. \\

     \item On sait par quadracité de la variance que 
       \[
       V ( X + Y ) = V ( X + Y + 1 ) = 2 . 
       \]

     \item On peut alors calculer : 
       \[
       \Cov ( X  , X + 5 Y ) = \Cov ( X , X ) + 5 \Cov ( X , Y ) = V (X ) + 5 \Cov ( X , Y ) . 
       \]

       Or on sait que
       \[
       V ( X + Y ) = V ( X ) + V ( Y ) + 2 \Cov ( X , Y ) 
       \]

       donc
       \[
       \Cov ( X , Y ) = \frac{ V ( X + Y ) - V ( X ) - V ( Y ) }{ 2 } = \frac{ 2 - \frac{5}{4} - \frac{5}{4} }{ 2 } = 1 - \frac{ 5 }{ 4 } = - \frac{ 1 }{ 4 } 
       \]

       et enfin : 
       \[
       \Cov ( X , X + 5 Y ) = \frac{ 5 }{ 4 } + 5 \times \frac{ -1 }{ 4 } = 0 . 
       \]

       La covariance nulle n'implique pas l'indépendance, elle ne permet pas de conclure. Par contre on a 
       \[
       \Ev{X=0} \cap (X + 5 Y = 1 ) = [ \Ev{X=0} \cap \Ev{ 5Y=1 } ] = \emptyset \ \ \text{ donc } \ \ P ( \Ev{X=0} \cap (X + 5 Y  = 1 ) ) = 0 \neq  P \Ev{ X = 0 } P (X + 5 Y = 1) 
       \]

       (car $\Ev{X+5Y=1} = \Ev{X=1} \cap \Ev{Y=0}$ a une probabilité
       non nulle). On en déduit que $X$ et $X+5Y$ ne sont pas
       indépendantes.

     \end{enumerate}

   \item

     On pose : $ Z = \frac{ 1 }{ X + 1 }$. \begin{enumerate}

     \item Par théorème de transfert, on s'intéresse à la convergence absolue de 
       \[
       \Sum{i=0}{+\infty} \frac{ 1 }{ i+1 } \times \frac{ i+1 }{ 2 e i! } = \frac{ 1 }{ 2e } \Sum{i=0}{+\infty} \frac{ 1 }{ i! } 
       \]

       On reconnaît une série exponentielle qui converge absolument donc $Z$ admet une espérance et
       \[
       E ( Z ) = \frac{ 1 }{ 2e } \times e = \frac{ 1 }{ 2 } . 
       \]

     \item Pour tout $i$ fixé dans $\N$, le support de $Y$ sachant $[X=i]$ est $\N$ et : 
       \[
       P_{ \Ev{X=i} } \Ev{Y=j } = \frac{ P [ \Ev{X=i} \cap \Ev{Y=j} }{ P \Ev{ X=i } } = \frac{ \frac{ i+j }{ 2 e^2 i! j! } }{ \frac{ i+1 }{ 2 e i! } } = \frac{ i+j }{ e (i+1) j! } . 
       \]

     \item On calcule d'abord pour tout $i \in \N$ : 
       \begin{eqnarray*}
         g_{ [X = i ] } (Y) & = & \Sum{k=0}{+\infty} k P_{ [X=i ] } \Ev{Y = k } = \frac{ 1 }{ e } \Sum{k=0}{+\infty} k \times \frac{ i + k }{ (i + 1) k! } = \frac{ 1 }{ e (i+1) } \left( i \Sum{k=0}{+\infty} \frac{ k }{ k! } + \Sum{k=0}{+\infty} \frac{ k^2 }{ k! } \right) \\ \\
         & = & \frac{ 1 }{ e (i+1) } \left( 0 + i \Sum{k=1}{+\infty} \frac{ 1 }{ (k-1)! } + 0 + \Sum{k=1}{+\infty}  \frac{ k }{ (k-1)! } \right) =  \frac{ 1 }{ e (i+1) } \left(  i \Sum{k=0}{+\infty} \frac{ 1 }{ k! } + \Sum{k=1}{+\infty}  \frac{ k-1 + 1 }{ (k-1)! } \right) \\ \\
         & = & \frac{ 1 }{ e (i+1) } \left(  i e + \Sum{k=1}{+\infty}  \frac{ k-1 }{ (k-1)! } + \Sum{k=1}{+\infty}  \frac{ 1 }{ (k-1)! } \right) = \frac{ 1 }{ e (i+1) } \left(  i e + 0 + \Sum{k=2}{+\infty}  \frac{ 1 }{ (k-2)! } + \Sum{k=0}{+\infty}  \frac{ 1 }{ k! } \right) \\ \\
         & = & \frac{ 1 }{ e (i+1) } \left(  i e + \Sum{k=0}{+\infty}  \frac{ 1 }{ k! } + e \right) = \frac{ e ( i + 2 ) }{ e (i+1 ) } = \frac{ i + 2 }{ i + 1 } 
       \end{eqnarray*}

       donc pour tout $\omega \in \Omega$ tel que $ X  (\omega ) = i$ on a : 
       \[
       g_{ [ X = X ( \omega) ] } (Y ) = g_{ [ X = i ] } (Y ) = \frac{ i + 2 }{ i + 1 } \ \ \text{ et } \ \ Z ( \omega ) = \frac{ 1 }{ X ( \omega ) + 1 } = \frac{ 1 }{ i + 1 } 
       \]

       donc la fonction $f$ doit vérifier : pour tout $i \in \N$,
       \[
       f \left( \frac{ 1 }{ i + 1 } \right) = \frac{ i + 2 }{ i + 1 } = \frac{ i + 1 + 1 }{ i + 1 } = 1 + \frac{ 1 }{ i + 1 } 
       \]

       et en posant $f(x) = 1 + x$, on a bien pour tout $\omega \in \Omega$,
       \[
       g_{ [X = X (\omega) ] } (Y ) = 1 + \frac{ 1 }{ X ( \omega) + 1 } = 1 + Z ( \omega ) = f ( Z (\omega ) ) . 
       \]

     \end{enumerate}

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\

   \begin{enumerate}
   \item Pas toujours. Par exemple, en posant 
     \[
     A = \begin{smatrix} 1 & 1 \\ 0 & 0 \\ \end{smatrix} \ \ \ \text{ et } \ \ \ B = \begin{smatrix} 0 & 1 \\ 0 & 1 \\ \end{smatrix} \ \ \ \text{ on a } \ \ \ A + B = \begin{smatrix} 1 & 2 \\ 0 & 1 \\ \end{smatrix} 
     \]

     avec $A$ et $B$ diagonalisables (deux valeurs propres distinctes) et $A+B$ qui ne l'est pas (seule valeur propre 1, si elle l'était on aurait $A + B = P I P^{-1} = I$, absurde). \\

   \item Pas forcément non plus. Par exemple en posant $A$ inversible et $B = -A$ qui est aussi inversible (d'inverse $- A^{-1}$), leur somme est la matrice nulle qui n'est pas inversible. \\

   \item Il faut penser à décomposer $A$ en une somme de deux matrices triangulaires : 
     \[
     A = \begin{smatrix} 0 & 0 & \dots & \dots & 0 \\ a_{2,1} & \ddots & \ddots & \dots & \vdots \\ \vdots & \ddots & \ddots & \ddots & \vdots \\ \vdots & \dots & \ddots & \ddots & 0 \\ a_{n,1} & \dots & \dots & a_{n , n-1} & 0 \\ \end{smatrix} + \begin{smatrix} a_{1,1} & a_{1,2} & \dots & \dots & a_{1,n} \\ 0 & \ddots & \ddots & \dots & 0 \\ \vdots & \ddots & \ddots & \ddots & \vdots \\ \vdots & \dots & \ddots & \ddots & a_{n-1,n} \\ 0 & \dots & \dots & 0 & a_{n,n} \\ \end{smatrix} 
     \]

     L'idée est ensuite de rajouter un nombre $a > 0$ sur chaque terme de la diagonale de la première matrice (ce qui la rendra inversible) et de retirer $-a$ sur la diagonale de la seconde matrice pour compenser. Il faut que $ a_{i,i} - a $ soit différent de 0 pour tout $i$, mais comme $\{ a_{i,i} \text{ tq } 1 \leq i \leq n \}$ est fini, il existe forcément un nombre $a$ non nul qui vérifiera $ a_{i,i} - a \neq 0$ pour tout $i$ (il suffit qu'il soit différent de chacun des $a_{i,i}$), et on obtient
     \[
     A = \begin{smatrix} a & 0 & \dots & \dots & 0 \\ a_{2,1} & \ddots & \ddots & \dots & \vdots \\ \vdots & \ddots & \ddots & \ddots & \vdots \\ \vdots & \dots & \ddots & \ddots & 0 \\ a_{n,1} & \dots & \dots & a_{n , n-1} & a \\ \end{smatrix} + \begin{smatrix} a_{1,1} - a & a_{1,2} & \dots & \dots & a_{1,n} \\ 0 & \ddots & \ddots & \dots & 0 \\ \vdots & \ddots & \ddots & \ddots & \vdots \\ \vdots & \dots & \ddots & \ddots & a_{n-1,n} \\ 0 & \dots & \dots & 0 & a_{n,n} - a \\ \end{smatrix} 
     \]

     qui est bien la somme de deux matrices inversibles.

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   On note $\mathcal{M}_3 (\R)$ l'ensemble des matrices carrées d'ordre 3 et $\R_2 [X]$ l'ensemble des polynômes à coefficients réels de degré inférieur ou égal à 2. \\ \\
   Dans tout l'exercice, $A$ est une matrice de $\mathcal{M}_3 (\R)$ ayant trois valeurs propres distinctes, notées $\lambda_1$, $\lambda_2$ et $\lambda_3$. \\
   \begin{noliste}{1.}
   \item Un polynôme $P$ est dit annulateur d'une matrice $A$ si $P(A) = 0$. On sait alors que toutes les valeurs propres de $A$ sont racines de $P$, ou encore : 
     \[
     \spc ( A ) \subset \left\{ \text{ racines de } P \rule{0cm}{0.3cm} \right\} 
     \]

   \item \begin{enumerate}

     \item $A$ est une matrice d'ordre 3 qui admet trois valeurs propres distinctes, elle est donc diagonalisable et plus exactement semblable à la matrice
       \[
       D = \begin{smatrix} \lambda_1 & 0 & 0 \\ 0 & \lambda_2 & 0 \\ 0 & 0 & \lambda_3 \\ \end{smatrix} 
       \]

       qui vérifie facilement que $ ( D - \lambda_1 I ) ( D - \lambda_2 I ) ( D - \lambda_3 I ) = 0$ et par similitude de $A$ et $D$, on a alors
       \[
       ( A - \lambda_1 I ) ( A - \lambda_2 I ) ( A - \lambda_3 I ) = 0 
       \]

       donc le polynôme $P(X) = ( X - \lambda_1 ) ( X - \lambda_2 ) ( X - \lambda_3 )$, de degré 3, est annulateur de $A$. \\

     \item Ce polynôme admettrait au plus 1 ou deux racines, ce qui est absurde puisque, annulateur de $A$, il devrait admettre chacune des valeurs propres de $A$ pour racines, donc au minimum 3 racines. Il ne peut donc pas exister de polynôme annulateur de $A$ de degré 1 ou 2. \\

     \end{enumerate}

   \item Soit $\varphi$ l'application de $\R_2 [X]$ dans $\R^3$ qui à tout polynôme $P \in \R_2 [X]$, associe le triplet $\big( P (\lambda_1^5) , P (\lambda_2^5) , P( \lambda_3^5 ) \big)$. \begin{enumerate}

     \item On se donne $P$ et $Q$ deux polynômes de $\R_2 [X]$ et $\lambda$ un réel, on a :
       \begin{eqnarray*}
         \varphi ( \lambda P + Q ) & = & \left( \rule{0cm}{0.4cm} ( \lambda P + Q ) (\lambda_1^5 ) , ( \lambda P + Q ) (\lambda_2^5 ) , ( \lambda P + Q ) (\lambda_3^5 ) \right) \\ \\
         & = & \left( \rule{0cm}{0.4cm} \lambda P (\lambda_1^5 ) + Q  (\lambda_1^5 ) , \lambda P (\lambda_2^5 ) + Q (\lambda_2^5 ) , \lambda P (\lambda_3^5 ) + Q  (\lambda_3^5 ) \right) \\ \\
         & = & \lambda \left( \rule{0cm}{0.4cm}  P (\lambda_1^5 ) , P (\lambda_2^5 )  , P (\lambda_3^5 )  \right) + \left( \rule{0cm}{0.4cm}  Q (\lambda_1^5 ) , Q (\lambda_2^5 )  , Q (\lambda_3^5 )  \right) \\ \\
         & = & \lambda \varphi (P) + \varphi (Q) 
       \end{eqnarray*}

       donc $\varphi$ est bien linéaire. \\

     \item On résout l'équation $\varphi (P ) = 0$ : 
       \begin{eqnarray*}
         \varphi (P ) = 0 & \Longleftrightarrow & P ( \lambda_1^5 ) = P ( \lambda_2^5 ) = P ( \lambda_3^5 ) = 0 
       \end{eqnarray*}

       Or les $\lambda_i$ sont distincts et la fonction x $\mapsto x^5$ est bijective, avec une dérivée $5 x^4$ strictement positive (sauf en 0 où elle est nulle) donc elle est continue et strictement croissante. \\

       On en déduit que les $\lambda_i^5$ sont distincts, donc $P$ admet trois racines distinctes et il est de degré inférieur ou égal à 2 : c'est impossible à moins que $P$ soit nul donc : 
       \[
       \varphi (P ) = 0 \Longleftrightarrow P = 0 
       \]

       et $\ker ( \varphi ) = \{ 0 \}$. \\

     \item Cette application est injective, et le théorème du rang assure que : 
       \[
       \dim ( \Im \varphi ) = \dim ( \R_2 [X ] ) - \dim ( \ker \varphi ) = 3 - 0 = 3 = \dim ( \R^3 ) 
       \]

       donc $\varphi$ est surjective, c'est donc une application linéaire bijective, soit un isomorphisme de $\R_2 [X]$ dans $\R^3$. \\

     \item $\varphi$ est bijective et $ (\lambda_1 , \lambda_2 , \lambda_3 ) \in \R^3$, ensemble d'arrivée de $\varphi$. On en déduit qu'il existe un unique polynôme $Q \in \R_2 [X]$ tel que : 
       \[
       \varphi (Q) = ( \lambda_1 , \lambda_2 , \lambda _3 ) \Longleftrightarrow \left\{ \begin{array}{cl} Q ( \lambda_1^5 ) = Q ( \lambda_1 ) \\ Q ( \lambda_2^5 ) = Q ( \lambda_2 ) \\ Q ( \lambda_3^5 ) = Q ( \lambda_3 ) \\ \end{array} \right. 
       \]

     \item On sait que $A$ est semblable à $D$, donc il suffit de prouver que $T$ est annulateur de $D$. Or un polynôme en une matrice diagonale se calcule en calculant les images des chaque valeur diagonale par le polynôme, donc : 
       \[
       T ( D ) = \begin{smatrix} T ( \lambda_1 ) & 0 & 0 \\ 0 & T ( \lambda_2 ) & 0 \\ 0 & 0 & T ( \lambda_3 ) \\ \end{smatrix} = \begin{smatrix} Q ( \lambda_1^5 ) - \lambda_1 & 0 & 0 \\ 0 & Q ( \lambda_2^5 ) - \lambda_2 & 0 \\ 0 & 0 & Q ( \lambda_3^5 ) - \lambda_3 \\ \end{smatrix} = 0 
       \]

       d'après la définition de $Q$ dans la question 3d. \\

       On en déduit que $T ( A ) = 0$, donc $T$ est annulateur de $A$. \\

     \end{enumerate}

   \item Montrons la double-inclusion : si $N \in \mathcal{E}$, alors $A N = N A$ donc : 
     \[
     A^5 N = A A A A A N = A A A A N A = A A A N A A = \cdots ) N A A A A A = N A^5 
     \]

     donc $N \in \mathcal{F}$, donc $\mathcal{E} \subset \mathcal{F}$. \\

     Supposons à présente que $N \in \mathcal{F}$, donc que $ A^5 N = N A^5 $, et on veut prouver que $A N = N A$. \\

     Or la question 3e montre que $T (A) = Q ( A^5 ) - A = 0$, donc $A = Q ( A^5)$. \\

     Alors en posant $Q (X) = a + b X + c X^2$ (il est de degré inférieur ou égal à 2), montrons que $Q ( A^5 ) N = N A Q (A^5 )$ : 
     \begin{eqnarray*}
       Q ( A^5 ) N & = & ( a I + b A^5 + c ( A^5)^2 ) N = ( a I + b A^5 + c A^5 A^5 ) N = a N + b A^5 N + c A^5 A^5 N = a N + b N A^5 + c A^5 N A^5 \\ \\
       & = & a N + b N A^5 + c N A^5 A^5 = N ( a I + b A^5 + c (A^5)^2 ) = N Q ( A^5 ) 
     \end{eqnarray*}

     donc on obtient bien $A N = N A$, et $\mathcal{F} \subset \mathcal{E}$, et finalement par double-inclusion : 
     \[
     \mathcal{E} = \mathcal{F} 
     \]

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $(X_n)_{ n \in \N^* }$ une suite de variables aléatoires définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$, indépendantes et de même loi exponentielle de paramètre $\lambda > 0$. \\
   Pour $n \in \N^*$, on pose : $M_n = \max ( X_1 , \dots , X_n)$ et on admet que $M_n$ est une variable aléatoire définie sur $(\Omega , \mathcal{A} , P)$.
   \begin{enumerate}
   \item Le maximum de $n$ variables est inférieur ou égal à $x$ si et seulement si elles le sont toutes, donc pour tout $x \in \R$, par indépendance des $X_i$ :
     \[
     \Ev{ M_n \leq x } = \bigcap\limits_{ i=1 }^n \Ev{X_i \leq x } \ \ \ \text{ puis } \ \ \ F_{ M_n } (x) = [ F(x) ]^n 
     \]

     où $F$ est la fonction de répartition commune des $X_i$ qui suivent la même loi. Cette fonction est continue sur $\R$ et de classe $C^1$ sauf en 0 car $F$ l'est (les $X_i$ sont à densité), donc $M_n$ admet une densité qu'on obtient en dérivant sa fonction de répartition sauf en 0, valeur arbitraire :
     \[
     f_{ M_n } (x) = n f(x) [ F(x) ]^{n-1} = \left\{ \begin{array}{cl} 0 & \text{ si } x < 0 \\ n \lambda e^{ - \lambda x } \left( 1 - e^{ - \lambda x } \right)^{ n-1 } & \text{ si } x \geq 0 \\ \end{array} \right. 
     \]

   \item Cette fonction est positive et continue sur $\R$ par opérations élémentaires, et on calcule $\int_{-\infty}^{+\infty} g(t) \ dt$ : 
     \begin{eqnarray*}
       \int_a^0 g(t) \ dt & = & \left[ e^{ - e^{ - t } } \right]_a^0 = e^{ - e^0 } - e^{ - e^{ - a } } = e^{ - 1 } - e^{ - e^{ - a } } \\ \\
       & \xrightarrow[ a \rightarrow - \infty ]{} & e^{ - 1 } - 0 = e^{ - 1 } 
     \end{eqnarray*}

     d'une part, et d'autre part
     \begin{eqnarray*}
       \int_0^b g(t) \ dt & = & \left[ e^{ - e^{ - t } } \right]_0^b = e^{ - e^{ - b } } - e^{ - e^0 } = e^{ - e^{ - b } } - e^{ - 1 } \\ \\
       & \xrightarrow[ b \rightarrow - \infty ]{} & 1 - e^{ - 1 } = 1 - e^{ - 1 } 
     \end{eqnarray*}

     donc $\int_{-\infty}^{+\infty} g(t) \ dt$ et vaut 1, et $g$ est une densité de probabilité. \\

   \item On cherche les fonctions de répartitions de $Y$ et $ Y_n = \lambda M_n - \ln n$, puis on montre que la seconde converge vers la première quand $n$ tend vers $+\infty$, pour tout $x \in \R$ (car $F_Y$ est continue sur $\R$) : pour tout $x \in \R$,
     \begin{eqnarray*}
       \int_a^x g(t) \ dt & = & \left[ e^{ - e^{ - t } } \right]_a^x = e^{ - e^{ - x } } - e^{ - e^{ - a } }  \\ \\
       & \xrightarrow[ a \rightarrow - \infty ]{} & e^{ - e^{ - x } } - 0 = e^{ - 1 } = \int_{- \infty}^x g(t) \ dt = F_Y (x) .
     \end{eqnarray*}

     D'autre part, pour tout $x \in \R$, on a : 
     \[
     \Ev{ Y_n \leq x } = ( \lambda M_n - \ln n \leq x ) = ( \lambda M_n \leq x + \ln n ) = \left( M_n \leq \frac{ x + \ln n }{ \lambda } \right) 
     \]

     donc
     \[
     F_{ Y_n } (x) = F_{ M_n } \left( \frac{ x + \ln n }{ \lambda } \right) = \left[ F \left( \frac{ x + \ln n }{ \lambda } \right) \right]^n . 
     \]

     On cherche la limite quand $n$ tend vers $+\infty$ de cette fonction, avec $x$ fixé. \\

     Comme $ \frac{ x + \ln n }{ \lambda }$ tend vers $+\infty$ lorsque $n$ tend vers $+\infty$, pour $n$ assez grand, il est toujours positif donc on peut utiliser l'expression sur $[0 ; +\infty[$ de $F$ : 
     \begin{eqnarray*}
       F_{ Y_n } (x) & = & \left( 1 - e^{ - \lambda \frac{ x + \ln n }{ \lambda } } \right)^n = \left( 1 - e^{ - x - \ln n } \right)^n = e^{ n \ln \left( 1 - e^{ - x - \ln n } \right) } 
     \end{eqnarray*}

     et avec $- e^{ - x - \ln n }$ qui tend vers 0 en $+\infty$, on peut appliquer le DL de $\ln (1+u)$ : 
     \begin{eqnarray*}
       F_{ Y_n } (x) & = & e^{ n \ln \left( 1 - e^{ - x - \ln n } \right) } = e^{ n \left( - e^{ - x - \ln n } + o ( e^{ - x - \ln n } ) \right) } = e^{ - \frac{ n e^{ - x } }{ e^{ \ln n } } + o \left( \frac{ n e^{ - x } }{ e^{ \ln n } } \right) } = e^{ - e^{ - x }  + o ( 1 ) } \\ \\
       & \xrightarrow[ n \rightarrow + \infty ]{} & e^{ - e^{ - x } } = F_Y (x) 
     \end{eqnarray*}

     et comme cette limite est vraie pour tout $x \in \R$, $(Y_n)$ converge en loi vers $Y$.

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une série numérique de terme général $(u_n)_{ n \geq n_0}$ est dite convergente si la suite des sommes partielles $(S_p)_{ p \geq n_0}$ définie par : 
     \[
     \forall p \geq n_0 , \ S_p = \Sum{n=n_0}{p} u_n 
     \]

     est convergente. \\

     \textit{Dans tout l'exercice}, $a$ \textit{est un réel strictement supérieur à 1}. \\

   \item \begin{enumerate}

     \item La fonction intégrée est continue et positive sur $[0 ; +\infty[$ donc l'intégrale n'est généralisée qu'en $+\infty$, et les théorèmes de comparaison s'appliquent. \\

       On cherche un équivalent en $+\infty$ de la fonction intégrée : 
       \[
       \frac{ 1 }{ (1+t^a )^n } = \frac{ 1 }{ \left[ t^a \left( 1 + \frac{ 1 }{ t^a } \right) \right]^n } = \frac{ 1 }{ t^{ a n } } \times \frac{ 1 }{ \left( 1 + \frac{ 1 }{ t^a } \right)^n } 
       \]

       et comme $a \geq 1$, la deuxième fraction tend vers 1 lorsque $t$ tend vers $+\infty$, donc : 
       \[
       \frac{ 1 }{ (1+t^a )^n } \underset{ t \rightarrow + \infty }{ \sim } \frac{ 1 }{ t^{ n a } } 
       \]

       Les deux fonctions sont positives, et l'intégrale de la seconde converge en $+\infty$ (Riemann, avec $n a > 1$ car $a >1$ et $n \geq 1$) donc par théorème de comparaison, l'intégrale de la première converge en $+\infty$, et $u_n (a) = \int_0^{+\infty} \frac{ 1 }{ (1+t^a )^n } \ dt $ existe bien. \\

     \item La suite est clairement minorée par 0 (intégrale d'une fonction positive avec des bornes dans l'ordre croissant). On cherche le sens de variation de $(u_n (a) )_n \in \N^*$ : 
       \[
       u_{n+1} (a) - u_n (a) = \int_0^{+\infty} \frac{ 1 - (1 + t^a ) }{ (1+t^a)^{n+1} } \ dt = - \int_0^{+\infty} \frac{ t^a }{ (1+t^a)^{n+1} }  \ dt \leq 0 
       \]

       car l'intégrale est positive (fonction intégrée positive et bornes dans l'ordre croissant). \\

       Le suite $(u_n(a))_{ n \geq 1 }$ est décroissante et minorée par 0 donc elle est convergente. \\

     \end{enumerate}

   \item \begin{enumerate}

     \item On part du côté droit, plus compliqué : 
       \[
       a n ( u_n (a) - u_{n+1} (a) ) = a n \int_0^{+\infty} \frac{ t^a }{ ( 1+t^a )^{ n + 1 } } \ dt  = n \int_0^{+\infty} t \times \frac{ a t^{ a-1 } }{ ( 1+t^a )^{ n + 1 } } \ dt . 
       \]

       On réalise une intégration par parties sur l'intégrale partielle en posant
       \[
       u = t \ \ \ \text{ et } \ \ \ v = \frac{ 1 }{ - n ( 1+t^a )^n } 
       \]

       qui sont de classe $C^1$, avec
       \[
       u' = 1 \ \ \ \text{ et } \ \ \ v' = \frac{ a t^{ a-1 } }{ ( 1+t^a )^{ n + 1 } } 
       \]

       donc on obtient : 
       \begin{eqnarray*}
         \int_0^M t \times \frac{ a t^{ a-1 } }{ ( 1+t^a )^{ n + 1 } } \ dt  & = & \left[ \frac{ t }{ - n ( 1+t^a )^n } \right]_0^M + \int_0^M \frac{ 1 }{ n ( 1+t^a )^n } \ dt  \\ \\
         & = & - \frac{ M }{ n ( 1+M^a )^n } + 0 + \frac{1}{n} \int_0^M \frac{ 1 }{ ( 1+t^a )^n } \ dt \\ \\
         & = & - \frac{ M }{ M^{ a n } \left( 1 + \frac{ 1 }{ M^a } \right)^n } + \frac{1}{n} \int_0^M \frac{ 1 }{ ( 1+t^a )^n } \ dt \\ \\
         & = & - \frac{ 1 }{ M^{ a n - 1 } \left( 1 + \frac{ 1 }{ M^a } \right)^n } + \frac{1}{n} \int_0^M \frac{ 1 }{ ( 1+t^a )^n } \ dt \\ \\
         & \xrightarrow[ M \rightarrow + \infty ]{} & 0 + \frac{ 1 }{ n } u_n (a) 
       \end{eqnarray*}

       donc on en déduit que : 
       \[
       a n ( u_n (a) - u_{n+1} (a) ) = n \int_0^{+\infty} t \times \frac{ a t^{ a-1 } }{ ( 1+t^a )^{ n + 1 } } \ dt = n \times \frac{ 1 }{ n } u_n (a) = u_n (a) . 
       \]

       On isole alors $u_{n+1} (a)$ dans cette égalité pour obtenir une relation de récurrence : 
       \[
       u_n (a) - u_{n+1} (a) = \frac{ u_n (a ) }{ a n } \ \ \text{ donc } \ \ u_{n+1} (a) = \left( 1 - \frac{ 1 }{ a n } \right) u_n (a) . 
       \]

       On peut alors itérer cette relation : 
       \begin{eqnarray*}
         u_n (a) & = & \left( 1 - \frac{ 1 }{ a (n-1) } \right) u_{n-1} (a) = \left( 1 - \frac{ 1 }{ a (n-1) } \right) \left( 1 - \frac{ 1 }{ a (n-2) } \right) u_{n-2} (a) = \dots  \\ \\
         & = & \left( 1 - \frac{ 1 }{ a (n-1) } \right) \left( 1 - \frac{ 1 }{ a (n-2) } \right) \dots \left( 1 - \frac{ 1 }{ a \times 1 } \right) u_1 (a) \\ \\
         & = & \left( \prod\limits_{i=1}^{n-1} \frac{ a i - 1 }{ a i } \right) u_1 = \frac{ u_1 }{ a^{n-1} (n-1)! } \prod\limits_{i=1}^{n-1} (a i - 1 ) . 
       \end{eqnarray*}

     \item On se sert d'une égalité vue plus haut : 
       \[
       \frac{ u_n (a) }{ a_n } = u_n (a) - u_{n+1} (a) 
       \]

       qu'on somme pour faire apparaître un télescopage sur la somme partielle : 
       \[
       \Sum{n=1}{p} \frac{ u_n (a) }{ a n } = \Sum{n=1}{p} [ u_n (a) - u_{n+1} (a) ] = u_n (1) - u_{p+1} (a) \xrightarrow[ p \rightarrow + \infty ]{} u_n (1 ) - \dlim{ k \rightarrow + \infty } u_k (a) 
       \]

       qui est une constante réelle puisque la suite $(u_n(a) )_{ n \geq 1}$ est convergente. On en déduit que la série de terme général $ \left( \frac{ u_n (a) }{ a n } \right) $ est convergente. \\

     \item On sait que la suite $(u_n (a))_{ n \geq 1 }$ converge vers un réel positif $\ell$. Si $\ell \neq 0$, alors on en déduit que
       \[
       \frac{ u_n (a) }{ a n } \underset{ + \infty }{ \sim } \frac{ \ell }{ a } \times \frac{ 1 }{ n } 
       \]

       et par théorème de comparaison, la série de terme général $\left( \frac{ u_n(a) }{ a n } \right)$ diverge. C'est absurde avec ce qu'on vient de montrer, donc la limite de $(u_n (a) )$ est forcément 0. \\

     \end{enumerate}

   \item On pose pour tout $n \in \N^*$ : $w_n (a) = \ln \big( u_n (a) \big) + \frac{ \ln n }{ a }$. \begin{enumerate}

     \item On commence par calculer $w_{n+1} (a) - w_n (a)$ : 
       \begin{eqnarray*}
         w_{n+1} (a) - w_n (a) & = & \ln \big( u_{n+1} (a) \big) + \frac{ \ln (n+1) }{ a } - \ln \big( u_n (a) \big) - \frac{ \ln n }{ a } = \ln \left( \frac{ u_{n+1} (a) }{ u_n (a) } \right) + \frac{ \ln \left( \frac{ n + 1 }{ n } \right) }{ a } \\ \\
         & = & \ln \left( \frac{ \frac{ u_1 }{ a^n n! } \prod\limits_{i=1}^n (a i - 1 ) }{ \frac{ u_1 }{ a^{n-1} (n-1)! } \prod\limits_{i=1}^{n-1} (a i - 1 ) } \right) + \frac{ \ln \left( 1 + \frac{ 1 }{ n } \right) }{ a } \\ \\
         & = &  \ln \left( \frac{ a n - 1 }{ a n } \right) + \frac{ \ln \left( 1 + \frac{ 1 }{ n } \right) }{ a } = \ln \left( 1 - \frac{ 1 }{ a n } \right) + \frac{ \ln \left(  1 + \frac{ 1 }{ n } \right) }{ a } . 
       \end{eqnarray*}

       Il faut alors penser à réaliser un DL pour retirer le logarithme et faire apparaitre des séries de Riemann : 
       \begin{eqnarray*}
         w_{n+1} (a) - w_n (a) & = &  - \frac{ 1 }{ a n } + \frac{ 1 }{ 2 (a n)^2 } + o \left( \frac{ 1 }{ n^2 } \right) + \frac{ 1 }{ a } \left( \frac{ 1 }{ n } - \frac{ 1 }{ 2 n^2 } + o \left( \frac{ 1 }{ n^2 } \right) \right) \\ \\
         & = & - \frac{ 1 }{ a n } + \frac{ 1 }{ 2 (a n)^2 } + o \left( \frac{ 1 }{ n^2 } \right) + \frac{ 1 }{ a n } - \frac{ 1 }{ 2 a n^2 } + o \left( \frac{ 1 }{ n^2 } \right) \\ \\
         & = & - \frac{ 1 }{ a n } + \frac{ 1 }{ 2 a^2 n^2 } + o \left( \frac{ 1 }{ n^2 } \right) + \frac{ 1 }{ a n } - \frac{ 1 }{ 2 a n^2 } + o \left( \frac{ 1 }{ n^2 } \right) \\ \\
         & = & \frac{ 1 - a }{ 2 a^2 n^2 } + o \left( \frac{ 1 }{ n^2 } \right) \\ \\
         & \sim & \frac{ 1 - a }{ 2 a^2 n^2 } 
       \end{eqnarray*}

       et comme l'équivalent est à termes négatifs (avec $a > 1$), au voisinage de l'infini, c'est aussi le cas de la suite de départ. Le théorème de comparaison s'applique et comme le deuxième terme général est celui d'une série convergente (Riemann, $2> 1$), la série de terme général $(w_{n+1} (a) - w_n (a) )$ est convergente. \\

     \item On calcule la somme partielle de la série précédente (qui est donc convergente) : 
       \[
       \Sum{n=1}{p} w_{n+1} (a) - w_n (a) = w_{p+1} (a) - w_1 (a) 
       \]

       qui converge si et seulement si la suite $(w_p)$ converge, donc celle-ci converge. \\

       On en déduit qu'il un réel $\ell (a)$ tel que : 
       \[
       w_n (a) = \ln \big( u_n (a) \big) + \frac{ \ln n }{ a } \xrightarrow[ n \rightarrow + \infty ]{} K 
       \]

       donc en isolant $u_n$ : 
       \[
       \ln \big( u_n (a) \big) =w_n (a)- \frac{ \ln n }{ a } \ \ \text{ puis } \ \ u_n (a) = e^{ w_n (a)- \frac{ \ln n }{ a } } = \frac{ e^{ w_n (a) } }{ n^{ \frac{1}{a} } } 
       \]

       et avec $w_n(a)$ qui tend vers $\ell (a)$, le numérateur tend vers $K (a) = e^{ \ell (a ) } \neq 0$ donc il lui est équivalent, et enfin : 
       \[
       u_n (a) \underset{ n \rightarrow + \infty }{ \sim } \frac{ K(a) }{ n^{ \frac{1}{a} } } . 
       \]

     \end{enumerate}

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Les variables aléatoires sont définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$. \\
   Soit $X$ une variable aléatoire qui suit la loi de Poisson de paramètre $\lambda > 0$ et soit $Y$ une variable aléatoire indépendante de $X$ telle que : $ Y ( \Omega ) = \{ 1 ; 2 \} , P \Ev{ Y = 1 } = P \Ev{ Y = 2 } = \frac{ 1 }{ 2 }$. On pose $Z = X Y$.
   \begin{enumerate}

   \item Lorsque $Y=1$, $Z = X Y$ prend toutes les valeurs de $\N$; lorsque $Y = 2$, $Z = X Y$ prend toutes les valeurs paires de $\N$. Finalement on obtient : 
     \[
     Z ( \Omega ) = \N . 
     \]

     Si $k = 2 j + 1$ est impair, il ne peut être atteint qu'avec $Y=1$ donc (avec $X$ et $Y$ indépendantes) : 
     \[
     \forall j \in \N , \ ( Z = 2 j + 1 ) = (X = 2 j + 1 ) \cap \Ev{Y = 1 } \ \ \text{ et } \ \ P ( Z = 2 j + 1 ) = \frac{ 1 }{ 2 } P ( X = 2 j + 1 ) = \frac{ e^{ - \lambda } \lambda^{ 2j + 1 } }{ 2 (2j+1)! } 
     \]

     Si  $k = 2j$ est pair, il peut être atteint avec $Y=1$ ou 2, donc : 
     \[
     \forall j \in \N , \ \Ev{ Z = 2 j } = [ \Ev{ X = 2j } \cap \Ev{ Y = 1 } ] \cup [ \Ev{ X = j } \cap \Ev{ Y = 2 } ] 
     \]

     et
     \[
     P \Ev{ Z = 2j } = \frac{ P \Ev{ X = 2j } + P \Ev{ X = j } }{ 2 } = \frac{ e^{ - \lambda } }{ 2 } \left( \frac{ \lambda^{ 2j } }{ (2j)! } + \frac{ \lambda^j }{ j! } \right) . 
     \]

   \item On décompose : 
     \[
     (Z \text{ est paire } ) = \bigcup\limits_{j=0}^{+\infty} \Ev{Z = 2j } 
     \]

     avec une union incompatible donc : 
     \begin{eqnarray*}
       P ( Z \text{ est paire } ) & = & \Sum{j=0}{+\infty} \left[ \frac{ e^{ - \lambda } }{ 2 } \left( \frac{ \lambda^{ 2j } }{ (2j)! } + \frac{ \lambda^j }{ j! } \right) \right] \\ \\
       & = & \frac{ e^{ - \lambda } }{ 2 } \left[ \Sum{j=0}{+\infty} \frac{ \lambda^{ 2j } }{ (2j)! } + \Sum{j=0}{+\infty} \frac{ \lambda^j }{ j! } \right] \\ \\
       & = & \frac{ e^{ - \lambda } }{ 2 } \left[ \frac{ e^{ \lambda } + e^{ - \lambda } }{ 2 } + e^{ \lambda } \right] = \frac{ 1 + e^{ - 2 \lambda } + 2 }{ 4 } = \frac{ 3 + e^{ - 2 \lambda } }{ 4 } . 
     \end{eqnarray*}

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une intégrale impropre en un point est dite convergente si l'intégrale partielle admet une limite finie. Si elle est impropre en ses deux bornes, elle est dit convergente si, en posant $c$ un point situé entre les deux bornes $a$ et $b$, les intégrales $\int_a^c f$ et $\int_b^c f$ sont convergentes. \\

     Enfin l'intégrale d'une fonction positive impropre au point $a$ est convergente lorsque : \begin{itemize}

     \item l'intégrale partielle est majorée (condition nécessaire et suffisante). \\

     \item $f \leq g$ au voisinage du point $a$, et l'intégrale de $g$ est convergente (condition suffisante). \\

     \item $ f = o(g)$ au voisinage du point $a$, où $g$ est positive et l'intégrale de $g$ est convergente (condition suffisante). \\

     \item $ \sim g$ su voisinage du point $a$, où $g$ est positive et l'intégrale de $g$ est convergente (condition nécessaire et suffisante). \\

     \end{itemize}

     Enfin lorsque $f$ n'est pas de signe constant, on a un dernier critère : si l'intégrale est absolument convergente (avec possibilité d'utiliser les théorèmes de comparaison ci-dessus), elle est convergente (condition suffisante). \\

     Soir $T$ une variable aléatoire définie sur un espace probabilisé $(\Omega , \mathcal{A} , P)$, suivant la loi normale centrée réduite. On note $\Phi$ et $\varphi$ respectivement, la fonction de répartition et une densité de $T$. \\ 

   \item \begin{enumerate}

     \item L'inégalité de Bienaymé-Chebychev donne, pour $X$ une variable aléatoire qui admet une variance et $a$ un nombre strictement positif : 
       \[
       P ( \vert X - E ( X ) \vert > a ) \leq \frac{ V ( X ) }{ a^2 } . 
       \]

       En l'appliquant à la variable $X$, on obtient : 
       \[
       P ( \vert X - 0 \vert > a ) \leq \frac{ 1 }{ a^2 } \ \ \text{ donc } \ \ P ( \vert X \vert > a ) \leq \frac{ 1 }{ a^2 } . 
       \]

       On calcule alors cette probabilité en faisant apparaitre $\Phi$ : 
       \begin{eqnarray*}
         P ( \vert X \vert > a ) & = & P \Ev{ X > a } + P \Ev{ X < - a } = 1 - P \Ev{ X \leq a } + P \Ev{ X \leq - a } = 1 - \Phi ( a) + \Phi ( - a ) \\ \\
         & = & 1 - \Phi ( a ) + 1 - \Phi ( a )= 2 [ 1 - \Phi ( a) ] . 
       \end{eqnarray*}

       On remplace $a$ par $x$ et on divise l'inégalité par $2 > 0$ : 
       \[
       \frac{ P ( \vert X \vert > a ) }{ 2 } =  1 - \Phi ( x) \leq \frac{ 1 }{ 2 x^2 } . 
       \]

       Enfin l'autre côté est immédiat, puisque : 
       \[
       1 - \Phi ( x) = 1 - P \Ev{ X \leq x } = P \Ev{ X > x } > 0 . 
       \]

       On obtient finalement : 
       \[
       0 < 1 - \Phi (x) < \frac{ 1 }{ x^2 } . 
       \]

     \item C'est l'intégrale d'une fonction positive, et la fonction est majorée par $\frac{1}{x^2}$. Enfin l'intégrale de $\frac{1}{x^2}$ converge (Riemann avec $\alpha > 1$) en $+\infty$, donc par théorème de comparaison $\int_0^{+\infty} (1 - \Phi (x) ) \ dx$ converge. \\

       Pour calculer sa valeur, on revient à l'intégrale partielle, mais on ne sait pas primitiver $\Phi$ : on va donc procéder à une intégration par parties pour la faire disparaître : on pose
       \[
       u = 1 - \Phi (x) \ \ \text{ et } \ \ v = x 
       \]

       qui sont de classe $C^1$ ($\Phi$ car sa dérivée $\varphi$ est continue sur $\R$) avec : 
       \[
       u' = - \varphi (x) \ \ \text{ et } \ \ v' = 1 . 
       \]

       On obtient : 
       \[
       \int_0^A ( 1 - \Phi (x) ) \ dx = \left[ \rule{0cm}{0.4cm} x ( 1 - \Phi (x) ) \right]_0^A + \int_0^A x \varphi (x) \ dx = A ( 1 - \Phi ( A) ) + \int_0^A x \varphi (x) \ dx . 
       \]

       Pour la première partie, on se sert de la question a pour l'encadrer : 
       \[
       0 \leq 1 - \varphi (A) \leq \frac{ 1 }{ A^2 } \ \ \text{ donc } \ \ 0 \leq A ( 1 - \varphi (A ) ) \leq \frac{ 1 }{ A } 
       \]

       et par théorème d'encadrement, avec les deux termes extrémaux qui tendent vers 0, 
       \[
       \dlim{ A \rightarrow + \infty } A ( 1 - \varphi (A) ) = 0 . 
       \]

       L'autre terme est une intégrale qu'on sait calculer : 
       \[
       \int_0^A x \varphi (x) \ dx = \frac{ 1 }{ \sqrt{ 2 \pi } } \int_0^A x e^{ - \frac{ x^2 }{ 2 } } \ dx = \frac{ 1 }{ \sqrt{ 2 \pi } } \left[ - e^{ - \frac{ x^2 }{ 2 } } \right]_0^A = \frac{ 1 }{ \sqrt{ 2 \pi } } \left( 1 - e^{ - \frac{ A^2 }{ 2 } } \right) \xrightarrow[ A \rightarrow + \infty ]{} \frac{ 1 }{ \sqrt{ 2 \pi } } . 
       \]

       Enfin on en déduit que : 
       \[
       \int_0^{+\infty} ( 1 - \Phi (x) ) \ dx = \frac{ 1 }{ \sqrt{ 2 \pi } } . 
       \]

     \end{enumerate}

   \item On note $\varphi'$ la dérivée de $\varphi$. \begin{enumerate}

     \item On calcule sans difficulté : 
       \[
       \varphi' (x) = \frac{ 1 }{ \sqrt{ 2 \pi } } \times \left( - x e^{ - \frac{ x^2 }{ 2 } } \right) = - x \varphi (x) . 
       \]

     \item L'énoncé parle d'IPP, il faut donc faire apparaître une intégrale : c'est $\Phi$ qui va nous le permettre : 
       \[
       1 - \Phi (x) = 1 - P \Ev{ X \leq x } = P \Ev{ X > x } = \int_x^{+\infty} \varphi (t) \ dt . 
       \]

       Pour faire apparaitre des $\frac{ 1 }{ x }$, on pense à utiliser la question précédente en remplaçant $ \varphi (t) = - \frac{ 1 }{ t } \varphi' (t) $ : 
       \[
       1 - \Phi (x) = \int_x^{ + \infty } - \frac{1}{t} \varphi' (t) \ dt . 
       \]

       On intègre par parties avec : 
       \[
       u = - \frac{ 1 }{ t } \ \ \text{ et } \ \ v = \varphi (t) 
       \]

       de classe $C^1$, avec
       \[
       u' = \frac{ 1 }{ t^2 } \ \ \text{ et } \ \ v' = \varphi' (t) . 
       \]

       On obtient : 
       \[
       \int_x^M - \frac{1}{t} \varphi' (t) \ dt =  - \frac{ \varphi (M ) }{ M } + \frac{ \varphi (x) }{ x } - \int_x^M \frac{ \varphi (t) }{ t^2 } \ dt . 
       \]

       On fait tendre $M$ vers $+\infty$, on obtient : 
       \[
       1 - \Phi (x) =  0 + \frac{ \varphi (x) }{ x } - \int_x^{+\infty} \frac{ \varphi (t) }{ t^2 } \ dt . 
       \]

       Enfin comme l'intégrale est positive (fonction positive et bornes dans l'ordre croissant) : 
       \[
       1 - \Phi ( x)  \leq \frac{ \varphi (x) }{ x } \ \ \text{ donc } \ \ \frac{ 1 - \Phi ( x ) }{ \varphi (x) } \leq \frac{ 1 }{ x } . 
       \]

       Pour l'autre inégalité, on va de nouveau remplacer $\varphi (t) = - \frac{ 1 }{ t } \varphi' (t)$ : 
       \[
       1 - \Phi (x) =  \frac{ \varphi (x) }{ x } + \int_x^{+\infty} \frac{ \varphi' (t) }{ t^3 } \ dt 
       \]

       Une nouvelle IPP donne : 
       \[
       u = \frac{ 1 }{ t^3 } \ \ \text{ et } \ \ v = \varphi (t) 
       \]

       de classe $C^1$, avec
       \[
       u' = - \frac{ 3 }{ t^2 } \ \ \text{ et } \ \ v' = \varphi' (t) . 
       \]

       On obtient : 
       \[
       \int_x^M \frac{1}{t^3} \varphi' (t) \ dt =  \frac{ \varphi (M ) }{ M^3 } - \frac{ \varphi (x) }{ x^3 } + \int_x^M \frac{  3\varphi (t) }{ t^2 } \ dt . 
       \]

       On passe à la limite et on obtient : 
       \[
       1 - \Phi (x) = \frac{ \varphi (x) }{ x } + 0 - \frac{ \varphi (x) }{ x^3 } + \int_x^{+\infty } \frac{  3\varphi (t) }{ t^2 } \ dt . 
       \]

       De nouveau l'intégrale est positive et on obtient : 
       \[
       1 - \Phi (x) \geq \frac{ \varphi (x) }{ x } - \frac{ \varphi (x) }{ x^3 } \ \ \text{ donc } \ \ \frac{ 1 - \Phi (x) }{ \varphi (x) } \geq \frac{ 1 }{ x } - \frac{ 1 }{ x^3 }  
       \]

       et enfin : 
       \[
       \frac{ 1 }{ x } - \frac{ 1 }{ x^3 } \leq \frac{ 1 - \Phi ( x) }{ \varphi (x) } \leq \frac{ 1 }{ x } . 
       \]

     \item On multiplie par $x > 0$ et on obtient : 
       \[
       1 - \frac{ 1 }{ x^2 } \leq \frac{ 1 - \Phi (x) }{ \frac{ \varphi (x) }{ x } } \leq 1 
       \]

       et par théorème d'encadrement,
       \[
       \dlim{ x \rightarrow + \infty } \frac{ 1 - \Phi (x) }{ \frac{ \varphi (x) }{ x } } = 1 \ \ \ \text{ donc } \ \ \ 1 - \Phi (x) \underset{ x \rightarrow + \infty }{ \sim } \frac{ \varphi (x) }{ x } . 
       \]

     \end{enumerate}

   \item On calcule cette probabilité à l'aide de $\Phi$ : 
     \[
     P_{ [T > x ] } \left[ T > x + \frac{ a }{ x } \right] = \frac{ P \left[ \Ev{T > x } \cap \left( T > x + \frac{ a }{ x } \right) \right] }{ P \Ev{ T > x } } 
     \]

     et comme $a > 0$ et $x > 0$, on a $x + \frac{ a }{ x } > x $ donc : 
     \[
     \left( T > x + \frac{ a }{ x } \right) \subset \Ev{ T > x } \ \ \text{ puis } \ \ \left( T > x + \frac{ a }{ x } \right) \cap \Ev{ T > x } = \left( T > x + \frac{ a }{ x } \right) 
     \]

     et enfin : 
     \[
     P_{ [T > x ] } \left[ T > x + \frac{ a }{ x } \right] = \frac{ P \left[  T > x + \frac{ a }{ x } \right] }{ P \Ev{ T > x } } = \frac{ 1 - \Phi \left[  x + \frac{ a }{ x } \right] }{ 1 - \Phi ( x ) } . 
     \]

     Comme $x$ et $x + \frac{ a }{ x }$ tendent vers $+\infty$, on peut utiliser l'équivalent de la question 3c au numérateur et au dénominateur : 
     \[
     P_{ [T > x ] } \left[ T > x + \frac{ a }{ x } \right] \underset{ x \rightarrow + \infty }{ \sim } \frac{ \varphi \left[ x + \frac{ a }{ x } \right] \times x }{ \varphi (x) \times \left( x + \frac{ a }{ x } \right) } 
     \]

     et on a de plus : 
     \[
     \frac{ x }{ x + \frac{ a }{ x } } = \frac{ 1 }{ 1 + \frac{ a }{ x^2 } } \xrightarrow[ x \rightarrow + \infty ]{} 1 
     \]

     donc
     \[
     P_{ [T > x ] } \left[ T > x + \frac{ a }{ x } \right] \underset{ x \rightarrow + \infty }{ \sim } \frac{ \varphi \left[  x + \frac{ a }{ x } \right]  }{ \varphi (x) } 
     \]

     Enfin on remplace $\varphi$ par son expression : 
     \[
     P_{ [T > x ] } \left[ T > x + \frac{ a }{ x } \right] \underset{ x \rightarrow + \infty }{ \sim } \frac{ e^{ - \frac{  \left( x + \frac{ a }{ x } \right)^2 }{2} } \times \sqrt{ 2 \pi } }{ \sqrt{ 2 \pi } \times e^{ - \frac{ x^2 }{ 2 } } } = e^{ \frac{ - \left( x + \frac{ a }{ x } \right)^2 + x^2 }{2} } . 
     \]

     Enfin remarque une identité remarquable : 
     \begin{eqnarray*}
       P_{ [T > x ] } \left[ T > x + \frac{ a }{ x } \right] & \underset{ x \rightarrow + \infty }{ \sim } & e^{ \frac{  \left( x +  x + \frac{ a }{ x } \right) \times \left( x - x - \frac{a }{ x } \right) }{2} } = e^{ \frac{ - \frac{ a }{ x } \left( 2 x + \frac{ a }{ x } \right)  }{2} } \\ \\
       & \underset{ x \rightarrow + \infty }{ \sim } & e^{ \frac{ - 2 a \left( 1 + \frac{ a }{ 2 x^2 } \right)  }{2} } \\ \\
       & \xrightarrow[ x \rightarrow + \infty ]{} & e^{ - a } = \frac{ 1 }{ e^a } . 
     \end{eqnarray*}

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $D$ la matrice définie par : $ D = \begin{smatrix} -1 & 0 \\ 0 & 4 \\ \end{smatrix}$.
   \begin{enumerate}
   \item On résout sans difficulté, avec $A = \begin{smatrix} a & b \\ c & d \\ \end{smatrix}$ : 
     \[
     A D = D A \Longleftrightarrow \begin{smatrix} - a & 4b \\ - c & 4 d \\ \end{smatrix} = \begin{smatrix} - a & - b \\ 4 c & 4d \\ \end{smatrix} \Longleftrightarrow \left\{ \begin{array}{cc} - a = - a & 4 b = - b \\ -c = 4 c & 4 d = 4 d \\ \end{array} \right. \Longleftrightarrow b=c= 0 \Longleftrightarrow A = \begin{smatrix} a & 0 \\ 0 & d \\ \end{smatrix} 
     \]

     donc les solutions sont toutes les matrices diagonales. \\ 

   \item Une telle matrice $M$ vérifie $M D = D M$, car : 
     \[
     M D = M ( M^3 - 2 M ) = M^4 - 2 M^2 = ( M^3 - 2 M ) M = D M . 
     \]

     La matrice est donc forcément solution de l'équation précédente, elle est diagonale. On cherche donc les solutions de l'équation posée parmi les matrices diagonales. Avec $M = \begin{smatrix} a & 0 \\ 0 & b \\ \end{smatrix}$, on a : 
     \[
     M^3 - 2 M = D \Longleftrightarrow \begin{smatrix} a^3 - 2 a & 0 \\ 0 & b^3 - 2 b \\ \end{smatrix} = \begin{smatrix} -1 & 0 \\ 0 & 4 \\ \end{smatrix} \Longleftrightarrow \left\{ \begin{array}{c} a^3 - 2a = - 1 \\ b^3 - 2 b = 4 \\ \end{array} \right. 
     \]

     Il reste à résoudre ces deux équations du troisième degré, en trouvant à chaque fois une solution évidente pour factoriser : 
     \[
     1^3 - 2 \times 1 + 1 = 0 
     \]

     donc $a^3 - 2 a + 1$ se factorise par $ a - 1$. Par une division euclidienne ou une identification, on obtient : 
     \[
     a^3 - 2a + 1 = ( a - 1 ) \left( a^2 + a - 1 \right) 
     \]

     et l'équation $a^3 - 2a  +1 = 0$ a pour solution $a = 1$ et les solutions de $a^2 - a + 1 = 0$, de discriminant $1 + 4 = 5 $, donc qui a pour solution
     : 
     \[
     a = \frac{ 1 - \sqrt{ 5 } }{ 2 } \ \ \text{ et } \ \ a = \frac{ 1 + \sqrt{ 5 } }{ 2 } . 
     \]

     Enfin : 
     \[
     a^3 - 2 a + 1 = 0 \Longleftrightarrow a \in \left\{ 1 ; \frac{ 1 - \sqrt{ 5 } }{ 2 } ; \frac{ 1 + \sqrt{ 5 } }{ 2 } \right\} . 
     \]

     De même
     \[
     2^3 - 2 \times 2 - 4 = 8 - 4 - 4 = 0 
     \]

     donc $b^3 - 2 b - 4$ se factorise par $ b - 2$. Par une division euclidienne ou une identification, on obtient : 
     \[
     b^3 - 2b -4 = ( b - 2 ) \left( b^2 + 2 b +2 \right) 
     \]

     et l'équation $b^3 - 2b -4 = 0$ a pour solution $b = 2$ et les solutions de $b^2 + 2 b + 2 = 0$, de discriminant $4 - 8 = -4 < 0$, donc qui n'a pas de solution. Enfin : 
     \[
     b^3 - 2 b - 4 = 0 \Longleftrightarrow b = 2 . 
     \]

     On en déduit que les seules matrices $M$ vérifiant $M^3 - 2 M = D$ sont 
     \[
     M = \begin{smatrix} 1 & 0 \\ 0 & 2 \\ \end{smatrix} \ \ , \ \ \begin{smatrix} \frac{ 1 - \sqrt{ 5 } }{ 2 }  & 0 \\ 0 & 2 \\ \end{smatrix} \ \ , \ \ \begin{smatrix} \frac{ 1 - \sqrt{ 5 } }{ 2 }  & 0 \\ 0 & 2 \\ \end{smatrix} 
     \]

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Deux matrices carrées d'ordre $n$ $A$ et $B$ sont semblables
     s'il existe une matrice $P$ carrée d'ordre $n$ inversible telle
     que :
     \[
     A = P B P^{-1} . 
     \]
     
     Soit $f$ un endomorphisme de $\R^3$ dont la matrice $A$ dans la
     base canonique de $\R^3$ est donnée par :
     \[
     A = \begin{smatrix} 3 & 2 & -2 \\ -1 & 0 & 1 \\ 1 & 1 & 0 \\ \end{smatrix} 
     \]
     
     On note $\id$ l'endomorphisme identité de $\R^3$ et on pose :
     $f^2 = f \circ f$. \\ 

   \item 
     \begin{enumerate}
     \item On passe par les matrices dans la base canonique
       $\mathcal{B}$, on calcule :
       \begin{eqnarray*}
         Mat_{ \mathcal{B} } ( 2 f - f^2 ) & = & 2 Mat_{ \mathcal{B} } (f) - [ Mat_{ \mathcal{B} } (f) ]^2 = 2 A - A^2 = \begin{smatrix} 6 & 4 & -4 \\ -2 & 0 & 2 \\ 2 & 2 & 0 \\ \end{smatrix} - \begin{smatrix} 5 & 4 & -4 \\ -2 & - 1 & 2 \\ 2 & 2 & -1 \\ \end{smatrix} \\ \\
         & = & \begin{smatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ \end{smatrix} = I = Mat_{ \mathcal{B} } ( \id ) 
       \end{eqnarray*}       
       donc on obtient bien : 
       \[
       2 f - f^2 = \id .
       \]
       
     \item On sait que $f$ est un endormorphisme, et de plus :
       \[
       2 f - f^2 = f \circ ( 2 \id - f ) = \id 
       \]       
       donc $f$ est bijective et sa réciproque est $2 f - \id$. On en
       déduit que $f$ est un automorphisme et que son automorphisme
       réciproque est
       \[
       f^{-1} = 2 f - \id . 
       \]
       
     \item On déduit également de la relation de la question 2a que :
       \[
       f^2 - 2 f + \id = 0 
       \]       
       donc le polynôme $X^2 - 2 X + 1 = ( X - 1 )^2 $ est annulateur
       de $f$. Sa seule racine est 1, donc c'est la seule valeur
       propre possible de $f$. \\
       On vérifie que 1 est valeur propre, il faut montrer que $f -
       \id$ n'est pas bijective. On passe par la matrice :
       \[
       Mat_{ \mathcal{B} } ( f - \id ) = Mat_{ \mathcal{B} } (f) -
       Mat_{ \mathcal{B} } (\id ) = A - I = \begin{smatrix} 2 & 2 & -2
         \\ -1 & -1 & 1 \\ 1 & 1 & -1 \\ \end{smatrix}
       \]
       qui n'est pas inversible (les deux première colonnes sont
       égales) donc $f - \id$ n'est pas bijective. On en déduit que 1
       est bien valeur propre de $f$, et enfin :
       \[
       \spc ( f ) = \{ 1 \} .
       \]
       Supposons que $f$ est diagonalisable, alors $A$ l'est aussi et
       elle est semblable à une matrice diagonale ne comportant que
       les valeurs propres de $A$ sur la diagonale. Comme $\spc ( f )
       = \spc ( A ) = \{ 1 \}$, on a alors :
       \[
       A = P I P^{-1} = I
       \]
       qui est absurde, donc $f$ n'est pas diagonalisable. 

     \item On peut résoudre sans difficulté la système $(A - I ) X =
       0$, ou bien procéder astucieusement par l'image : on remarque
       que les trois colonnes de $A - I$ sont colinéaires, donc $\Im
       (A - I )$ est de dimension 1, puis $\ker ( A - I )$ et $\ker (
       f - \id )$ sont de dimension 2. De plus on remarque que :
       \[
       C_1 = C_2 = - C_3 \ \ \text{ donc } \ \ (f - \id) ( e_1 ) =
       (f-\id) (e_2 ) = - (f-\id ) ( e_3 )
       \]       
       et donc : 
       \[
       (f - \id ) ( e_1 - e_2 ) = ( f - \id) (e_1 + e_3 ) = 0
       \]

       donc la famille $[ (1,-1,0) , (1,0,1) ]$ est une famille de
       $E_1 (f)$, libre (deux vecteurs non colinéaires) et dont le
       cardinal est égal à la dimension de $E_1 (f)$, c'en est une
       base et :
       \[
       E_1 (f) = \Vect [ (1,-1,0) , (1,0,1) ] .
       \]

     \end{enumerate}

   \item \begin{enumerate}

     \item Plusieurs approches sont possibles. Comme $A$ n'est pas diagonalisable, on ne peut utiliser la diagonalisation. Sans autre indication, l'approche naturelle est de calculer les premières puissances pour essayer de conjecturer une formule à prouver ensuite par récurrence : 
       \[
       A^0 = I = \begin{smatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ \end{smatrix} \ \ , \ \ A = \begin{smatrix} 3 & 2 & -2 \\ -1 & 0 & 1 \\ 1 & 1 & 0 \\ \end{smatrix} \ \ , \ \   A^2 = \begin{smatrix} 5 & 4 & -4 \\ -2 & - 1 & 2 \\ 2 & 2 & -1 \\ \end{smatrix} \ \ , \ \ A^3 = \begin{smatrix} 7 & 6 & -6 \\ -3 & -2 & 3 \\ 3 & 3 & -2 \\ \end{smatrix} 
       \]

       ce qui permet de conjecturer : 
       \[
       A^n = \begin{smatrix} 1 + 2n & 2n & - 2n \\ -n & 1 - n & n \\ n & n & 1 - n \\ \end{smatrix} = I + n \begin{smatrix} 2 & 2 & -2 \\ -1 & -1 & 1 \\ 1 & 1 & -1 \\ \end{smatrix} = I + n ( A - I ) 
       \]

       qu'on prouve ensuite par récurrence sur $n$ (sans difficulté). \\

       Une autre possibilité était de se servir de la relation $A^2 = 2 A - I$ pour conjecturer une écriture : 
       \[
       A^n = u_n A + v_n I 
       \]

       qu'on prouve par récurrence sur $n$, en obtenant au passage des relations de récurrences sur $u_n$ et $v_n$ : 
       \[
       A^{n+1} = A A^n = A ( u_n A + v_n I ) = u_n A^2 + v_n A = u_n ( 2 A - I ) + v_n A = ( 2u_n + v_n ) A - u_n I 
       \]

       donc $u_{n+1} = 2 u_n + v_n$ et $v_{n+1} = -u_n$ qu'on désimbriquent en passant à des relations de récurrence double : 
       \[
       u_{n+2} = 2 u_{n+1} + v_{n+1}  = 2 u_{n+1} - u_n \ \ \text{ et } \ \ v_{n+2} = - u_{n+1} = - 2 u_n - v_n = 2 v_{n+1} - v_n 
       \]

       puis on cherche les valeurs de $u_n$ et $v_n$ avec leurs premiers termes et la méthode classique sur les suites récurrentes linéaires doubles. \\

     \item Il existe plusieurs méthodes pour obtenir les puissances négatives : on peut calculer $A^{-1}$ puis calculer ses puissances par récurrence (soit ne refaisant le travail de conjecture, soit en reprenant la formule précédente qui est celle qu'on est censé tester). Mais la méthode la plus habile est de remarquer que pour tout $n \in \N$,
       \[
       A^{ - n } = ( A^n )^{ - 1 } 
       \]

       et que les puissances négatives sont donc les inverses des puissances positives. Plutôt que de calculer l'inverse avec Gauss-Jordan (pénible avec des coefficients qui dépendent de $n$), on se sert de la connaissance de $A^n$ pour tester si la formule demandée fonctionne : pour tout $n \in \N$,
       \[
       A^n \times \left[ \rule{0cm}{0.4cm} I -n ( A - I ) \right] = \left[ \rule{0cm}{0.4cm} I +n ( A - I ) \right] \times \left[ \rule{0cm}{0.4cm} I -n ( A - I ) \right] = I - n ( A - I ) + n ( A - I ) - n^2 ( A - I )^2 = I 
       \]

       car on a vu que $(A-I)^2 = A^2 - 2 A + I = 0$, on en déduit que que pour tout $n \in \N$, 
       \[
       A^{-n} = ( A^n )^{-1} = I - n ( A - I ) 
       \]

       donc pour tout $n$ négatif, on obtient bien : 
       \[
       A^n = A^{ - (-n) } = I - (-n) ( A - I ) = I + n ( A - I ) 
       \]

       et la formule se généralise pour les entiers négatifs, donc elle est valable pour tout entier relatif. \\

     \end{enumerate}

   \item Il faut donc obtenir $(u,v,w)$ tels que : 
     \[
     f ( u ) = u \ \ , \\ f ( v ) = v \ \ , \ \ f ( w ) = v + w . 
     \]

     On en déduit que $u$ et $v$ sont des vecteurs propres de $f$ associés à la valeur propre 1, ils doivent être pris dans $E_1 (f)$, on choisit donc : 
     \[
     u = ( 1 , -1 , 0 ) \ \ \text{ et } \ \ v = ( 1 , 0 , 1 ) 
     \]

     On cherche ensuite $w$ sous la forme $w = (a,b,c)$ et on résout : 
     \[
     A W = V + W \Longleftrightarrow \left\{ \begin{array}{c} 3 a + 2 b - 2 c = 1 + a \\ - a + c = b \\ a + b = 1 + c \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{c} 2 a + 2 b - 2 c = 1 \\ - a - b + c = 0 \\ a + b - c = 1 \\ \end{array} \right. 
     \]

     qu'on résout avec les pivots $L_2 \leftarrow 2 L_2 + L_1$, $L_3 \leftarrow 2 L_3 - L_1$ : 
     \[
     A W = V + W \Longleftrightarrow \left\{ \begin{array}{c} 2 a + 2 b - 2 c = 1 \\  0 = 1 \\ 0 = 1 \\ \end{array} \right. 
     \]

     qui est absurde. On peut essayer en échangeant l'ordre des vecteurs $u$ et $v$, mais cela échoue encore. Il faut donc remplacer $v$ par un autre vecteur propre associé à 1, donc sous la forme : 
     \[
     v = x (1,-1,0) + y (1,0,1) = (x+y , -x , y ) 
     \]

     et on résout de même l'équation : 
     \[
     A W = V + W \Longleftrightarrow \left\{ \begin{array}{c} 3 a + 2 b - 2 c = x+y + a \\ - a + c = -x + b \\ a + b = y + c \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{c} 2 a + 2 b - 2 c = x+y \\ - a - b + c = -x \\ a + b - c = y \\ \end{array} \right. 
     \]

     qu'on résout avec les pivots $L_2 \leftarrow 2 L_2 + L_1$, $L_3 \leftarrow 2 L_3 - L_1$ : 
     \[
     A W = V + W \Longleftrightarrow \left\{ \begin{array}{c} 2 a + 2 b - 2 c = x+y \\  0 = y-x \\ 0 = y-x\\ \end{array} \right. 
     \]

     qui a des solutions à condition que $y=x$. On prend donc par exemple $x=y=1$, donc (on récapitule) : 
     \[
     u = (1,-1,0) \ \ , \ \ v = (2 , -1 , 1 ) 
     \]

     et enfin $w=(a,b,c)$ avec 
     \[
     2 a + 2 b - 2c = 2 \Longleftrightarrow a + b - c = 1 \Longleftrightarrow a = 1 + c - b 
     \]

     et on peut prendre par exemple $b=c=0$ et $a=1$, qui donnent $w = (1,0,0)$. On peut alors vérifier que :
     \[
     a u + b v + x w = 0 \Longleftrightarrow \left\{ \begin{array}{c} a + 2 b + c = 0 \\ - a - b = 0 \\ b = 0 \\ \end{array} \right. \Longleftrightarrow a=b=c=0 
     \]

     donc la famille est $(u,v,w)$ est libre, et son cardinal est égal à la dimension de $\R^3$, c'est bien une base de $\R^3$, et on calcule la matrice de $f$ dans cette base : 
     \[
     A U = \begin{smatrix} 1 \\ -1 \\ 0 \\ \end{smatrix} = U \ \ , \ \ A V = \begin{smatrix} 2 \\ -1 \\ 1 \\ \end{smatrix} = V \ \ , \ \ A W = \begin{smatrix} 3 \\ -1 \\ 1 \\ \end{smatrix} = \begin{smatrix} 2 \\ -1 \\ 1 \\ \end{smatrix} + \begin{smatrix} 1 \\ 0 \\ 0 \\ \end{smatrix} = V + W 
     \]

     donc on a : 
     \[
     f ( u ) = u \ \ , \ \ f ( v ) = v \ \ , \ \ f w ) = v + w \ \ \text{ et enfin } \ \ C = Mat_{ (u,v,w) } (f) = \begin{smatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \\ \end{smatrix} . 
     \]

   \end{noliste}

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $(X_n)_{ n \in \N^* }$ une suite de variables aléatoires réelles indépendantes définies sur le même espace probabilisé $(\Omega , \mathcal{A} , P)$ et suivant toutes la loi uniforme sur l'intervalle $[0;1]$.
   \begin{enumerate}
   \item Le plus grand des $X_i$ est inférieur ou égal à $x$ si et seulement si ils le sont tous, donc pour tout $x \in \R$,
     \[
     \Ev{Y_k \leq x } = \bigcap\limits_{i=1}^k \Ev{ X_i \leq x } 
     \]

     et par indépendance des $X_i$, on obtient : 
     \[
     F_{ Y_k } (x) = \prod\limits_{i=1}^k F_{ X_i } (x) = [ F_{ X_1 } (x) ]^n 
     \]

     car les $X_i$ suivent toutes les même loi. \\

     De plus $X_1$ est à densité, donc elle est continue sur $\R$, et elle est de classe $C^1$ sauf en 0 et 1, donc par opérations élémentaires (composition avec la puissante $n$-ème qui est $C^{\infty}$), c'est aussi de cas de $F_{ Y_k }$, et $Y_k$ est à densité. \\

     Enfin on obtient une densité de $Y_k$ en dérivant $F_{ Y_k }$ sauf en 0 et 1, valeurs arbitraires : 
     \[
     f_{ Y_k } (x) = k f_{ X_1 } (x) [ F_{ X_1 } (x) ]^{k-1} = \left\{ \begin{array}{cl} 0 & \text{ si } x < 0 \\ k x^{k-1} & \text{ si } 0 \leq x \leq 1 \\ 0 & \text{ si } x > 1 \\ \end{array} \right. 
     \]

   \item On commence par la fonction de répartition : pour tout $x \in \R$ , 
     \[
     \Ev{Z_k \leq x } = ( - Y_k \leq x ) = \Ev{ Y_k \geq - x } 
     \]

     donc (avec $Y_k$ à densité donc $\Prob(\Ev{ Y_k < }) = P \Ev{ Y_k \leq x }$) :
     \[
     F_{ Z_k } (x ) = 1 - P \Ev{ Y_k < - x} = 1 - P \Ev{ Y_k \leq - x } = 1 - F_{ Y_k } (-x) = 1 - \left\{ \begin{array}{cl} 0 & \text{ si } -x < 0 \\ (-x)^k & \text{ si } 0 \leq -x \leq 1 \\ 1 & \text{ si } -x > 1 \\ \end{array} \right. 
     \]

     et en résolvant les conditions : 
     \[
     F_{ Z_k } (x) = \left\{ \begin{array}{cl} 1 & \text{ si } x > 0 \\ 1 - (-x)^k & \text{ si } -1 \leq x \leq 0 \\ 0 & \text{ si } x < - 1 \\ \end{array} \right. 
     \]

     Enfin cette fonction est continue sur $\R$ et de classe $C^1$ sauf en $-1$ et 0 par opérations élémentaires (avec l'expression $ 1 - F_{ Y_k } (x)$), donc $Z_k$ est à densité et une densité est donnée par (avec des valeurs arbitraires en $-1$ et 0) : 
     \[
     f_{ Z_k } (x) = \left\{ \begin{array}{cl} 0 & \text{ si } x > 0 \\  k (-x)^{k-1} & \text{ si } -1 \leq x \leq 0 \\ 0 & \text{ si } x < - 1 \\ \end{array} \right. 
     \]

   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Soit $E$ un espace vectoriel de dimension finie, alors un endomorphisme de $E$ est diagonalisable si et seulement si la somme des dimensions de ses sous-espaces propres est égale à $ \dim E$.

     On considère la matrice $A \in \mathcal{M}_2 (\R)$ définie par $A = \begin{smatrix} 2 & 4 \\ 1 & 2 \\ \end{smatrix}$. \\

   \item On note $\mathcal{M}_{2 , 1} ( \R )$ l'espace vectoriel des matrices à 2 lignes et 1 colonne à coefficients réels. \\
     Soit $u$ l'endomorphisme de $\mathcal{M}_{2,1} (\R)$ défini par : pour tout $X \in \mathcal{M}_{2,1} (\R)$, $u(X) = A X$. \begin{enumerate}

     \item On commence par $\Im u$, on a : 
       \[
       \Im u = \Im A = \Vect { \begin{smatrix} 2 \\ 1 \\ \end{smatrix}
         , \begin{smatrix} 4 \\ 2 \\ \end{smatrix} } = \Vect
       { \begin{smatrix} 2 \\ 1 \\ \end{smatrix} }
       \]

       car les deux colonnes sont colinéaires, puis la famille génératrice obtenue est libre (1 vecteur non nul) donc c'est une base de $\Im u$, et $\dim ( \Im u ) = 1$. On en déduit par théorème du rang, avec $\dim ( \mathcal{M}_{2,1} (\R) ) = 2$, que $\dim( \ker u ) = 1$. \\

       On cherche alors un vecteur non nul de $\ker u$ : il constituera une famille libre (1 vecteur non nul) de $\ker u$, avec le bon cardinal, c'en sera donc une base, et donc une famille génératrice. Or on a vu que : 
       \[
       C_2 = 2 C_1 \ \ \text{ donc } \ \ u ( e_2 ) = 2 u( e_1 ) \ \ \text{ et enfin } \ \ u ( e_2 - 2 e_1 ) = 0 
       \]

       Avec $e_2 - 2 e_1 = \begin{smatrix} -2 \\ 1 \\ \end{smatrix} \neq 0$,
       c'est un vecteur non nul de $\ker u$, donc libre, donc c'en est un
       base car $\dim ( \ker u ) = 1$, et enfin on obtient :
       \[
       \ker u = \Vect { \begin{smatrix} -2 \\ 1 \\ \end{smatrix} } . 
       \]

     \item On pourrait chercher (c'est un peu lourd mais faisable en un temps raisonnable avec une matrice d'ordre 2) les valeurs propres de $u$, puis chercher les sous-espaces propres associés. Mais on va ici mener une méthode plus originale et beaucoup plus efficace, qui ne fonctionne qu'avec les matrices de rang 1. \\

       On a déjà vu que 0 est valeur propre, et que le sous-espace propre associé est de dimension 1. \\

       Soit alors $\lambda$ une valeur propre non nulle et $ x$ un vecteur propre associé de $u$; montrons que $x \in \Im u$. En effet, on a $\lambda \neq 0$ et : 
       \[
       u (x) = \lambda x \ \ \text{ donc } \ \ x = \frac{ 1 }{ \lambda } u (x) \ \ \text{ et enfin } \ \ x = u \left( \frac{ 1 }{ \lambda } x \right) 
       \]

       donc $x$ est l'image d'un vecteur par $u$, c'est donc un vecteur de $\Im u$. On en déduit alors que : 
       \[
       x \in \Im u = \Vect { \begin{smatrix} 2 \\ 1 \\ \end{smatrix} } \ \ \text{ donc } \ \ \exists \mu \in \R, \text{ tq } x = \mu \begin{smatrix} 2 \\ 1 \\ \end{smatrix} . 
       \]

       On calcule alors $u(x)$ : 
       \[
       u (x) = A \mu \begin{smatrix} 2 \\ 1 \\ \end{smatrix} = \mu \begin{smatrix} 8 \\ 4 \\ \end{smatrix} = 4 { \mu \begin{smatrix} 2 \\ 1 \\ \end{smatrix} } = 4 x . 
       \]

       On en déduit que 4 est valeur propre de $u$, et que le sous-espace propre associé est $ \Vect { \begin{smatrix} 2 \\ 1 \\ \end{smatrix} } = \Im u$, de dimension 1 : la somme des dimensions des sous-espaces propres de $u$ vaut 2, et $u$ est diagonalisable. \\

       Remarque : si l'exercice était plus théorique, on aurait pu s'en sortir sans calculer $u ( x )$ : en effet on prend $a$ une base de $\Im u$, on a alors : 
       \[
       u (a) \in \Im  u = \Vect{ a } \Longleftrightarrow \exists \lambda \in \R , \text{ tq } u (a) = \lambda a . 
       \]

       On en déduit que $a$ est vecteur propre de $u$, et comme $a \notin \ker u$, la valeur propre associée ne peut pas être 0. On en déduit qu'il existe une autre valeur propre, et que la dimension du sous-espace propre associé est 1 (on a vu que tous les vecteurs propres associés sont colinéaires à $a$, donc cet espace propre est $\Vect{a}$), et la somme des dimensions des sous-espaces propres de $u$ vaut 2, $u$ est diagonalisable. \\

     \item On a vu que $u$, donc $A$, est diagonalisable. On obtient une base de vecteurs propres en concaténant les bases des sous-espaces propres, et ne posant : 
       \[
       P \begin{smatrix} -2 & 2 \\ 1 & 1 \\ \end{smatrix} \ \ \ \text{ et } \ \ \ D = \begin{smatrix} 0 & 0 \\ 0 & 4 \\ \end{smatrix} 
       \]

       on a $A = P D P^{-1}$. On en déduit immédiatement que pour tout $n \in \N^*$ ($n=0$ doit être écarté car $0^0 = 1$ donne une valeur différente de$0^n = 0$ pour $n \geq 1$), on a : 
       \[
       A^n = P D^n P^{-1} = P \begin{smatrix} 0 & 0 \\ 0 & 4^n \\ \end{smatrix} P^{-1} . 
       \]

       On pourrait calculer $P^{-1}$ pour conclure, mais on peut remarquer astucieusement que $D^n$ est colinéaire à $D$ et faire apparaître $D$ : 
       \[
       A^n = P \left[ 4^{n-1} \begin{smatrix} 0 & 0 \\ 0 & 4 \\ \end{smatrix} \right] P^{-1}  = 4^{n-1} P D P^{-1} = 4^{n-1} A . 
       \]

     \end{enumerate}

   \item Soit $v$ l'endomorphisme de $\mathcal{M}_2 (\R)$ défini par : pour tout $M \in \mathcal{M}_2 (\R)$, $v(M) = A M$. \\
     On note $\mathcal{B} = (E_{1,1} , E_{1,2} , E_{2,1} , E_{2,2})$ la base canonique de $\mathcal{M}_2 (\R)$ et on rappelle que : 
     \[
     E_{ 1,1 } = \begin{smatrix} 1 & 0 \\ 0 & 0 \\ \end{smatrix} \ , \ E_{ 1,2 } = \begin{smatrix} 0 & 1 \\ 0 & 0 \\ \end{smatrix} \ , \ E_{ 2,1 } = \begin{smatrix} 0 & 0 \\ 1 & 0 \\ \end{smatrix} \ , \ E_{ 2,2 } = \begin{smatrix} 0 & 0 \\ 0 & 1 \\ \end{smatrix} 
     \]

     \begin{enumerate}

     \item On calcule sans difficulté : 
       \[
       v ( E_{1,1} ) = A E_{1,1} = \begin{smatrix} 2 & 0 \\ 1 & 0 \\ \end{smatrix} = 2 E_{1,1} + E_{2,1} \ \ , \ \ v ( E_{1,2} ) = A E_{1,2} = \begin{smatrix} 0 & 2 \\ 0 & 1 \\ \end{smatrix} = 2 E_{1,2} + E_{2,2} , 
       \]

       \[
       v ( E_{2,1} ) = A E_{2,1} = \begin{smatrix} 4 & 0 \\ 2 & 0 \\ \end{smatrix} = 4 E_{1,1} + 2 E_{2,1} \ \ , \ \ v ( E_{2,2} ) = A E_{2,2} = \begin{smatrix} 0 & 4 \\ 0 & 2 \\ \end{smatrix} = 4 E_{1,2} + 2 E_{2,2} 
       \]

       donc : 
       \[
       V = \begin{smatrix} 2 & 0 & 4 & 0 \\ 0 & 2 & 0 & 4 \\ 1 & 0 & 2 & 0 \\ 0 & 1 & 0 & 2 \\ \end{smatrix} . 
       \]

     \item En retirant les deux dernières colonnes égales aux deux premières, on obtient : 
       \[
       \Im V = \Vect { \begin{smatrix} 2 \\ 0 \\ 1 \\ 0
           \\ \end{smatrix}, \begin{smatrix} 0 \\ 2 \\ 0 \\ 1
           \\ \end{smatrix} } \ \ \text{ donc } \ \ \Im v =
       \Vect{ 2 E_{1,1} + E_{2,1} , 2 E_{1,2} + E_{2,2}} = \Vect
       { \begin{smatrix} 2 & 0 \\ 1 & 0 \\ \end{smatrix}
         , \begin{smatrix} 0 & 2 \\ 0 & 1 \\ \end{smatrix} }
       \]

       (Remarque : on est passé par $V$ car on venait de la faire trouver, mais c'est maladroit : il était plus rapide de conclure directement en écrivant $\Im v$ à l'aide des images de la base $\mathcal{B}$) \\

       qui est de dimension 2 car les 2 vecteurs générateurs ne sont pas colinéaires, donc la famille génératrice est libre, c'est une base de $\Im v$. On en déduit que $\ker v$ est de dimension 2 (théorème du rang, avec $\dim ( \mathcal{M}_2 (\R) ) = 4$), et puisque $C_1 = C_3$ et $C_2 = C_4$, on obtient : 
       \[
       v ( E_{1,1} ) = v ( E_{2,1} ) \ \ \text{ et } \ \ v ( E_{1,2} = v ( E_{2,2} ) \ \ \text{ donc } \ \ v ( E_{ 1,1 } - E_{2,1} ) = v ( E_{1,2 } - E_{ 2,2 } ) = 0 
       \]

       (Remarque : à nouveau on avait directement ces relations sans la matrice avec les images calculées à la question a) \\

       donc la famille $( E_{ 1,1 } - E_{2,1} , E_{1,2 } - E_{ 2,2 } ) = { \begin{smatrix} 1 & 0 \\ -1 & 0 \\ \end{smatrix} , \begin{smatrix} 0 & 1 \\ 0 & -1 \\ \end{smatrix} }$  est une famille de $\ker v$, libre car constituée de deux vecteurs non colinéaires, et dont le cardinal est égal à la dimension de $\ker v$ : c'est donc une base de $\ker v$ et on obtient : 
       \[
       \ker v= \Vect { \begin{smatrix} 1 & 0 \\ -1 & 0 \\ \end{smatrix} , \begin{smatrix} 0 & 1 \\ 0 & -1 \\ \end{smatrix} } . 
       \]

     \item A nouveau on peut calculer valeurs propres et sous-espaces propres. Mais on va généraliser la méthode précédente avec cette fois-ci un en endomorphisme de rang $2$ (c'est plus difficile mais possible, par contre au-delà du rang 2 ça reste faisable en théorie mais très compliqué en pratique). \\

       La valeur propre 0 est déjà traitée : $\ker v = E_0 (v)$ est de dimension 2, donc 0 est valeur propre. \\

       Soit $\lambda \neq 0$ une valeur propre de $v$ et $x$ un vecteur propre associé. Pour les mêmes raisons que précédemment, $x$ est un vecteur de $\Im v$. On va alors s'intéresser à la restriction $\tilde{v}$ de $v$ à $\Im v$, et on commence par prouver que c'est un endomorphisme : 
       \[
       \tilde{v} : \left\{ \begin{array}{ccc} \Im v & \rightarrow & ?? \\ x & \mapsto & v ( x ) \\ \end{array} \right. 
       \]

       Cette application est immédiatement linéaire par linéarité de $v$ : soient $x$ et $y$ dans $\Im v$ et $\lambda \in \R$, $ \lambda x + y \in \Im v$ par stabilité d'un espace vectoriel et :
       \[
       \tilde{v} ( \lambda x + y ) = v ( \lambda x + y ) = \lambda v(x) + v(y) = \lambda \tilde{v} (x) + \tilde{v} (y) 
       \]

       par linéarité de $v$, donc $\tilde{v}$ est linéaire et pour tout $x \in \Im v$, on a : 
       \[
       \tilde{v} (x) = v ( x ) \in \Im v 
       \]

       puisque c'est l'image d'un élément par $v$, donc $\tilde{v}$ est un endomorphisme. Enfin, toujours pour $\lambda \neq 0$, on a vu que les vecteurs propres de $v$ associés à $\lambda$ appartiennent à $\Im v$, donc vérifient : 
       \[
       v(x) = \tilde{v} ( x ) = \lambda x 
       \]

       donc ce sont exactement les vecteurs propres de $\tilde{v}$ associés à $\lambda$. Pour trouver les éléments propres de $\tilde{v}$, on cherche sa matrice dans la base de $\Im v$ trouvée précédemment : 
       \[
       \tilde{v} { \begin{smatrix} 2 & 0 \\ 1 & 0 \\ \end{smatrix} } = \begin{smatrix} 8 & 0 \\ 4 & 0 \\ \end{smatrix} = 4 \begin{smatrix} 2 & 0 \\ 1 & 0 \\ \end{smatrix} \ \ \text{ et } \ \ \tilde{v} { \begin{smatrix} 0 & 2 \\ 0 & 1 \\ \end{smatrix} } = \begin{smatrix} 0 & 8 \\ 0 & 4 \\ \end{smatrix} = 4 \begin{smatrix} 0 & 2 \\ 0 & 1 \\ \end{smatrix} 
       \]

       donc la matrice $\tilde{V}$ de $\tilde{v}$ dans cette base est $4 I$, dont la seule valeur propre est 4 avec un sous-espace propre de dimension 2, donc $\tilde{v}$ admet 4 pour unique valeur propre, avec un sous-espace propre de dimension 2 : on en déduit que 4 est valeur propre de $v$ avec un sous-espace propre de dimension 2. \\

       Enfin la somme des dimensions des sous-espaces propres de $v$ vaut 4, qui est la dimension de $\mathcal{M}_2 ( \R )$, et $v$ est diagonalisable. \\

     \end{enumerate}

   \end{noliste}

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $A_i$ l'évènement : "la boule rouge est dans l'urne $i$" et $B_{1,j}$ l'évènement : "on a tiré une boule bleue dans au $j$-ème tirage sans remise dans l'urne 1". On demande de calculer la probabilité : 
   \[
   P_{ B_{1,1} \cap B_{1,2} } ( A_2 ) . 
   \]

   \noindent Mais comme l'évènement $A_2$ est antérieur aux tirages, on ne peut pas exprimer la condition. On revient alors à la définition de la probabilité conditionnelle, puis aux probabilités composées : 
   \[
   P_{ B_{1,1} \cap B_{1,2} } ( A_2 ) = \frac{ P ( A_2 \cap B_{1,1} \cap B_{1,2} ) }{ P ( B_{1,1} \cap B_{1,2} ) } = \frac{ P ( A_2 ) P_{A_2} ( B_{1,1} ) P_{ A_2 \cap B_{1,1} } ( B_{1,2} ) }{ P ( B_{1,1} \cap B_{1,2} ) } 
   \]

   \noindent et on n'a pas appliqué les probabilités composées au dénominateur car il est impossible de connaître ces probabilités sans connaître la place de la boule rouge (urne 1 ou pas). On traite séparément le numérateur et le dénominateur : 
   \[
   P ( A_2 ) P_{A_2} ( B_{1,1} ) P_{ A_2 \cap B_{1,1} } ( B_{1,2} ) = P ( A_2 ) \times 1 \times 1 = P ( A_2 ) 
   \]

   \noindent Pour le dénominateur, comme $(A_1 , \overline{A_1})$ est un sce, on obtient : 
   \[
   B_{1,1} \cap B_{1,2} = \left[ \rule{0cm}{0.4cm} A_1 \cap B_{1,1} \cap B_{1,2} \right] \cup \left[ \rule{0cm}{0.4cm} \overline{A_1} \cap B_{1,1} \cap B_{1,2} \right] 
   \]

   \noindent donc : 
   \[
   P ( B_{1,1} \cap B_{1,2} ) = P ( A_1 ) \times \frac{ 2 }{ 3 } \times \frac{ 1 }{ 2 } + [ 1 - P (A_1 ) ] \times 1 \times 1 = \frac{ 1 }{ 3 } P ( A_1 ) + 1 - P ( A_1 ) = 1 - \frac{ 2 }{ 3 } P ( A_1 ) . 
   \]

   On obtient finalement : 
   \[
   P_{ B_{1,1} \cap B_{1,2} } ( A_2 ) = \frac{ P ( A_2 ) }{ 1 - \frac{ 2 }{ 3 } P ( A_1 ) } . 
   \]

   \noindent Pour continuer il faut connaître les probabilités des évènements $A_i$, qui dépendent de la manière dont les urnes ont été remplies. On suppose que la remplissage a été fait au hasard, toutes les urnes jouent alors le même rôle et on en déduit par symétrie que : 
   \[
   \forall i \in \llb 1 ; n \rrb , \ P ( A_i ) = \frac{ 1 }{ n} 
   \]

   \noindent ce qui permet de conclure le calcul : 
   \[
   P_{ B_{1,1} \cap B_{1,2} } ( A_2 ) = \frac{ 1 }{ n \left( 1 - \frac{ 2 }{ 3 n } \right) } = \frac{ 1 }{ n - \frac{ 2 }{ 3 } } = \frac{ 3 }{ 3 n - 2 } . 
   \]

 \end{exercice}

 \newpage

\section{\underline{Annales 2013}}

% \setcounter{exercice}{0}
\begin{exercice}{\it (Exercice avec préparation)}~
  \begin{noliste}{1.}
  \item Soit $n$ un entier supérieur ou égal à 1. On dit qu'une
    expérience aléatoire suit le schéma binomial si elle consiste en
    $n$ répétitions indépendantes de la même épreuve, pouvant mener à
    2 résultat : succès ou échecs.\\
    On sait alors que la variable aléatoire comptant le nombre de
    succès pendant l'expérience suit une loi binomiale de paramètres
    $n$ et $p$, où $p$ est la probabilité du succès lors d'une
    épreuve. On sait de même que le nombre d'échecs suit la loi
    binomiale de paramètres $n$ et $q = 1-p$.

  \item Soit $(X_n)_{ n \in \N^* }$ une suite de variables aléatoires
    définies sur un espace probabilisé $(\Omega , \mathcal{A} , P )$,
    indépendantes et de même loi de Bernouilli de paramètre
    $\frac{1}{2}$.

    On pose, pour tout $n \in \N^*$, $W_n = \Sum{k=1}{n} k X_k$ et
    $s_n = \frac{ n ( n+1 ) }{ 2 }$.
    
    \begin{noliste}{a)}

    \item Par linéarité de l'espérance, $W_n$ admet une espérance et :
      \[
      E ( W_n ) = \Sum{k=1}{n} k E ( X_k ) = \Sum{k=1}{n} \frac{ 1 }{
        2 } k = \frac{ 1 }{ 2} \Sum{k=1}{n} k = \frac{ 1 }{ 2} \times
      \frac{ n (n+1 ) }{ 2 } = \frac{ 1 }{ 2 } s_n .
      \]      
      Par indépendance des variables $X_i$ puis par quadracité de la
      variance, $W_n$ admet une variance et :
      \[
      V ( X_n ) = \Sum{k=1}{n} V ( k X_k ) = \Sum{k=1}{n} k^2 V ( X_k
      ) = \Sum{k=1}{n} \frac{ 1 }{ 4 } k^2 = \frac{ 1 }{ 4 }
      \Sum{k=1}{n} k^2 = \frac{ 1 }{ 4 } \times \frac{ n (n+1 ) (2n+1
        ) }{ 6 } = \frac{ 2n + 1 }{ 12 } s_n .
      \]

    \item Comme une somme de termes positifs n'est nulle que si tous
      les termes sont nuls,
      \[
      \Ev{W_n = 0} = \bigcap\limits_{i=1}^n \Ev{X_i = 0}
      \]
      et par indépendances des $X_i$,
      \[
      P \Ev{ W_n = 0 } = \prod\limits_{i=1}^n P \Ev{X_i = 0 } =
      \prod\limits_{i=1}^n \frac{1}{2} = \left( \frac{ 1 }{ 2 }
      \right)^n .
      \]
      D'autre par on remarque $W_n$ atteint la valeur $s_n$ lorsque
      tous les $X_i$ valent 1. De plus si un ou plus des $X_i$ ne vaut
      pas 1, la somme sera alors inférieure ou égale à $s_n - 1$, donc
      ne pourra pas valoir $s_n$. On en déduit l'écriture d'évènements
      suivante, puis par indépendance :
      \[
      \Ev{W_n = s_n } = \bigcap\limits_{i=1}^n \Ev{X_i = 1} \ \ \
      \text{ et } \ \ \ P \Ev{ X_n = 0 } = \prod\limits_{i=1}^n P
      \Ev{X_i = 1} = \prod\limits_{i=1}^n \frac{1}{2} = \left( \frac{
          1 }{ 2 } \right)^n .
      \]
      
    \item $W_n = 3$ peut être atteinte sous les formes suivantes :
      \[
      3 = 0 + 0 + 3 + 0 + \dots + 0 \ \ \text{ ou } \ \ 3 = 1 + 2 + 0
      + \dots + 0
      \]
      qui existent ou non selon que $n$ dépasse 2, puis 3. On a donc
      : 
      \begin{itemize}
      \item Pour $n=1$,
        \[
        \Ev{W_1 = 3} = \emptyset \ \ \ \text{ et } \ \ \ P \Ev{W_1 = 3 } = 0 . 
        \]
        
      \item Pour $n=2$,
        \[
        \Ev{W_2 = 3 } = \Ev{X_1 = 1 } \cap \Ev{X_2 = 1 } \ \ \ \text{
          et } \ \ \ P \Ev{ W_2 = 3 } = \frac{ 1 }{ 4 } .
        \]

      \item Pour $n \geq 3$,
        \[
        \Ev{W_n = 3 } = \left[ \Ev{X_1 = 1 } \cap \Ev{X_2 = 1 } \cap
          \left( \bigcap\limits_{i=3}^n \Ev{X_i = 0} \right) \right]
        \cup \left[ \Ev{X_1 = 0 } \cap \Ev{X_2 = 0} \cap \Ev{X_3 = 1}
          \cap \left( \bigcap\limits_{i=4}^n \Ev{X_i = 0 } \right)
        \right]
        \]
        puis par incompatibilité de la réunion et indépendance des
        $X_i$,
        \[
        P \Ev{ W_n = 3 } = \left( \frac{ 1 }{ 2 } \right)^n + \left(
          \frac{ 1 }{ 2 } \right)^n = \frac{ 2 }{ 2^n } = \left(
          \frac{ 1 }{ 2 } \right)^{ n-1 } .
        \]
      \end{itemize}
    \end{noliste}
    
  \item Question difficile. Il faut comprendre que pour chaque
    situation menant à $W_n = k$, la situation "inverse" (tous les
    échecs transformés en succès et tous les succès en échecs) donnera
    $W_n = s_n - k$. Pour formaliser l'égalité d'évènements, on écrit
    :
    \[
    (W_n = s_n - k ) = ( s_n - W_n = k ) = \left( \Sum{i=1}{n} i -
      \Sum{i=1}{n} i X_i = k \right) = \left( \Sum{i=1}{n} i (1-X_i)
      = k \right) = \Ev{ Z_n = k }
    \]
    où $Z_n = \Sum{i=1}{n} i (1 - X_i )$ avec les variables $Y_i = 1
    - X_i$ qui sont indépendantes et qui suivent la loi de Bernouilli
    de paramètre $\frac{1}{2}$, donc $Z_n$ suit la même loi que
    $W_n$. On en déduit bien que :
    \[
    P \Ev{ W_n = k } = P \Ev{ Z_n = k } = P ( W_n = s_n - k ) .
    \]
    
  \item
    \begin{noliste}{a)}
    \item Sachant $\Ev{W_n = j}$, on a alors :
      \[
      W_{n+1} = \Sum{k=1}{n+1} k X_k = \Sum{k=1}{n} k X_k + (n+1)
      X_{n+1} = W_n + (n+1) W_{n+1} = j + (n+1) X_{n+1}
      \]
      avec $X_{n+1}$ qui vaut 0 ou 1, donc :
      \[
      W_{n+1} \backslash \Ev{W_n = j } ( \Omega ) = \{ j ; j + n + 1
      \} \ \ , \ \ P_{ \Ev{W_n = j } } ( W_{n+1} = j ) = P_{ \Ev{W_n
          = j } } ( X_{n+1} = 0 ) =\frac{ 1 }{ 2 }
      \]
      et
      \[
      P_{ \Ev{W_n = j } } ( W_{n+1} = j + n + 1 ) = P_{ \Ev{W_n = j }
      } ( X_{n+1} = 1 ) =\frac{ 1 }{ 2 } .
      \]
      
    \item De plus $\Ev{W_n =j}_{ j \in \llb 0 ; s_n \rrb }$ est un
      système complet d'évènements donc par formule des probabilités
      totales, pour tout $k \in \llb 0 ; s_{n+1} \rrb$ :
      \[
      ( W_{n+1} = k ) = \bigcup\limits_{j=0}^{ s_n } [ \Ev{W_n = j }
      \cap (W_{n+1} = k ) ]
      \]
      donc par incompatibilité de la réunion et probabilités
      composées :
      \[
      P ( W_{n+1} = k ) = \Sum{j=0}{ s_n } P \Ev{ W_n = j } P_{
        \Ev{W_n = j} } ( W_{n+1} = k )
      \]
      et cette probabilité conditionnelle est non nulle nulle si et
      seulement $k \in \{ j ; j + n + 1 \}$ donc pour tout $j$
      vérifiant :
      \[
      j = k \ \ \text{ ou } \ \ j+n+1 = k \Longleftrightarrow j = k -
      n - 1 .
      \]       
      On sépare alors selon que ces valeur existent ou non :
      \begin{itemize}
      \item Si $ k \leq n $, $k-n-1 \leq -1$, $k$ est bien dans le
        support de $W_n$ mais $k-n-1$ n'y est pas donc :
        \[
        P ( W_{n+1} = k ) = \Sum{j=0}{k-1} 0 + \frac{ 1 }{ 2}
        \Prob(\Ev{ W_n = k }) + \Sum{j=k+1}{s_n} 0 .
        \]
        
      \item Si $n+1 \leq j \leq s_n$, $k$ est bien dans le support de
        $W_n$ et $0 \leq k-n-1 \leq s_n - n - 1 \leq s_n$ donc $k-n-1$
        y est aussi et :
        \[
        P ( W_{n+1} = k ) = \Sum{j=0}{k-n-2} 0 + \frac{1}{2} P ( W_n
        = k-n-1 ) + \Sum{j=k-n}{k-1} 0 + \frac{ 1 }{ 2 } \Prob(\Ev{
          W_n = k }) + \Sum{j=k+1}{s_n} 0
        \]
        
      \item Si $s_n + 1 \leq k \leq s_{n+1}$, alors $k$ n'est pas dans
        le support de $W_n$ (trop grand) mais $k-n-1$ y est bien (en
        effet $s_{n+1} = s_n + n + 1$ donc $k-n-1$ est inférieur à
        $s_n$, et supérieur à $s_n - n = s_{n-1} $ qui est
        positif). On obtient alors :
        \[
        P ( W_{n+1} = k ) = \Sum{j=0}{k-n-2} 0 + \frac{1}{2} P ( W_n =
        k-n-1 ) + \Sum{j=n-k}{s_n} 0
        \]
      \end{itemize}
      et en rassemblant :
      \[
      P ( W_{n+1} = k ) = \left\{ 
        \begin{array}{ll} \frac{ 1 }{ 2 } P \Ev{ W_n = k } & \text{ si
          } k \leq n \\ \\ \frac{ 1 }{ 2 } P \Ev{W_n = k} + \frac{ 1
          }{ 2 } P ( W_n = k-n-1) & \text{ si } n+1 \leq k \leq s_n \\
          \\ \frac{ 1 }{ 2 } P ( W_n = k-n-1) & \text{ si } s_n + 1
          \leq k \leq s_{n+1}
          \\
        \end{array} 
      \right.
      \]
    \end{noliste}
  \end{noliste}
\end{exercice}

\begin{exercice}{\it (Exercice sans préparation)}~\\
  On pose pour tout $n \in \N^*$ : $S_n = \Sum{k=1}{n} k^2 \ln \left(
    \frac{ k }{ n } \right)$.
  \begin{noliste}{1.}
  \item On fait apparaître une somme de Riemann :
    \[
    \frac{ S_n }{ n^3 } = \frac{ 1 }{ n } \Sum{k=1}{n} \frac{ k^2 }{
      n^2 } \ln \left( \frac{ k }{ n } \right) = \frac{ 1 }{ n }
    \Sum{k=1}{n} f \left( \frac{ k }{ n } \right)
    \]
    et posant la fonction $f(x) = x^2 \ln (x)$ qui est continue sur
    $]0;1]$, et qui se prolonge par continuité en 0 en posant $f(0) =
    0$ (par croissances comparées). Les sommes de Riemann donnent
    alors :
    \[
    \dlim{ n \rightarrow + \infty } \frac{ S_n }{ n^3 } = \int_0^1 f(x) \ dx 
    \]    
    qu'on calcule en revenant à l'intégrale partielle (car $\ln$ n'est
    pas définie en 0) : avec une IPP évidente,
    \begin{eqnarray*}
      \int_y^1 f(x) \ dx & = & \int_y^1 x^2 \ln x \ dx = \left[ \frac{
          x^3 }{ 3 } \ln x \right]_y^1 - \int_y^1 \frac{ x^3 }{ 3 }
      \times \frac{ 1 }{ x } \ dx \\
      & = & \frac{ 1 }{ 3 } \ln 1 - \frac{ y^3 \ln y }{ 3 } - \frac{ 1
      }{ 3 } \int_y^1 x^2 \ dx = - \frac{ y^3 \ln y }{ 3 } - \frac{ 1
      }{ 3 } \left( \frac{ 1 }{ 3 } - \frac{ y^3 }{ 3 } \right) \\
      & \xrightarrow[ y \rightarrow 0 ]{} & 0 - \frac{ 1 }{ 3 } \left(
        \frac{ 1 }{ 3 } - 0 \right) = - \frac{ 1 }{ 9 } 
    \end{eqnarray*}    
    donc :
    \[
    \dlim{ n \rightarrow + \infty } \frac{ S_n }{ n^3 } = \int_0^1
    f(x) \ dx = - \frac{ 1 }{ 9 } .
    \]
    
  \item On fait apparaître la somme précédente :
    \begin{eqnarray*}
      \frac{1}{n^3} \Sum{k=1}{n} k^2 \ln \left( \frac{ k+1 }{ n }
      \right) & = & \frac{1}{n^3} \Sum{k=1}{n} k^2 \ln \left( \frac{ k
          \left( 1 + \frac{ 1 }{ k } \right) }{ n } \right) =
      \frac{1}{n^3} \Sum{k=1}{n} k^2 \left[  \ln \left( \frac{ k  }{ n
          } \right) + \ln \left( 1 + \frac{ 1 }{ k } \right) \right] \\
      & = & \frac{1}{n^3} \Sum{k=1}{n} k^2 \ln \left( \frac{ k  }{ n }
      \right)  + \frac{1}{n^3} \Sum{k=1}{n} k^2  \ln \left( 1 + \frac{
          1 }{ k } \right) 
    \end{eqnarray*}    
    La première somme a pour limite $- \frac{1}{9}$, la deuxième n'est
    plus une somme de Riemann, on ne peut plus la calculer. On va
    chercher sa limite par encadrement. \\
    Comme $ 1 + \frac{ 1 }{ k } \geq 1$, cette somme est à termes
    positifs donc elle est positive. Pour la minorer on minore le
    $\ln$, avec l'inégalité classique :
    \[
    \ln ( 1 + x ) \leq x \ \text{ sur } ]-1 ; +\infty[ 
    \]
    (par étude de la fonction $g(x) = \ln (1+x) - x$, ou par
    concavité de $\ln$ et en remarquant que $y=x$ est la tangente au
    point $a = 1$ de $ h(x) = \ln (1+x)$). On obtient alors :
    \[
    0 \leq \frac{1}{n^3} \Sum{k=1}{n} k^2 \ln \left( 1 + \frac{ 1 }{ k
      } \right) \leq \frac{1}{n^3} \Sum{k=1}{n} k^2 \times \frac{ 1 }{
      k } = \frac{ 1 }{ n^3 } \Sum{k=1}{n} k = \frac{ n (n+1) }{ 2 n^3
    } = \frac{ 1 + \frac{ 1 }{ n } }{ n } \xrightarrow[ n \rightarrow
    + \infty ]{} 0 .
    \]
    Par théorème d'encadrement, cette deuxième somme tend vers 0 puis
    la somme de départ tend vers $- \frac{ 1 }{ 9 }$.\\
    Deuxième méthode pour faire apparaître $S_n$ : changement
    d'indice : on pose $k' = k+1$ et on obtient :
    \begin{eqnarray*}
      \frac{1}{n^3} \Sum{k=1}{n} k^2 \ln \left( \frac{ k+1 }{ n }
      \right) & = & \frac{1}{n^3} \Sum{k=2}{n+1} (k-1)^2 \ln \left(
        \frac{ k }{ n } \right) = \frac{1}{n^3} \Sum{k=2}{n+1} (k^2 -
      2 k + 1 ) \ln \left( \frac{ k }{ n } \right)  \\
      & = & \frac{1}{n^3} \Sum{k=2}{n+1} k^2 \ln \left( \frac{ k }{ n
        } \right) - 2 \frac{1}{n^3} \Sum{k=2}{n+1} k \ln \left( \frac{
          k }{ n } \right) + \frac{1}{n^3} \Sum{k=2}{n+1}  \ln \left(
        \frac{ k }{ n } \right)  \\
      & = & \frac{1}{n} \Sum{k=2}{n+1} \frac{ k^2 }{ n^2 } \ln \left(
        \frac{ k }{ n } \right) - \frac{ 2 }{ n } \times \frac{ 1 }{
        n } \Sum{k=2}{n+1} \frac{ k }{ n } \ln \left( \frac{ k }{ n }
      \right) + \frac{1}{n^2} \times \frac{ 1 }{ n } \Sum{k=2}{n+1}
      \ln \left( \frac{ k }{ n } \right)  
    \end{eqnarray*}
    La première somme de Riemann tend à nouveau vers $- \frac{ 1 }{
      9}$ avec les sommes de Riemann. La deuxième tend vers $\int_0^1
    g(x) \ dx $ en posant $g(x) = x \ln x$ qui se prolonge à nouveau
    par continuité en 0, donc cette intégrale est une constante finie
    et le deuxième terme tend vers 0 (on multiplie par une quantité
    qui tend vers 0).\\
    Enfin le troisième terme ne peut pas s'écrire comme une somme de
    Riemann (car $h(x) = \ln x$ ne se prolonge pas par continuité en
    0). Par contre on peut encadrer ce terme entre $ \frac{ 1 }{ n^3 }
    \ln \left( 1 + \frac{ 1 }{ n } \right)$ (majorant, avec tous les
    autres termes négatifs) qui tend vers 0 (DL) et avec $k \leq n$ :
    \[
    \frac{ 1 }{ n^2} \Sum{k=2}{n} \frac{ k }{ n } \ln \left( \frac{ k
      }{ n } \right) + \frac{ 1 }{ n^3 } \ln \left( 1 + \frac{ 1 }{ n
      } \right)
    \]
    (minorant, qui tend aussi vers 0 pour les mêmes raisons que le
    second terme) et conclure par encadrement.
  \end{noliste}
\end{exercice}


\newpage


\begin{exercice}{\it (Exercice avec préparation)}~
  \begin{noliste}{1.}
  \item 
    \begin{noliste}{}
    \item Soit $(\Omega, \A, \Prob)$ un espace probabilisé.
    \item Soient $X$ et $Y$ deux \var discrètes.
    \end{noliste}
    \begin{noliste}{$\sbullet$}
    \item Les {\bf \var $X$ et $Y$ sont indépendantes} (pour la
      probabilité $\Prob$) si :
      \[
      \forall x \in X(\Omega), \forall y \in Y(\Omega), \ \Prob(\Ev{X
        = x} \, \cap \, \Ev{Y = y}) = \Prob(\Ev{X = x}) \times
      \Prob(\Ev{Y = y})
      \]
    \item Autrement dit, les \var $X$ et $Y$ sont indépendantes si pour
      tout $x \in X(\Omega)$ et tout $y \in Y(\Omega)$, les événements
      $\Ev{X = x}$ et $\Ev{Y = y}$ sont indépendants.
    \end{noliste}

  \item 
    \begin{noliste}{a)}
    \item $X$ représente les nombres de succès (obtention de 1) lors
      de la succession de $n$ épreuves de Bernoulli (lancers du dé)
      indépendantes et de même paramètre $p$. Ainsi : $X \suit
      \Bin{n}{p}$.\\
      En considérant l'obtention de $2$ comme succès, on obtient : $Y
      \suit \Bin{n}{q}$.

    \item D'après la question précédente, $X(\Omega) = Y (\Omega) =
      \llb 0, n \rrb$.\\
      Soit $(k, j) \in \llb 0, n \rrb^2$. On procède alors par
      disjonciton de cas.
      \begin{noliste}{$\sbullet$}
      \item \dashuline{Si $k+j > n$} alors $\Ev{X=k} \cap \Ev{Y=j} =
        \varnothing$.\\
        En effet, on ne peut obtenir plus de résultats que de
        lancers. Ainsi :
        \[
        \Prob(\Ev{X=k} \cap \Ev{Y=j}) = 0
        \]

      \item \dashuline{Si $0 \leq k+j \leq n$} alors un tirage qui
        réalise $\Ev{X=k} \cap \Ev{Y=j}$ contient exactement $k$ fois
        le nombre 1, $j$ fois le nombre 2 et donc $n-k-j$ fois le
        nombre 3.\\[.2cm]
        Ainsi, chaque tirage réalisant $\Ev{X=k} \cap \Ev{Y=j}$ est
        entièrement déterminé par :
        \begin{noliste}{$\stimes$}
        \item la position des $k$ nombres 1 : $\dbinom{n}{k}$ possibilités.
        \item la position des $j$ nombres 2 dans les places restantes :
          $\dbinom{n-k}{j}$ possibilités.
        \item la position des $n-k-j$ nombres 3 dans les places
          restantes : $\dbinom{n-k-j}{n-k-j} = 1$ possibilité.
        \end{noliste}
        Il y a donc : 
        \[
        \dbinom{n}{k} \ \dbinom{n-k}{j} = \dbinom{n}{k} \binom{n-k}{j}
        \times 1 = \dfrac{n!}{k! \ \bcancel{(n-k)!}} \
        \dfrac{\bcancel{(n-k)!}}{j!(n-k-j)!} = \dfrac{n!}{k! \ j! \
          (n-k-j)!}
        \]
        tels tirages.\\[.2cm]
        De plus, la probabilité d'un tirage ne dépend que du nombre de
        1, 2 et 3 qu'il contient. Ainsi, les tirages qui réalisent
        $\Ev{X=k} \cap \Ev{Y=j}$ ont tous la même probabilité
        d'apparaître, qui est la probabilité d'apparition du tirage :
        \[
        \begin{array}{cccccccccccc}
          1 & \ldots & \ldots & \ldots & 1 & 2 & \quad \ldots \quad & 2 & 3 &
          \quad \ldots \quad & \quad \ldots \quad & 3 \\[-.4cm]  
          \multicolumn{5}{c}{\bbacc{3cm}} &
          \multicolumn{3}{c}{\bbacc{2cm}} &
          \multicolumn{4}{c}{\bbacc{3.2cm}} \\[.4cm]
          \multicolumn{5}{c}{\text{$k$ occurrences}} &
          \multicolumn{3}{c}{\text{$j$ occurrences}} &
          \multicolumn{4}{c}{\text{$n-k-j$ occurrences}}
        \end{array}
        \]
        Ainsi, la probabilité d'apparition d'un tel tirage est : $p^k
        \ q^j \ (1-p-q)^{n-k-j}$.\\
        On en déduit que :
        \[
        \Prob(\Ev{X=k} \cap \Ev{Y=j}) = \dfrac{n!}{k! \ j! \ (n-k-j)!}
        \ p^k \ q^j \ (1-p-q)^{n-k-j}
        \]
      \end{noliste}
      
    \item On remarque tout d'abord que $\Ev{X=n} \cap \Ev{Y=n} =
      \varnothing$ car on ne peut obtenir à la fois $n$ fois le nombre
      1 et $n$ fois le nombre 2 en seulement $n$ lancers. Ainsi :
       \[
       \Prob(\Ev{X=n} \cap \Ev{Y=n}) = 0 \neq \Prob(\Ev{X=n}) \times
       \Prob(\Ev{Y=n})
       \]
       et $X$ et $Y$ ne sont donc pas indépendantes. 


       \newpage

       
     \item La \var $T_n$ admet une espérance (resp. une variance) car
       la \var $X$ en admet une.\\
       De plus, par linéarité de l'espérance et propriété de la
       variance :
       \[
       \E_p(T_n) = \dfrac{1}{n+1} \ \E(X) = \dfrac{n p}{n + 1} \qquad
       \text{ et } \qquad \V_p(T_n) = \dfrac{1}{(n+1)^2} \ \V(X) =
       \dfrac{n p (1-p)}{(n+1)^2}
       \]
       Comme $T_n$ admet une espérance, $T_n$ admet un biais :
       \[
       \begin{array}{rcl}
         b_p(T_n) & = & \E_p(T_n - p) \ = \ \E_p(T_n) - p \\[.2cm]
         & = & \dfrac{n p}{n+1} - p \ = \ \left( \dfrac{n}{n+1} - 1
         \right) \ p \ = \ \dfrac{n - (n+1)}{n+1} \ p \ = \ -
         \dfrac{p}{n+1}
       \end{array}       
       \]
       Comme $T_n$ admet une variance, $T_n$ admet un risque
       quadratique. Par la formule de décomposition biais variance, on
       obtient :
       \[
       \begin{array}{rcl}
         r_p(T_n) & = & \V_p(T_n) + (b_p(T_n))^2 \\[.2cm]
         & = & \dfrac{n p (1-p)}{(n+1)^2} + \left(
           -\dfrac{p}{n+1}\right)^2 \\[.4cm]
         & = & \dfrac{n p (1-p)}{(n+1)^2} + \dfrac{p^2}{(n+1)^2} \ = \
         \dfrac{p \ (n(1-p) + p)}{(n+1)^2} 
       \end{array}
       \]
     \end{noliste}

   \item 
     \begin{noliste}{a)}
     \item Pour tout $n \in \N$, si $N = n$ alors la \var $X$ prend
       ses valeurs dans $\llb 0, n \rrb$. On en déduit que :
       \[
       X(\Omega) = \dcup{n = 0}{+ \infty} \llb 0, n \rrb = \N
       \]
       La famille $(\Ev{N=n})_{n \in \N}$ forme un système complet
       d'événements.\\
       On en déduit, par la formule des probabilités totales que, pour
       tout $k \in \N$ :
       \[
       \begin{array}{rcl@{\quad}>{\it}R{5cm}}
         \Prob(\Ev{X=k}) & = & \Sum{n=0}{+\infty} \Prob(\Ev{N=n} \cap
         \Ev{X=k}) \\[.4cm]
         & = & \Sum{n=k}{+\infty} \Prob(\Ev{N=n} \cap
         \Ev{X=k}) & (car si $k > n$ alors \\ $\Ev{N=n} \cap \Ev{X=k}
         = \varnothing$) \nl
         \\[-.2cm]
         & = & \Sum{n=k}{+\infty} \Prob(\Ev{N=n}) \
         \Prob_{\Ev{N=n}}(\Ev{X=k}) & (car $\Prob(\Ev{N=n}) \neq 0$)
         \nl
         \\[-.2cm]
         & = & \Sum{n=k}{+\infty} \dfrac{\lambda^n \ \ee^{-
             \lambda}}{\bcancel{n!}} \times \dfrac{\bcancel{n!}}{k!
           (n-k)!} \ p^k (1-p)^{n-k} 
         \\[.6cm]
         & = & \dfrac{p^k \ \ee^{-\lambda}}{k!} \ \Sum{n=k}{+\infty}
         \dfrac{\lambda^n}{(n-k)!} (1-p)^{n-k} 
         \\[.6cm]
         & = & \dfrac{p^k \ \ee^{-\lambda}}{k!} \ \Sum{n=0}{+\infty}
         \dfrac{\lambda^{n+k}}{n!} (1-p)^{n} \ = \ \dfrac{\lambda^k \ p^k \
           \ee^{-\lambda}}{k!} \ \Sum{n=0}{+\infty}
         \dfrac{\lambda^n}{n!} (1-p)^{n} & (par décalage d'indice) 
         \nl
         \\[-.2cm]         
         & = & \dfrac{\lambda^k \ p^k \ \ee^{ - \lambda }}{k!} \ \ee^{
           \lambda (1-p) } \ = \ \dfrac{(\lambda p)^k \ \ee^{- \lambda +
             \lambda - \lambda p}}{k!} \ = \ \dfrac{(\lambda p)^k \ 
           \ee^{- \lambda p}}{k!}
       \end{array}
       \]
       On en déduit que : $X \suit \Pois{\lambda p}$.\\
       En raisonnant de même pour $Y$, on obtient : $Y \suit
       \Pois{\lambda p}$.


       \newpage

       
     \item Soit $(k, j) \in \N^2$. La famille $(\Ev{N=n})_{n \in \N}$
       forme un système complet d'événements.\\
       On en déduit, par la formule des probabilités totales que :
       \[
       \begin{array}{cl@{\quad}>{\it}R{5cm}}
         & \Prob(\Ev{X=k} \cap \Ev{Y=j}) \\[.2cm]
         = & \Sum{n=0}{+\infty} \Prob(\Ev{N=n} \cap \Ev{X=k} \cap \Ev{Y=j}) 
         \\[.4cm]
         = & \Sum{n=j+k}{+\infty} \Prob(\Ev{N=n} \cap \Ev{X=k} \cap
         \Ev{Y=j}) & (car si $j+k > n$ alors \\ $\Ev{N=n} \cap \Ev{X=k} \cap
         \Ev{Y=j}$) 
         \nl
         \\[-.2cm]
         = & \Sum{n=j+k}{+\infty} \Prob(\Ev{N=n}) \
         \Prob_{\Ev{N=n}}(\Ev{X=k} \cap \Ev{Y=j}) & (car
         $\Prob(\Ev{N=n}) \neq 0$) 
         \nl
         \\[-.2cm]
         = & \Sum{n=j+k}{+\infty} \dfrac{\lambda^n \ \ee^{-
             \lambda}}{\bcancel{n!}} \times \dfrac{\bcancel{n!}}{j! \
           k! \ (n-k-j)!} \ p^k q^j (1-p-q)^{n-j-k} & (d'après la
         question \itbf{2.b)})
         \nl
         \\[-.2cm]
         = & \dfrac{p^k \ q^j \ \ee^{ - \lambda}}{j! \ k!} \
         \Sum{n=j+k}{+\infty} \lambda^n \ \dfrac{(1-p-q)^{n-j-k}}{(n-k-j)!}  
         \\[.6cm]
         = & \dfrac{p^k \ q^j \ \ee^{ - \lambda}}{j! \ k!} \
         \Sum{n=0}{+\infty} \lambda^{n+j+k} \ \dfrac{(1-p-q)^{n}}{n!}
         & (par décalage d'indice) \nl
         \\[-.2cm]
         = & \dfrac{\lambda^{j+k} \ p^k \ q^j \ \ee^{-\lambda}}{j! \ k!} \
         \Sum{n=0}{+\infty} \lambda^{n} \ \dfrac{(1-p-q)^{n}}{n!}
         \\[.6cm]
         = & \dfrac{\lambda^{j+k} \ p^k \ q^j \ \ee^{- \lambda}}{j! \
           k!} \ \ee^{\lambda (1-p-q)} \ = \ \dfrac{\lambda^{j+k} \
           p^k \ q^j \ \bcancel{\ee^{- \lambda}}}{j! \ k!} \
         \bcancel{\ee^{\lambda}} \ \ee^{-\lambda p} \ \ee^{- \lambda q}
         \\[.6cm]
         = & \dfrac{(\lambda p)^k \ \ee^{- \lambda p}}{k!} \times
         \dfrac{(\lambda q)^j \ \ee^{- \lambda q}}{j!} 
         \\[.6cm]
         = & \Prob(\Ev{X = k}) \ \Prob(\Ev{Y = j})
       \end{array}
       \]
       On en conclut que $X$ et $Y$ sont indépendantes.

     \item Sous réserve de convergence, le théorème de transfert
       permet d'affirmer :
       \[
       \begin{array}{rcl@{\quad}>{\it}R{5cm}}
         \E\left( \dfrac{X}{N + 1} \right) & = & \Sum{(n, k) \in
           N(\Omega) \times X(\Omega)}{} \dfrac{k}{n+1} \ \Prob(\Ev{N
           = n} \cap \Ev{X = k})
         \\[.6cm]
         & = & \Sum{n=0}{+\infty} \Sum{k=0}{n} \dfrac{k}{n+1}
         \dfrac{\lambda^n \ \ee^{ - \lambda}}{n!} \times \dbinom{n}{k}
         p^k \ (1-p)^{n-k} 
         \\[.6cm] 
         & = & \Sum{n=0}{+\infty} \dfrac{\lambda^n \ \ee^{- \lambda}}{
           (n+1)!} \ \Sum{k=0}{n} k \dbinom{n}{k} p^k q^{n-k}  
         \\[.6cm] 
         & = & \Sum{n=0}{+\infty} \dfrac{\lambda^n \ \ee^{- \lambda}}{
           (n+1)!} \ \E(W) & (si $W$ est une \var \\ telle que $T \suit
         \Bin{n}{p}$) 
         \nl
         \\[-.2cm] 
         & = & \Sum{n=0}{+\infty} \dfrac{\lambda^n \ \ee^{- \lambda}}{
           (n+1)!} \ n p 
         \\[.6cm]
         & = & p \ \ee^{-\lambda} \ \Sum{n=0}{+\infty}
         \dfrac{\lambda^n}{(n+1)!} \ n 
       \end{array}
       \]
       Enfin :
       \[
       \dfrac{n}{(n+1)!} \ = \ \dfrac{(n+1 - 1)}{(n+1)!} \ = \
       \dfrac{n+1}{(n+1)!} - \dfrac{1}{(n+1)!} = \dfrac{1}{n!} -
       \dfrac{1}{(n+1)!}
       \]


       \newpage


       \noindent
       Et ainsi :
       \[
       \begin{array}{rcl}
         \E\left( \dfrac{X}{N + 1} \right) & = & p \ \ee^{ - \lambda}
         \ \left( \Sum{n=0}{+\infty} \dfrac{\lambda^n}{n!} -
           \Sum{n=0}{+\infty}  \dfrac{\lambda^n}{(n+1)!} \right) 
         \\[.6cm]
         & = & p \ \ee^{- \lambda} \left( \ee^{\lambda} -
           \Sum{n=1}{+\infty} \dfrac{\lambda^{n-1}}{n!} \right) \ = \ 
         p \ \ee^{- \lambda} \left( \ee^{\lambda} - \dfrac{1}{\lambda}
           \left( \Sum{n=0}{+\infty} \dfrac{\lambda^n}{n!} - 1
           \right) \right)  
         \\[.6cm]
         & = & p \ \ee^{- \lambda} \left(\ee^{\lambda} -
           \dfrac{\ee^{\lambda} - 1}{\lambda} \right) = p \ \left( 1 - 
           \dfrac{1 - \ee^{ - \lambda}}{\lambda} \right)  
       \end{array}
       \]
       La réserve est ainsi levée et $\E\left( \dfrac{X}{n + 1}
       \right) = p \ \left( 1 - \dfrac{1 - \ee^{ - \lambda}}{\lambda}
       \right)$.\\[.2cm]
       Comme la \var $T$ admet une espérance, elle admet aussi un
       biais :
       \[
       \begin{array}{rcl}
         b_p(T) & = & \E_p(T - p) \ = \ \E_p(T_n) - p \\[.2cm]
         & = & p \ \left(1 - \dfrac{1 - \ee^{- \lambda}}{\lambda}
         \right) - p \ = \ p \dfrac{\ee^{- \lambda} - 1}{\lambda} 
         \ \neq \ 0
       \end{array}       
       \]
       donc $T$ n'est pas un estimateur sans biais de $p$.
     \end{noliste}
     \begin{remark}~
       \begin{noliste}{$\sbullet$}
       \item La question précédente était 
         
       \item 
         Question extrêmement difficile. On peut répondre très
         rapidement dans le cas $p=q=\frac{1}{3}$ par une astuce : en
         posant $Z$ le nombre de 3 obtenus, on a :
         \[
         X + Y + Z = N
         \]
         
         donc par linéarité :
         \[
         E \left( \frac{ X + Y + Z }{ N + 1 } \right) = E \left(
           \frac{ X }{ N + 1 } \right) + E \left( \frac{ Y }{ N + 1 }
         \right) + E \left( \frac{ Z }{ N + 1 } \right) = E \left(
           \frac{ N }{ N + 1 } \right) = 1 - E \left( \frac{ 1 }{ N +
             1 } \right) .
         \]
         
         Or dans ce cas, $X$, $Y$ et $Z$ suivent la même loi de
         Poisson de paramètre $\frac{ \lambda }{ 3 }$ donc on en
         déduit que :
         \[
         E \left( \frac{ X }{ N + 1 } \right) = \frac{ 1 }{ 3 } - E
         \left( \frac{ 1 }{ 3 ( N + 1 ) } \right)
         \]
         
         et avec $N +1 > 0$, on obtient $E \left( \frac{ 1 }{ 3 (N+1)
           } \right) > 0 $ donc $E \left( \frac{ X }{ N+1 } \right)
         \neq \frac{ 1 }{ 3 }$ : on en déduit que $E \left( \frac{ X
           }{ N+1 } \right) $ ne peut être égale à $p$ pour tout $p$,
         donc que $\frac{ X }{ N + 1 }$ n'est pas un estimateur sans
         biais de $p$.
       \end{noliste}
     \end{remark}
   \end{noliste}
 \end{exercice}
 
\addtocounter{exercice}{-1}
\begin{exercice}{\it (Exercice sans préparation)}~\\
  Soit $A$ une matrice carrée de $\mathcal{M}_3 ( \R ) $.
  \begin{noliste}{1.}
  \item On suppose que $A$ est diagonalisable, alors il existe $P$
    inversible et $D$ diagonale telles que :
    \[
    A = P D P^{-1} 
    \]    
    On obtient alors sans difficulté :
    \[
    A^3 = P D P^{-1} P D P^{-1} P D P^{-1} = P D I D I D P^{-1} = P D^3 P^{-1 } 
    \]    
    avec $P$ inversible et $D^3$ diagonale, comme puissance d'une
    matrice diagonale, donc $A^3$ est bien diagonalisable.

  \item On suppose dans cette question que $A = \begin{smatrix} 0 & 0
      & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0
      \\ \end{smatrix}$. 
    \begin{noliste}{a)}
    \item On obtient sans difficulté :
      \[
      A^3 = I
      \]
      
    \item On en déduit que le polynôme $X^3 - 1$ est annulateur de
      $A$, et on a :
      \[
      X^3 - 1 = 0 \Longleftrightarrow X^3 = 1 \Longleftrightarrow X =
      1
      \]      
      donc 1 est la seule valeur propre possible de $A$. \\

      Si $A$ était diagonalisable, 1 serait forcément valeur propre,
      et il existerait $P$ inversible et $D$ diagonale telle que $ A =
      P D P^{-1}$.

      Enfin $D$ ne peut comporter que des valeurs propres de $A$ sur
      sa diagonale, donc $D = I$ et enfin :
      \[
      A = P I P^{-1} = I 
      \]
      
      qui est absurde, donc $A$ n'est pas diagonalisable.
      
    \end{noliste}
  \end{noliste}
\end{exercice}


 \newpage


 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item On peut écrire, avec $\varphi$ la densité de la loi normale centrée réduite : 
     \[
     P ( X \in [a;b] ) = P ( a \leq X \leq b ) = \int_a^b \varphi (t) \ dt = \frac{ 1 }{ \sqrt{ 2 \pi } } \int_a^b e^{ - \frac{ t^2 }{ 2 } } \ dt . 
     \]

     Cette probabilité apparaît comme limite dans le théorème central limite : si $X_n$ est une suite de variables aléatoires indépendantes et identiquement distribuées, admettant une espérance $m$ et une variance $\sigma^2$ commune, alors la somme centrée réduite des $X_i$ converge en loi vers la loi normale centrée réduite, donc : 
     \[
     P \left( \frac{ \Sum{i=1}{n} X_i - n m }{ \sigma \sqrt{n} } \in [a ; b ] \right) \xrightarrow[ n \rightarrow + \infty ]{} \frac{ 1 }{ \sqrt{ 2 \pi } } \int_a^b e^{ - \frac{ t^2 }{ 2 } } \ dt . 
     \]

     Soit $X$ une variable aléatoire définie sur un espace probabilisé $(\Omega , \mathcal{A} , P)$ suivant la loi normale centrée réduite. On note $\Phi$ la fonction de répartition de de $X$. On pose $Y = \vert X \vert$ (valeur absolue de $X$).

   \item \begin{enumerate}

     \item Par théorème de transfert, on s'intéresse à l'intégrale : 
       \[
       \int_{-\infty}^{+\infty} \vert t \vert \varphi (t) \ dt = 2 \int_0^{+\infty}  \vert t \vert \varphi (t) \ dt  = \frac{ 2 }{ \sqrt{ 2 \pi } } \int_0^{+\infty} t e^{ - \frac{ t^2 }{ 2 } } \ dt 
       \]

       par parité de $\varphi$ et de la valeur absolue. On passe alors par l'intégrale partielle : 
       \[
       \int_0^A t e^{ - \frac{ t^2 }{ 2 } } \ dt = \left[ - e^{ - \frac{ t^2 }{ 2 } } \right]_0^A = 1 - e^{ - \frac{ A^2 }{ 2 } } \xrightarrow[ A \rightarrow + \infty ]{} 1 . 
       \]

       Cette intégrale est donc convergente, et comme la fonction intégrée est positive, elle converge absolument. On en déduit que $Y$ admet une espérance et que : 
       \[
       E ( Y ) = \frac{ 2 }{ \sqrt{ 2 \pi } } \times 1 = \sqrt{ \frac{ 2 }{ \pi } } . 
       \]

       Pour la variance, on s'intéresse au moment d'ordre 2 de $Y$, donc à : 
       \[
       E ( Y^2 ) = E [ \vert X \vert^2 ] = E ( X^2 ) = V (X) + [ \E(X) ]^2 = 1 + 0^2 . 
       \]

       On en déduit que $Y$ admet un moment d'ordre deux et donc une variance (car $x$ admet un moment d'ordre 2) et que : 
       \[
       V ( Y ) = 1 - \left( \sqrt{ \frac{ 2 }{ \pi } } \right)^2 = 1 - \frac{ 2 }{ \pi } = \frac{ \pi - 2 }{ \pi } . 
       \]

     \item De nouveau avec le théorème de transfert, avec $X Y = X \vert X \vert$, on s'intéresse à l'intégrale :
       \[
       \int_{-\infty}^{+\infty} t \times \vert t \vert \varphi (t) \ dt 
       \]

       La fonction intégrée est clairement impaire : si l'intégrale est convergente, alors elle vaudra 0. De plus elle converge si et seulement si l'intégrale suivante converge :
       \[
       \int_0^{ + \infty }  t \vert t \vert \varphi (t) \ dt = \int_0^{+\infty} t^2 \varphi (t) \ dt 
       \]

       qui converge puisque la loi normale centrée réduite admet un moment d'ordre deux. On en déduit que $X Y$ admet une espérance et que : 
       \[
       E ( X Y ) = 0 . 
       \]

     \end{enumerate}

   \item On pose $Z = X + Y$. \begin{enumerate}

     \item On remarque que 
       \[
       Z = X + \vert X \vert = \left\{ \begin{array}{cl} X - X & \text{ si } X \leq 0 \\ X + X & \text{ si } X \geq 0 \\ \end{array} \right. = \left\{ \begin{array}{cl} 0 & \text{ si } X \leq 0 \\ X + X & \text{ si } X \geq 0 \\ \end{array} \right. 
       \]

       donc on obtient : 
       \[
       Z = 0 \Longleftrightarrow X \leq 0 \ \ \text{ ou } \ \ X \geq 0 \text{ et } 2 X = 0 \Longleftrightarrow X = 0 
       \]

       et finalement : 
       \[
       \Ev{Z = 0 } = \Ev{X \leq 0 } \ \ \text{ donc } \ \ P \Ev{ Z = 0 } = \Phi ( 0 ) = \frac{ 1 }{ 2 } . 
       \]

     \item D'après ce qu'on a vu précédemment, $Z$ est nulle quand $X$ est négative, et vaut $2X$ quand $X$ est positive, donc prend toutes les valeurs de $\R_+$ dans ce cas. On en déduit que $Z ( \Omega ) = \R_+$, donc :
       \begin{itemize}

       \item $ \forall x < 0 $ (avant le support), 
         \[
         F_Z ( x )= P \Ev{ Z \leq x } = 0 
         \]

       \item Pour $x = 0$,
         \[
         F_Z (0) = P \Ev{ Z \leq 0 } = P \Ev{ Z < 0 } + P \Ev{ Z = 0 } = 0 + \frac{ 1 }{ 2 } = \frac{ 1 }{ 2 } . 
         \]

       \item Pour $x > 0$, en utilisant le sce $\Ev{X \leq 0} , \Ev{X > 0}$ : 
         \begin{eqnarray*}
           \Ev{ Z \leq x } & = & [ \Ev{X \leq 0} \cap \Ev{Z \leq x } ] \cup [ \Ev{X > 0} \cap \Ev{Z \leq x } ] = [ \Ev{X \leq 0 } \cap \Ev{ 0 \leq x } ] \cup [ \Ev{X > 0 } \cap \Ev{ 2 X \leq x } ] \\ \\
           & = & [ X \leq 0 ] \cup \left[ 0 < X \leq \frac{ x }{ 2 } \right] = \left( X \leq \frac{ x }{ 2 } \right)
         \end{eqnarray*}

         ce qui donne : 
         \[
         F_Z ( x ) = \Prob(\Ev{ Z \leq x }) = P \left( X \leq \frac{ x }{ 2 } \right) = \Phi \left( \frac{ x }{ 2 } \right) 
         \]

       \end{itemize}

       et en rassemblant : 
       \[
       F_Z ( x ) = \left\{ \begin{array}{cl} 0 & \text{ si } x < 0 \\ \frac{1}{2} & \text{ si } x = 0 \\ \Phi \left( \frac{ x }{ 2 } \right) & \text{ si } x > 0 \\ \end{array} \right. 
       \]


     \item $Z$ n'est pas à densité puisqu'il existe une valeur (0) pour laquelle la probabilité de tomber "pile" est non nulle. Elle n'est pas discrète car son support ($\R_+$) est un intervalle. \\

     \end{enumerate}

   \item Soit $y \in \R$. \begin{enumerate}

     \item Lorsque $y< 0$, l'évènement $\Ev{ Y \leq y} $ est impossible donc :
       \[
       P [ \Ev{X \leq 1 } \cap \Ev{Y \leq y } ] = 0 . 
       \]

       Lorsque $y \geq 0$, on a : 
       \[
       [ \Ev{X \leq 1 } \cap \Ev{Y \leq y} ] = [ \Ev{ X \leq 1 } \cap ( \vert x \vert \leq y ) ] = [ \Ev{ X \leq 1 } \cap ( -y \leq X \leq y ) ] = [ -y \leq X \leq \min (1 , y) ] . 
       \]

       On en déduit que pour $0 \leq y \leq 1$ :
       \[
       P [ \Ev{X \leq 1 } \cap \Ev{Y \leq y} ] = P [ -y \leq X \leq y ] = \Phi ( y) - \Phi ( -y) = \Phi (y) - [ 1 - \Phi (y) ] = 2 \Phi ( y) - 1 . 
       \]

       Enfin pour $y > 1$ on a :
       \[
       P [ \Ev{X \leq 1 } \cap \Ev{Y \leq y} ] = P [ -y \leq X \leq 1 ] = \Phi ( 1) - \Phi ( -y) = \Phi (1) - [ 1 - \Phi (y) ] = \Phi (1) + \Phi ( y) - 1 . 
       \]

     \item On calcule dans les 3 cas : \begin{itemize}

       \item Si $y< 0$,
         \[
         P \Ev{ X \leq 1 } P \Ev{ Y \leq y } = \Phi (1) \times 0 = 0 = P [ \Ev{X \leq 1 } \cap \Ev{Y \leq y } ] 
         \]

         donc les évènements $\Ev{X \leq 1}$ et $\Ev{Y \leq y}$ dont indépendants. \\

       \item Si $0 \leq y \leq 1$,
         \[
         P \Ev{ X \leq 1 } P \Ev{ Y \leq y } = \Phi ( 1 ) P ( - y \leq X \leq y ) = \Phi (1) \times \left[ 2 \Phi (y) - 1 \right] \neq P [ \Ev{X \leq 1 } \cap \Ev{Y \leq y } ] 
         \]

         (sauf si $ 2 \Phi ( y) - 1 = 0$) car $\Phi (1) < 1$ ($\varphi$ n'est pas nulle après 1, donc la probabilité d'être supérieure à 1 est non nulle), et les évènements $\Ev{X \leq 1}$ et $\Ev{Y \leq y}$ ne sont pas indépendants, sauf pour 
         \[
         2 \Phi (y) - 1 = 0 \Longleftrightarrow \Phi (y) = \frac{ 1 }{ 2 } \Longleftrightarrow y = 0 . 
         \]

       \item Si $y > 1$,
         \[
         P \Ev{ X \leq 1 } P \Ev{ Y \leq y } = \Phi ( 1 ) P ( - y \leq X \leq y ) = \Phi (1) \times \left[ 2 \Phi (y) - 1 \right] 
         \]

         donc
         \begin{eqnarray*}
           P \Ev{ X \leq 1 } P \Ev{ Y \leq y } = P [ \Ev{X \leq 1 } \cap \Ev{Y \leq y } ] & \Longleftrightarrow & \Phi (1) \times \left[ 2 \Phi (y) - 1 \right] = \Phi (1) + \Phi ( y) - 1 \\ \\
           & \Longleftrightarrow & \Phi (y) \left( 2 \Phi (1) - 1 \right) = 2 \Phi (1) - 1  \Longleftrightarrow \Phi (y) = 1 
         \end{eqnarray*}

         qui est absurde car $\Phi$ est strictement croissante donc n'atteint jamais 1, qui est sa limite en $+\infty$.

       \end{itemize}

       Finalement les évènements $\Ev{X \leq 1}$ et $\Ev{Y \leq y}$ sont indépendants si et seulement si $y \leq 0$.

     \end{enumerate}

   \end{enumerate}

   \indent

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $A$ une matrice de $\mathcal{M}_2 (\R)$ telle que $A^3=0$.
   \begin{enumerate}

   \item Question très difficile. On suppose que $A^2 \neq 0$, alors en posant $u$ l'application linéaire de $\R^2$ dont la matrice dans la base canonique est $A$, on a $u^2 \neq 0$ et $u^3 = 0$. \\

     On en déduit alors qu'il existe un vecteur $x$ dans $\R^2$ tel que $u^2 (x) \neq 0$, et on étudie la liberté de la famille $(x , u (x) , u^2 (x) )$  : on considère $a$, $b$ et $c$ tels que 
     \[
     a x + b u(x) + c u^2 (x) = 0 
     \]

     alors en composant par $u$ et par $u^2$, on obtient (avec $u(0) = u^2 (0) = 0$ car ce sont des applications linéaires) : 
     \[
     a u(x) + b u^2 (x) = 0  \ \ \text{ et } \ \ a u^2 (x) = 0 . 
     \]

     De plus $u^2 (x) \neq 0$, donc on obtient $a=0$. On en déduit alors que : 
     \[
     b u(x) + c u^2 (x) = 0 \ \ \text{ et } \ \ b u^2 (x) = 0 
     \]

     On obtient de même $b=0$, puis $c u^2 (x) = 0$, et enfin $c=0$, donc la famille $(x , u(x) , u^2 (x) )$ est libre; or elle comporte trois vecteurs dans un espace de dimension 2, elle ne peut être libre : c'est absurde et on obtient donc : 
     \[
     A^2 = 0 . 
     \]

   \item On peut conclure à la première partie par caractérisation des sous-espaces vectoriels. Mais comme la dimension est demandée, il faudra de toute manière expliciter l'espace. On sépare deux cas : \begin{itemize}

     \item si $A=0$, alors $A M  = 0 = M A$ est vrai pour toute matrice $M$, donc l'ensemble considéré est $\mathcal{M}_2 ( \R )$, qui est bien un espace vectoriel, de dimension 4. \\

     \item si $A \neq 0$, un raisonnement analogue à la question 1 montre qu'il existe $x$ dans $\R^2$ tel que la famille $(x , u(x) )$ est une base de $\R^2$, et dans cette base la matrice de $u$ est : 
       \[
       N = \begin{smatrix} 0 & 0 \\ 1 & 0 \\ \end{smatrix} . 
       \]

       On en déduit par formule de changement de base qu'il existe une matrice $P$ inversible (constituée des coordonnées de $x$ et $u(x)$, en colonnes, dans la base canonique) telle que :
       \[
       A = P N P^{-1} . 
       \]

       On montre alors que l'équation recherchée se ramène à : 
       \begin{eqnarray*}
         A M = M A & \Longleftrightarrow & P N P^{-1} M = M P N P^{-1} \Longleftrightarrow P^{-1} [ P N P^{-1} M ] P = P^{-1} [ M P N P^{-1} ] P \\ \\
         & \Longleftrightarrow & N ( P ^{-1} M P ) = ( P^{-1} M P ) N . 
       \end{eqnarray*}

       On pose alors $P^{-1} M P = \begin{smatrix} a & b \\ c & d \\ \end{smatrix} $ et on obtient : 
       \begin{eqnarray*}
         A M = M A & \Longleftrightarrow & \begin{smatrix} 0 & 0 \\ 1 & 0 \\ \end{smatrix} \begin{smatrix} a & b \\ c & d \\ \end{smatrix} = \begin{smatrix} a & b \\ c & d \\ \end{smatrix} \begin{smatrix} 0 & 0 \\ 1 & 0 \\ \end{smatrix} \Longleftrightarrow \begin{smatrix} 0 & 0 \\ a & b \\ \end{smatrix} = \begin{smatrix} b & 0 \\ d & 0 \\ \end{smatrix} \\ \\
         & \Longleftrightarrow & \left\{ \begin{array}{c} b = 0 \\ a = d \\ \end{array} \right. \Longleftrightarrow P^{-1} M P = \begin{smatrix} a & 0 \\ c & a \\ \end{smatrix} = a I + c N \\ \\
         & \Longleftrightarrow & M = P ( a I + c N ) P^{-1} = a P I P^{-1} + c P N P^{-1} = a I + c A .
       \end{eqnarray*}

       On en déduit que l'ensemble cherché est $\Vect{ I , A }$ qui est un espace vectoriel, et la famille $(I,A)$ en est génératrice. Elle est de plus libre si et seulement si $A$ n'est pas colinéaire à $I$. Si c'était le cas, on aurait : 
       \[
       A = \lambda I \ \ \text{ donc } \ \ A^3 = \lambda^3 I = 0 \ \ \text{ donc } \ \ \lambda^3 = 0 \ \ \text{ donc } \ \ \lambda = 0 
       \]

       et enfin $A = 0 I = 0$ ce qui est absurde. Cette famille est donc libre, c'est une base de l'espace vectoriel donc celui-ci est de dimension 2. \\

       Remarque : on pouvait éviter ce raisonnement par l'absurde en écrivant : 
       \[
       a I + b A = 0 \Longleftrightarrow P^{-1} ( a P I P^{-1} + b P N P^{-1} ) P = 0 \Longleftrightarrow a I + b N = 0 
       \]

       qui donne immédiatement $a=c=0$.

     \end{itemize}

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une suite de variables aléatoires $(X_n)_{ n \in \N }$ converge en loi vers une variable $X$ si pour tout $x$ réel tel que $F_X$ est continue en $x$, on a : 
     \[
     \dlim{ n \rightarrow + \infty } F_{ X_n } (x) = F_X ( x) . 
     \]

     Cas particulier des variables discrètes (la définition précédente est toujours valable, mais il y a une caractérisation beaucoup plus simple). S'il existe un ensemble \textbf{discret} I tel que les supports de tous les $X_n$ et de $X$ sont inclus dans I (en général on trouve I comme "limite" des supports des $X_n$), $(X_n)$ converge en loi vers $X$ si et seulement si : 
     \[
     \forall i \in I , \ \dlim{ n \rightarrow + \infty } P \Ev{ X_n = i } = P \Ev{ X = i } . 
     \]

     Soit $(X_n)_{ n \in \N }$ une suite de variables aléatoires indépendantes définies sur un espace probabilité $(\Omega , \mathcal{A} , P)$, suivant toutes la loi de Bernouilli de paramètre $\frac{1}{2}$. \\

     On définit la suite de variables aléatoires $(Z_n)_{ n \in \N }$ par les relations : 
     \[
     Z_0 = \frac{ X_0 }{ 2 } \ \text{ et } \ \forall n \in \N^* \ , \ Z_n = \frac{ Z_{n-1} + X_n }{ 2 } . 
     \]

   \item \begin{enumerate}

     \item On itère la relation de récurrence (on ne peut procéder par récurrence puisque le résultat n'est pas donné) : 
       \[
       Z_n = \frac{ Z_{n-1} }{ 2 } + \frac{ X_n }{ 2 } = \frac{ \frac{ Z_{n-2} }{ 2 } + \frac{ X_{n-1} }{ 2 } }{ 2 } + \frac{ X_n }{ 2 } = \frac{ Z_{n-2} }{ 4 } + \frac{ X_{n-1} }{ 4 } + \frac{ X_n }{ 2 } . 
       \]

       A l'ordre suivant on obtient alors : 
       \[
       Z_n = \frac{ Z_{n-3} }{ 8 } + \frac{ X_{n-2} }{ 8 } + \frac{ X_{n-1} }{ 4 } + \frac{ X_n }{ 2 } = \frac{ Z_{n-3} }{ 2^3 } + \frac{ X_{n-2} }{ 2^3 } + \frac{ X_{n-1} }{ 2^2 } + \frac{ X_n }{ 2 } . 
       \]

       En itérant $k$ fois on obtiendra alors :
       \[
       Z_n = \frac{ Z_{n-k} }{ 2^k } + \frac{ X_{n- (k-1)} }{ 2^k } + \frac{ X_{n- (k-2) } }{ 2^{k-1} } + \dots + \frac{ X_n }{ 2 } . 
       \]

       Enfin lorsqu'on arrive à $k=n$ on obtiendra : 
       \[
       Z_n = \frac{ Z_0 }{ 2^n } + \frac{ X_1 }{ 2^n } + \frac{ X_2 }{ 2^{n-1 } } + \dots + \frac{ X_n }{ 2 } = \frac{ X_0 }{ 2^{n+1} } + \frac{ X_1 }{ 2^n } + \dots + \frac{ X_{n-1} }{ 2^2 } + \frac{ X_n }{ 2 } . 
       \]

       Enfin cette somme se réécrit avec le symbole $\sum$ : en posant $i$ l'indice sur les variables $X$ on remarque que : 
       \[
       Z_n = \Sum{ i = 0 }{n} \frac{ X_i }{ 2^{ n+1 - i } } = \Sum{ i = 0 }{n} \left( \frac{ 1 }{ 2 } \right)^{ n+1 - i } X_i . 
       \]

     \item On en déduit que
       \[
       Z_{n-1} = \Sum{i=0}{n-1} \left( \frac{ 1 }{ 2 } \right)^{ n - i } X_i 
       \]

       ne dépend que des variables $X_0 , \dots , X_{n-1}$. De plus les $(X_i)$ sont indépendantes, donc par lemme des coalitions, $Z_{n-1}$ et $X_n$ sont indépendantes. \\

     \item Par linéarité de l'espérance d'une part, indépendance des $X_i$ et quadracité de la variance d'autre part, $Z_n$ admet une espérance et une variance et : 
       \begin{eqnarray*}
         E ( Z_n ) & = & \Sum{i=0}{n} \left( \frac{ 1 }{ 2 } \right)^{ n+1 - i } E ( X_i ) = \Sum{i=0}{n} \left( \frac{ 1 }{ 2 } \right)^{ n+1 - i } \times \frac{1}{2} = \Sum{i=0}{n} \left( \frac{ 1 }{ 2 } \right)^{ n+2 - i } \\ \\
         & = & \left( \frac{ 1 }{ 2 } \right)^{n+2} \Sum{i=0}{n} 2^i = \left( \frac{ 1 }{ 2 } \right)^{ n + 2 } \times \frac{ 1 - 2^{n+1} }{ 1 - 2 } = \left( \frac{ 1 }{ 2 } \right)^{ n + 2 } (2^{n+1} - 1 ) \\ \\
         & = & \frac{ 1 }{ 2 } - \left( \frac{ 1 }{ 2 } \right)^{ n + 2 } = \frac{ 1 }{ 2 } \left( 1 - \left( \frac{ 1 }{ 2 } \right)^{n+1} \right) . 
       \end{eqnarray*}

       et
       \begin{eqnarray*}
         V ( Z_n ) & = & \Sum{i=0}{n} V \left[ \left( \frac{ 1 }{ 2 } \right)^{ n+1 - i }  X_i \right] = \Sum{i=0}{n} \left( \frac{ 1 }{ 2 } \right)^{ 2n+2 -2 i } V ( X_i ) = \Sum{i=0}{n} \left( \frac{ 1 }{ 2 } \right)^{ 2n+2 -2 i } \times \frac{1}{4} = \Sum{i=0}{n} \left( \frac{ 1 }{ 2 } \right)^{ 2n+4 -2 i } \\ \\
         & = & \left( \frac{ 1 }{ 2 } \right)^{2n+4} \Sum{i=0}{n} 4^i = \left( \frac{ 1 }{ 2 } \right)^{ 2n + 4 } \times \frac{ 1 - 4^{n+1} }{ 1 - 4 } = \left( \frac{ 1 }{ 2 } \right)^{ 2n + 4 } \times \frac{ 2^{2n+2} - 1 }{ 3 } \\ \\
         & = & \frac{ \frac{ 1 }{ 4 } - \left( \frac{ 1 }{ 2 } \right)^{ 2n + 4 } }{ 3 } = \frac{ 1 }{ 12 } \left( 1 - \left( \frac{ 1 }{ 2 } \right)^{2n+2} \right)
       \end{eqnarray*}

     \end{enumerate}

   \item On va montrer ce résultat par récurrence sur $n$, puisque c'est ainsi qu'est définie la suite $(Z_n)$ : \begin{itemize}

     \item \textbf{Initialisation} : $2^{0+1} Z_0 = 2 E_0 = X_0$ suit la loi de Bernouilli de paramètre $\frac{ 1 }{ 2 }$, qui est bien la loi uniforme sur $\llb 0 ; 1 \rrb$, avec $2^{0+1} - 1 = 2^1 - 1 = 2 - 1 = 1 $. \\

     \item \textbf{Hérédité} : on suppose qu'il existe $n \in \N$ fixé tel que $ 2^{n+1}  Z_n$ suit la loi uniforme discrète sur $\llb 0 ; 2^{n+1} - 1 \rrb$, alors : 
       \[
       2^{n+2} Z_{n+1} = 2^{n+2} \times \frac{ Z_n + X_{n+1} }{ 2 } = 2^{ n+1 } ( Z_n + X_{n+1} ) = 2^{n+1} Z_n + 2^{n+1} X_{n+1} . 
       \]

       La valeur minimale possible est obtenue avec la valeur minimale de $2^{n+1} Z_n$, qui vaut 0, et de $X_{n+1}$, qui vaut 0 : c'est donc 0. \\

       La valeur maximale possible est obtenue avec la valeur maximale de $2^{n+1} Z_n$, qui vaut $2^{n+1}-1$, et de $X_{n+1}$, qui vaut 1 : c'est donc :
       \[
       2^{n+1} - 1 + 2^{n+1} = 2 \times 2^{n+1} - 1 = 2^{n+2} - 1 . 
       \]

       Vérifions que toutes les valeurs entières sont obtenues (il est immédiat que toutes les valeurs sont bien entières comme somme de deux entiers) : \begin{itemize}

       \item Avec $X_{n+1} = 0$, $Z_{n+1}$ peut prendre toutes les valeurs de $( 2^{n+1} Z_n ) ( \Omega)$, donc tous les entiers de 0 à $2^{n+1} - 1$. \\

       \item Avec $X_{n+1} = 1$, $Z_{n+1}$ donne un entier entre 0 et $2^{n+1} - 1$, auquel on ajoute $2^{n+1}$ : cela donne tous les entiers de 
         \[
         0 + 2^{n+1} = 2^{n+1} \ \ \text { à } \ \ 2^{n+1} -1 + 2^{n+1} = 2^{n+2} - 1 . 
         \]

       \end{itemize}

       On obtient bien que $( 2^{n+2} Z_{n+1} ) (\Omega) = \llb 0 ; 2^{n+2 - 1 } \rrb$. Montrons à présent que toutes les probabilités de la loi sont égales : on sait que toutes celles de la loi de $2^{n+1} Z_n$ le sont donc il existe $a \in ]0 ;1[$ tel que : 
       \[
       \forall k \in \llb 0 ; 2^{n+1} - 1 \rrb , \ P ( 2^{n+1} Z_n = k ) = a . 
       \]

       On a vu dans la recherche du support que : 
       \[
       \forall k \in \llb 0 ; 2^{n+1} - 1 \rrb , \ ( 2^{n+2} Z_{n+1} = k ) = ( 2^{n+1} Z_n = k ) \cap (X_{n+1} = 0 ) 
       \]

       et par indépendance (question 2b), 
       \[
       P ( 2^{n+2} Z_{n+1} = k ) = \frac{ 1 }{ 2 } a . 
       \]

       De même on a vu que (et par indépendance pour la probabilité) :
       \[
       \forall k \in \llb 2^{n+1} ; 2^{n+2} - 1 \rrb , \ ( 2^{n+2} Z_{n+1} = k ) = ( 2^{n+1} Z_n = k - 2^{n+1} ) \cap (X_{n+1} = 1 ) \ \ \ \text{ donc } \ \ \ P ( 2^{n+2} Z_{n+1} = k ) = \frac{ 1 }{ 2 } a . 
       \]

       On en déduit bien que toutes les probabilités de la loi sont égales, c'est une loi uniforme et $2^{n+2} Z_{n+1}$ suit bien la loi uniforme discrète sur $\llb 0 ; 2^{n+2} - 1 \rrb$, la propriété est encore vraie au rang $n+1$. \\

     \item \textbf{Conclusion} : pour tout $n \in \N$, $ 2^{n+1}  Z_n$ suit la loi uniforme discrète sur $\llb 0 ; 2^{n+1} - 1 \rrb$. \\

     \end{itemize}

   \item Comme la variable aléatoire d'arrivée est à densité, sa fonction de répartition est continue sur $\R$, et il faut donc prouver la convergence de $F_{ Z_n } (x)$ vers $F_Z (x)$ pour tout $x \in \R$. On commence donc par chercher la fonction de répartition de $Z_n$. \\

     D'après la question précédente, on remarque que $Z_n$ suit la loi uniforme discrète sur l'ensemble : 
     \[
     Z_n ( \Omega ) = \left\{ \frac{ k }{ 2^{n+1} } \text{ tq } k \in \llb 0 ; 2^{n+1} - 1 \rrb \right\} = \left\{ 0 ; \frac{ 1 }{ 2^{n+1} } ; \dots ; \frac{ 2^{n+1} - 1 }{ 2^{ n+1 } } \right\} = \left\{ 0 ; \frac{ 1 }{ 2^{n+1} } ; \dots ;  1 - \frac{ 1 }{ 2^{ n+1 } } \right\} 
     \]

     On en déduit que pour tout $x < 0$ (avant le support) on a : 
     \[
     F_{ Z_n } (x) = 0 \xrightarrow[ n \rightarrow + \infty ]{} 0 . 
     \]

     De même pour tout $x \geq 1$ (après le support pour tout $n$, puisque $ 1 - \frac{ 1 }{ 2^{n+1} }$ tend en croissant vers 1 : 
     \[
     F_{ Z_n } (x) = 1 \xrightarrow[ n \rightarrow + \infty ]{} 1 . 
     \]

     Considérons à présent un $x \in [ 0 ; 1 [$, on a : 
     \[
     F_{ Z_n } (x) = \Sum{ k = 0 }{ j (x) } \Pr \Ev{ Z_n = k } 
     \]

     avec $j(x)$ le plus grand entier vérifiant : 
     \[
     \frac{ j }{ 2^{n+1} } \leq x . 
     \]

     On en déduit d'une part que : 
     \[
     F_{ Z_n } (x) = \Sum{ k = 0 }{ j (x) } \frac{ 1 }{ \Card ( \llb 0 ; 2^{n+1} - 1 \rrb ) } = \Sum{ k = 0 }{ j (x) } \frac{ 1 }{ 2^{n+1} } = \frac{ j(x) + 1 }{ 2^{ n + 1 } } 
     \]

     et d'autre part pour exprimer $j(x)$, que 
     \[
     \frac{ j(x) }{ 2^{ n + 1 } } \leq x < \frac{ j(x) + 1 }{ 2^{n+1} } \Longleftrightarrow j(x) \leq 2^{n+1} x < j(x) + 1 \Longleftrightarrow j(x) = \lfloor 2^{ n+1 } x \rfloor . 
     \]

     Finalement on en déduit que : 
     \[
     F_{ Z_n } (x) = \frac{ \lfloor 2^{ n+1 } x \rfloor + 1 }{ 2^{ n+1 } } . 
     \]

     Pour déterminer la limite de cette quantité, il faut encadrer la partie entière et utiliser le théorème d'encadrement. On sait que : 
     \[
     \lfloor 2^{ n+1 } x \rfloor \leq 2^{n+1} x < \lfloor 2^{ n+1 } x \rfloor + 1 \ \ \text{ donc } \ \ 2^{ n+1 } x - 1 < \lfloor 2^{ n+1 } x \rfloor \leq 2^{n+1} x . 
     \]

     On en déduit (on rajoute un et on divise par un nombre strictement positif, opérations strictement croissantes) que : 
     \[
     \frac{ 2^{n+1} x }{ 2^{n+1} } < F_{ Z_n } (x) \leq \frac{ 2^{n+1} x + 1 }{ 2^{ n+1 } } \Longleftrightarrow  x  < F_{ Z_n } (x) \leq x +  \frac{ 1 }{ 2^{ n+1 } } 
     \]

     et comme les termes de gauche et droite ont pour limite immédiate $x$ lorsque $n$ tend vers $+\infty$, on obtient finalement : 
     \[
     \dlim{ n \rightarrow + \infty } F_{ Z_n } (x) = x . 
     \]

     Enfin en rassemblant tous les cas, on a pour $x \in \R$ : 
     \[
     F_{ Z_n } (x) \xrightarrow[ n \rightarrow + \infty ]{} \left\{ \begin{array}{cl} 0 & \text{ si } x < 0 \\ x & \text{ si } 0 \leq x < 1 \\ 1 & \text{ si } x \geq 1 \\ \end{array} \right. 
     \]

     On reconnaît exactement la fonction de répartition de la loi uniforme sur $[0 ; 1]$ (à densité), donc si $Z$ est une variable aléatoire qui suit cette loi, $(Z_n)$ converge en loi vers $Z$.

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\

   \begin{enumerate}

   \item Cette intégrale est généralisée en 0 (car $\ln$ n'y est pas défini) et en 1 (le dénominateur vaut alors 0). \\ \begin{itemize}

     \item Au voisinage de 0, par croissances comparées, on a 
       \[
       \dlim{ x \rightarrow 0 } \frac{ x^n \ln x }{ x^n - 1 } = \frac{ 0 }{ -1 } = 0 
       \]

       donc l'intégrale est faussement généralisée (la fonction intégrée est prolongeable par continuité en 0), elle converge donc. \\

     \item Au voisinage de 1, la limite donne une forme indéterminée. On cherche un équivalent de chaque facteur au point 1. \begin{itemize}

       \item Par produit immédiat, $x^n$ converge vers 1 donc 
         \[
         x^n \underset{ x \rightarrow 1 }{ \sim } 1 . 
         \]

       \item Pour le $\ln$, on pose $ x = 1 + y \Longleftrightarrow y = x - 1$, avec $y$ qui tend vers 0, on a par DL : 
         \[
         \ln (1+y ) = y + o (y) \underset{ y \rightarrow 0 }{ \sim } y 
         \]

         donc
         \[
         \ln x \underset{ x \rightarrow 1 }{ \sim } x - 1 . 
         \]

       \item Enfin avec le même changement de variable et par DL : 
         \[
         (1+y)^n - 1 = 1 + n y + o(y) - 1 = n y + o(y) \underset{ y \rightarrow 0 }{ \sim } n y 
         \]

         donc
         \[
         x^n - 1 \underset{ x \rightarrow 1 }{ \sim } n (x-1 ) . 
         \]

       \end{itemize}

       On obtient finalement : 
       \[
       \frac{ x^n \ln x }{ x^n - 1 } \underset{ x \rightarrow 1 }{ \sim } \frac{ 1 \times (x-1 ) }{ n (x-1 ) } = \frac{ 1 }{ n } \xrightarrow[ x \rightarrow 1 ]{} \frac{1}{n} . 
       \]

       De nouveau la fonction est prolongeable par continuité, et l'intégrale est faussement généralisée donc converge au voisinage de 1. \\

     \end{itemize}

     Finalement l'intégrale $\int_0^1 \frac{ x^n \ln x }{ x^n - 1 } $ converge, elle existe bien. \\

   \item La question appelle clairement le théorème de convergence monotone, puisque la limite n'est pas demandée. On étudie la monotonie de $(u_n)$, en cherchant le signe de : 
     \begin{eqnarray*}
       u_{n+1} - u_n & = & \int_0^1 \ln x \left( \frac{ x^{n+1} }{ x^{n+1} - 1 } - \frac{ x^n }{ x^n - 1 } \right) \ dx = \int_0^1 \ln x \times \frac{ x^{n+1} (x^n - 1 ) - x^n (x^{n+1} - 1 ) }{ (x^{n+1} - 1 ) ( x^n - 1 ) } \ dx \\ \\
       & = & \int_0^1 \frac{ \ln x }{ (x^{n+1} - 1 ) ( x^n - 1 ) } \times x^n \times \left( \rule{0cm}{0.4cm} x ( x^n - 1 ) - x^{n+1} + 1 \right) \ dx \\ \\
       & = & \int_0^1 \frac{x^n \ln x }{ (x^{n+1} - 1 ) ( x^n - 1 ) }  \left( \rule{0cm}{0.4cm} x^{ n+1 } - x - x^{n+1} + 1 \right) \ dx = \int_0^1 \frac{ x^n (1-x) \ln x }{ (x^{n+1} - 1 ) ( x^n - 1 ) }   dx
     \end{eqnarray*}

     Or pour tout $x \in ] 0 ; 1[$, on a : 
     \[
     x^n \geq 0 \ \ , \ \ 1-x \geq 0 \ \ , \ \ \ln x \leq 0 \ \ , \ \ x^n \leq 1 \text{ donc } x^n - 1 \leq 0 \ \ , \ \ x^{n+1} \leq 1 \text{ donc } x^{n+1} - 1 \leq 0 
     \]

     et par produit et quotient de deux facteurs positifs et trois négatifs, la fonction intégrée est négative sur $]0;1[$. Comme les bornes sont dans l'ordre croissant, on en déduit que pour tout $n \in \N$ : 
     \[
     u_{n+1} - u_n \leq 0 
     \]

     et la suite $(u_n)$ est décroissante. \\

     Enfin la suite est positive car la fonction intégrée est positive sur $]0 ; 1[$ avec un facteur positif et deux négatifs et les bornes sont dans l'ordre croissant, donc elle décroît et elle est minorée par 0 : c'est une suite convergente.

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Soit $f$ une fonction de classe $C^{p+1}$ sur $[0;1]$, alors pour tous $a$ et $b$ de $[0;1]$, on a :
     \begin{eqnarray*}
       f(b) & = & f(a) + f'(a) (b-a) + \dots + f^{(p)} (a) \frac{ (b-a)^p }{ p! } + \int_a^b \frac{ (b-t)^p }{ p ! } f^{ (p+1) } (t) \ dt \\ \\
       & = & \Sum{k=0}{p} f^{ (k) } (a) \frac{ (b-a)^k }{ k! } + \int_a^b \frac{ (b-t)^p }{ p ! } f^{ (p+1) } (t) \ dt 
     \end{eqnarray*}

   \item Soit $x$ un réel de l'intervalle $[0;1[$. \begin{enumerate}

     \item Comme $t \in [0;x]$ et $x \in [0;1[$ on a $0 \leq t \leq x < 1$, et donc : 
       \[
       x - t \geq 0 \ \ \text{ et } \ \ 1-t > 0 \ \ \text{ donc par quotient } \ \ \frac{ x-t }{ 1-t } \geq 0 . 
       \]

       Pour l'autre côté, on considère la différence : 
       \[
       \frac{ x-t }{ 1-t } - x = \frac{ x - t - x (1-t ) }{ 1 - t } = \frac{ x - t - x + x t }{ 1 - t } = \frac{ t (x-1 ) }{ 1 - t } 
       \]

       et comme $t \geq 0$, $x-1 \leq 0$ et $1-t > 0$, cette quantité est négative donc on a bien : 
       \[
       \forall t \in [0 ; x ] , \ 0 \leq \frac{ x - t }{ 1 - t } \leq x . 
       \]

     \item On reconnaît à droite une intégration de la série géométrique. On part de la somme géométrique finie : pour tout $p \geq 1$, on a : 
       \[
       \Sum{n=1}{p} x^{n-1} = \Sum{n=0}{p-1} x^n = \frac{ 1 - x^p }{ 1 - x } . 
       \]

       On intègre cette inégalité entre 0 et $x$ (en la réécrivant pour tout $t$ au lieu de $x$) : 
       \[
       \Sum{n=1}{p} \int_0^x t^{n-1} \ dt = \int_0^x \frac{ 1 - t^p }{ 1 - t } \ dt = \int_0^x \frac{ 1 }{ 1-t } \ dt - \int_0^x \frac{ t^p }{ 1-t } \ dt . 
       \]

       On calcule les intégrales "faciles" : 
       \[
       \Sum{n=1}{p} \int_0^x t^{n-1} \ dt = \Sum{n=1}{p} \frac{ x^n }{ n } 
       \]

       et
       \[
       \int_0^x \frac{ 1 }{ 1-t } \ dt = \left[ - \ln (1 - t) \right]_0^x = - \ln (1-x ) 
       \]

       donc :
       \[
       \ln (1-x ) = \int_0^x \frac{ t^p }{ 1-t } \ dt - \Sum{n=1}{p} \frac{ x^n }{ n } . 
       \]

       Enfin l'intégrale restante ne peut pas se calculer, mais on va montrer que sa limite lorsque $p$ tend vers $+\infty$ est nulle : pour cela on encadre l'intégrale en écrivant : 
       \[
       0 \leq t \leq x \ \ \text{ donc } \ \ - x \leq -t \leq 0 \ \ \text{ donc } \ \ 1-x \leq 1-t \leq 1 
       \]

       et comme tous les termes sont strictement positifs (avec $x < 1$), par stricte décroissance de l'inverse sur $\R_+^*$ : 
       \[
       1 \leq \frac{ 1 }{ 1-t } \leq \frac{ 1 }{ 1-x } \ \ \text{ et enfin } \ \ t^p \leq \frac{ t^p }{ 1-t } \leq \frac{ t^p }{ 1 - x } 
       \]

       avec $t^p \geq 0$. On intègre avec les bornes dans l'ordre croissant : 
       \[
       \int_0^x t^p \ dt \leq \int_0^x \frac{ t^p }{ 1-t } \ dt \leq \frac{1}{1-x} \int_0^x t^p \ dt 
       \]

       et en calculant les termes de gauche et droite : 
       \[
       \frac{ x^{p+1} }{ p+1 } \leq \int_0^x \frac{ t^p }{ 1-t } \ dt \leq \frac{ x^{p+1} }{ (1-x) (p+1) } . 
       \]

       Enfin comme $ \vert x \vert < 1$, par quotient de limites, les deux termes extrémaux tendent vers 0 lorsque $p$ tend vers $+\infty$, et par encadrement le terme central aussi. \\

       Enfin on peut passer à la limite l'égalité obtenue précédemment sur $\ln (1-x)$, avec tous les termes qui convergent : 
       \[
       \ln (1-x ) = - \Sum{n=1}{+\infty} \frac{ x^n }{ n } . 
       \]

     \end{enumerate}

   \item Soit $X$ une variable aléatoire discrète définie sur un espace probabilisé $(\Omega , \mathcal{A} , P)$ telle que pour tout $n \in \N^*$, on a : $\Prob(\Ev{X=n}) = \frac{ 1 }{ n ( n+1) }$.

     \begin{enumerate}

     \item Difficile. Comme ce n'est pas une série usuelle, il faut penser à faire apparaître un télescopage en remarquant que : 
       \[
       \frac{ 1 }{ n (n+1) } = \frac{ (n+1) - n }{ n (n+1) } = \frac{ 1 }{ n } - \frac{ 1 }{ n+1 } . 
       \]

       On peut alors écrire : 
       \[
       P ( X \in \N^* ) = \Sum{n=1}{+\infty} P \Ev{ X = n } = \dlim{ p \rightarrow + \infty} \left( \Sum{n=1}{p} \frac{ 1 }{ n } - \frac{ 1 }{ n+1 } \right) = \dlim{ p \rightarrow + \infty} \left( 1 - \frac{ 1 }{ p+1} \right) = 1 . 
       \]

     \item Soit $r \geq 1$, on étudie l'existence du moment d'ordre $r$ de $X$, en considérant la série de terme général : 
       \[
       n^r \Pr \Ev{ X = n } = \frac{ n^r }{ n (n+1) } = \frac{ n^{ r-1 } }{ n \left( 1 + \frac{1}{n} \right) } \underset{ + \infty }{ \sim } n^{ r- 2 } = \frac{ 1 }{ n^{ 2 - r } } . 
       \]

       Les deux termes généraux sont positifs donc par théorème de comparaison $X$ admet un moment d'ordre $r$ si et seulement si la série de terme général $\frac{ 1 }{ n^{ 2-r } }$ est convergente. Cette série de Riemann converge si et seulement si : 
       \[
       2 - r > 1 \Longleftrightarrow r < 1 \Longleftrightarrow r \leq 0 
       \]

       puisque $r$ est un entier : la variable $X$ n'admet donc aucun moment (à part celui d'ordre 0, qui correspond à la somme des probabilités et qui doit obligatoirement exister et valoir 1 pour $x$ soit une variable aléatoire). \\

     \item Par théorème de transfert, on s'intéresse à la série (sous réserve de convergence absolue) : 
       \begin{eqnarray*}
         E ( s^X ) & = & \Sum{n=1}{+\infty} s^n P \Ev{ X = n } = \Sum{ n=1 }{ +\infty } \frac{ s^n }{ n (n+1) } . 
       \end{eqnarray*}

       La convergence absolue de cette série s'obtient immédiatement, avec $s^n \leq 1$ donc $\frac{ s^n }{ n (n+1) } \leq \frac{ 1 }{ n (n+1) }$ qui est le terme général d'une série convergente (3a), et avec les deux termes généraux positifs le théorème de comparaison donne la convergence puis la convergence absolue (puisque le terme général est positif) de la série considérée. \\

       Pour la calculer (seulement valable pour $s \in ]0 ; 1[$), on pense à utiliser la question 2, et pour ne plus avoir que $n$ au dénominateur on pense à la transformation de la question 3a : 
       \begin{eqnarray*}
         E ( s^X ) & = &  \Sum{ n=1 }{ +\infty } \frac{ s^n }{ n } - \Sum{ n=1 }{ + \infty } \frac{ s^n }{ n+1 } = - \ln (1-s) - \Sum{ n=2 }{ + \infty } \frac{ s^{n-1} }{ n } \\ \\
         & = & - \ln (1-s) - \frac{1}{s} \left( \Sum{ n=1 }{ + \infty } \frac{ s^n }{ n } - \frac{ s }{ 1 } \right) = - \ln (1-s) - \frac{1}{s} \left( - \ln (1-s) - s \right) \\ \\
         & = & - \ln (1-s) + \frac{ \ln (1-s) }{ s} + 1 = \frac{ - s \ln (1-s) + \ln (1-s) + s }{ s } = \frac{ s + (1-s) \ln (1-s) }{ s } . 
       \end{eqnarray*}

     \item On calcule les valeurs en $s=0$ et $s=1$ : 
       \[
       E ( 0^X ) = E (0 ) = 0 \ \ \text{ et } \ \ E ( 1^X ) = E (1) = 1 
       \]

       donc : 
       \[
       \phi (s) = \left\{ \begin{array}{cl} 0 & \text{ si } s = 0 \\ \frac{ s + (1-s) \ln (1-s) }{ s } & \text{ si } 0 < s < 1 \\ 1 & \text{ si } s = 1 \\ \end{array} \right. 
       \]

       Cette fonction est continue (et dérivable) sur $]0;1[$ comme somme, produit, quotient, composée de fonctions dérivables, avec $s \neq 0$ et $1-s > 0$. \\

       Etudions la continuité en 0 et 1 : \\ \begin{itemize}

       \item Au voisinage de 0, par DL de $\ln (1-s)$, on a : 
         \[
         \phi (s) = 1 + \frac{ (1-s) \left( -s - \frac{ s^2 }{ 2 } + o ( s^2 ) \right) }{ s } = 1 + (1-s ) \left(- 1 - \frac{ s }{ 2 } + o(s) \right) \xrightarrow[ s \rightarrow 0]{} 1 + 1 \times (-1) = 0 = \phi (0) 
         \]

         et $\phi$ est continue en 0. \\

       \item Au voisinage de 1, par croissances comparées (avec $y=1-s$ qui tend vers 0), on obtient : 
         \[
         \dlim{ s \rightarrow 1 } \phi (s) = \frac{ 1 + 0 }{ 1 } = 1 = \phi (1) 
         \]

         et $\phi$ est continue en 1.

       \end{itemize}

       On en déduit que $\phi$ est continue sur $[0;1]$, on étudie à présent sa dérivabilité en 0 et 1 : \begin{itemize}

       \item Au voisinage de 0, on a : 
         \begin{eqnarray*}
           \frac{ \phi (s) - \phi (0) }{ s - 0 } & = & \frac{ \phi (s) }{ s } = \frac{ s + (1-s) \ln (1-s) }{ s^2 } = \frac{ s + (1-s) \left( - s - \frac{ s^2 }{ 2 } + o(s^2 ) \right) }{ s^2 } \\ \\
           & = & \frac{ 1 + (1-s) \left( - 1 - \frac{ s }{ 2 } + o(s ) \right) }{ s } = \frac{ 1 - 1 - \frac{ s }{ 2 } + s + o(s) }{ s } = \frac{ 1 }{ 2 } + o(1 ) \\ \\
           & \xrightarrow[ s \rightarrow 0]{} \frac{ 1 }{ 2 } 
         \end{eqnarray*}

         donc $\phi$ est dérivable en 0. \\

       \item Au voisinage de 1, on a :
         \begin{eqnarray*}
           \frac{ \phi (s) - \phi (1) }{ s - 1 } & = & \frac{ \phi (s) - 1 }{ s - 1 } = \frac{ \frac{ s + (1-s) \ln (1-s) }{ s } - \frac{ s }{ s } }{ s - 1 } = \frac{ s + (1-s) \ln (1-s) - s }{ s (s-1) } \\ \\
           & = & \frac{ (1-s) \ln (1-s) }{ s (s-1) } = - \frac{ \ln (1-s) }{ s } \xrightarrow[ s \rightarrow 1 ]{} - \frac{ - \infty }{ 1 } = + \infty 
         \end{eqnarray*}

         donc $\phi$ n'est pas dérivable en 1.

       \end{itemize}

       On en déduit que $\phi$ est continue sur $[0;1]$, dérivable sur $[0;1[$, mais pas dérivable en 1. \\

     \item Par théorème de transfert, on s'intéresse à la série :
       \[
       \Sum{n=1}{+\infty} n s^n P \Ev{ X=n } = \Sum{n=1}{+\infty} \frac{ s^n }{ n + 1 } . 
       \]

       Cette série est à termes positifs; pour $0 \leq s < 1$, le terme général est négligeable devant $s^n$ qui est le terme général d'une série convergente, donc l'espérance existe bien. \\

       Pour $s=1$, on reconnaît la série harmonique, la série diverge et l'espérance n'existe pas. \\

       Pour $s > 1$, le terme général diverge par croissances comparées, la série diverge donc grossièrement et l'espérance n'existe pas. Enfin, pour $0 \leq s < 1$ : 
       \[
       E ( X s^X ) = \Sum{n=1}{+\infty} \frac{ s^n }{ n + 1 } = \Sum{n=2}{+\infty} \frac{ s^{n-1} }{ n } = \frac{1}{s} \Sum{n=2}{+\infty} \frac{ s^n }{ n } = \frac{ - \ln (1-s) - s }{ s } . 
       \]

       Pour le moment d'ordre deux, toujours par transfert, on s'intéresse à : 
       \[
       \Sum{n=1}{+\infty} n^2 s^{2n} P \Ev{ X=n } = \Sum{n=1}{+\infty} \frac{ n s^{2n} }{ n + 1 } . 
       \]

       Cette série est à termes positifs, et le terme général est équivalent en $+\infty$ à $s^{2n} = (s^2)^n$, donc la série converge absolument si et seulement si : 
       \[
       s^2 < 1 \Longleftrightarrow 0\leq s < 1 
       \]

       Enfin pour ces valeurs de $s$, on a : 
       \begin{eqnarray*}
         E ( X^2 s^{ 2X } ) & = & \Sum{n=1}{+\infty} \frac{ (n + 1 - 1 ) (s^2)^n }{ n + 1 } = \Sum{n=1}{+\infty} (s^2)^n  - \Sum{n=1}{+\infty} \frac{ (s^2)^n }{ n + 1 } \\ \\
         & = & \frac{ 1 }{ 1 - s^2 } - 1 -  E ( X s^X ) = \frac{ s^2 }{ 1 - s^2 } + \frac{ \ln (1-s) + s }{ s } . 
       \end{eqnarray*}

       Enfin $X s^X$ admet une variance si et seulement si $0 \leq s < 1$, et : 
       \[
       V ( X s^X ) =  \frac{ s^2 }{ 1 - s^2 } + \frac{ \ln (1-s) + s }{ s } - \left( \frac{ \ln (1-s) + s }{ s } \right)^2 . 
       \]

     \end{enumerate}

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\

   \begin{enumerate}

   \item On étudie les variations de $f$ : $f$ est dérivable et pour tout $x \in \R$,
     \[
     f'(x) = 3 x^2 + 2 x + 1 
     \]

     et ce polynôme du second degré a pour discriminant $\Delta = 4 - 12 = -8 < 0$, donc avec $3 > 0$, $f'(x)$ est strictement positive sur $\R$. \\

     On en déduit que $f$ est strictement croissante sur $\R$, avec pour limites $+\infty$ et $-\infty$ en $+\infty$ et $- \infty$ (terme prépondérant $x^3$ à chaque fois), donc par théorème de bijection, $f$ est bijective de $\R$ dans $\R$. \\

   \item Pour qu'un polynôme soit surjectif, il faut qu'il prenne toutes les valeurs de $-\infty$ à $+\infty$. Pour cela, comme il est continue, il faut et il suffit qu'il ait une limite égale à $-\infty$ et une égale à $+\infty$, et comme les seules limites sont en $+\infty$ et $-\infty$, l'une doit être égale à $+\infty$, l'autre à $-\infty$. \\

     Or un polynôme est équivalent en $\pm \infty$, à son terme de plus haut degré, qui sera de la forme $a_n x^n$, avec $a_n \neq 0$ (où le polynôme est de degré $n$). La limite en $+\infty$ sera décidée par le signe de $a_n$, et celle en $-\infty$ par le signe de $a_n$ \text{ et la parité de n}. \\

     Si $n$ est pair, les deux limites sont les mêmes (car $x^n$ a la même limite en $+\infty$et $-\infty$), si $n$ est impaire elle seront opposées, et dans tous les cas elles seront infinies. \\

     On en déduit qu'un polynôme est surjectif de $\R$ dans $\R$ si et seulement si il est de degré impair. \\

   \item Pour être injective, une fonction continue à valeurs réelles doit être strictement monotone, donc sa dérivée (tout polynôme est dérivable) ne doit jamais s'annuler. \\

     on en déduit que la dérivée $f'$ de $f$ n'est pas surjective, donc par 2. $f'$ est de degré pair, et $f$ est de degré impair. \\

     Je ne vois comment aller plus loin sans la décomposition en facteurs premiers des polynômes réels, qui n'est pas au programme, et qui permettrait de prouver que $f'$ doit s'écrire : 
     \[
     f'(x) = \prod\limits_{k=1}^p P_k(x) 
     \]

     avec, pour tout $k$, $P_k$ un polynôme du second degré dont le discriminant est strictement négatif.

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Soit $(A_i)_{ i \in I }$ un système complet d'évènements, avec $I$ un ensemble discret (fini ou indexé par $\N$) et $B$ un évènement, alors : 
     \[
     B = \bigcup\limits_{ i \in I } ( A_i \cap B ) 
     \]

     avec l'union incompatible car les $(A_i)$ le sont, et les $A_i$ de probabilité non nulles donc par probabilités composées on obtient : 
     \[
     P ( B ) = \sum\limits_{i \in I} P ( A_i \cap B ) = \sum\limits_{ i \in I } P (A_i ) P_{ A_i } ( B ) . 
     \]

     Soit $p$ et $q$ deux réels vérifiant $0<p<1$ et $p+2q=1$. On note $\Delta$ la matrice de $\mathcal{M}_3 ( \R )$ définie par :
     \[
     \Delta = \begin{smatrix} p & q & q \\ q & p & q \\ q & q & p \\ \end{smatrix} 
     \]

   \item $\Delta$ est symétrique donc diagonalisable. \\

   \item Il faut donc déterminer les valeurs propres de $\Delta$. On remarque que
     \[
     \Delta = p I + q A 
     \]

     avec $A = \begin{smatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \\ \end{smatrix} $ qui est symétrique donc diagonalisable, donc en diagonalisant $A$ on obtiendra : 
     \[
     \Delta = p I + q P D' P^{-1} = p P I P^{-1} + q P D' P^{-1} = P ( p I + q D' ) P^{-1} 
     \]

     et les valeurs propres de $\Delta$ sont donc données par : 
     \[
     \spc ( \Delta ) = \{ p + q \lambda \text{ tq } \lambda \in \spc ( A  ) \} . 
     \]

     On cherche donc les valeurs propres de $A$, c'est-à-dire les $\lambda$ réels tels que $A - \lambda I$ n'est pas inversible : 
     \begin{eqnarray*}
       A - \lambda I = \begin{smatrix} - \lambda & 1 & 1 \\ 1 & -\lambda & 1 \\ 1 & 1 & - \lambda \\ \end{smatrix} & \Longleftrightarrow L_1 \leftrightarrow L_3 & \begin{smatrix} 1 & 1 & - \lambda \\ 1 & -\lambda & 1 \\ - \lambda & 1 & 1 \\ \end{smatrix} \\ \\
       & \Longleftrightarrow \begin{array}{c} \\ L_2 \leftarrow L_2 - L_1 \\ L_3 \leftarrow L_3 + \lambda L_1 \\ \end{array} & \begin{smatrix} 1 & 1 & - \lambda \\ 0 & -\lambda - 1 & 1 + \lambda \\ 0 & 1 + \lambda & 1 - \lambda^2 \\ \end{smatrix} \\ \\
       & \Longleftrightarrow \begin{array}{c} \\  \\ L_3 \leftarrow L_3 +  L_2 \\ \end{array} & \begin{smatrix} 1 & 1 & - \lambda \\ 0 & -\lambda - 1 & 1 + \lambda \\ 0 & 0 & 2 + \lambda - \lambda^2 \\ \end{smatrix} 
     \end{eqnarray*}

     et les valeurs propres de $A$ sont les solution de $- \lambda - 1 = 0 \Longleftrightarrow \lambda = -1$ et de $- \lambda^2 + \lambda + 2 = 0$, qui a pour discriminant et racines : 
     \[
     \Delta = 1 + 8 = 9 \ \ , \ \ \lambda_1 = \frac{ - 1 - 3 }{ -2 } = 2 \ \ , \ \ \lambda_2 = \frac{ -1 + 3 }{ -2 } = - 1 
     \]

     On en déduit que
     \[
     D' = \begin{smatrix} -1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 2 \\ \end{smatrix} \ \ \text{ puis } \ \ D = \begin{smatrix} p - q & 0 & 0 \\ 0 & p - q & 0 \\ 0 & 0 & p + 2 q \\ \end{smatrix} = \begin{smatrix} p - q & 0 & 0 \\ 0 & p-q & 0 \\ 0 & 0 & 1 \\ \end{smatrix} . 
     \]

     On ne déduit que 
     \[
     D^n = \begin{smatrix} (p - q)^n & 0 & 0 \\ 0 & (p-q)^n & 0 \\ 0 & 0 & 1 \\ \end{smatrix} 
     \]

     et on remarque par inégalité triangulaire que
     \[
     \vert p - q \vert = \vert p + (-q) \vert \leq \vert p \vert + \vert - q \vert = p + q = p + 2q - q = 1 - q < 1 
     \]

     donc on peut conclure que : 
     \[
     D^n \xrightarrow[ n \rightarrow + \infty ]{} \begin{smatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \\ \end{smatrix} . 
     \]

     Un village possède trois restaurants $R_1$, $R_2$ et $R_3$. Un couple se rend dans un de ces trois restaurants chaque dimanche. A l'instant $n=1$ (c'est-à-dire le premier dimanche) il choisit le restaurant $R_1$, puis tous les dimanches suivants (instants $n=2$, $n=3$, etc.) il choisit le même restaurant que le dimanche précédent avec la probabilité $p$ ou change de restaurant avec la probabilité $2q$, chacun des deux autres restaurants étant choisis avec la même probabilité. \\

     On suppose que l'expérience est modélisée par un espace probabilisé $(\Omega , \mathcal{A} , P)$.

   \item On note $A_n$, $B_n$ et $C_n$ les trois évènements. La formule des probabilités totales avec le sce $(A_n , B_n , C_n)$ donne : 
     \[
     A_{n+1} = (A_n \cap A_{n+1} ) \cup ( B_n \cap A_{n+1} ) \cup ( C_n \cap (A_{n+1} ) 
     \]

     et de même pour $B_{n+1}$ et $C_{n+1}$; par incompatibilité de la réunion et probabilité composées, on obtient : 
     \[
     \left\{ \begin{array}{c} P ( A_{n+1} ) = p P (A_n ) + q P ( B_n ) + q P (C_n ) \\ P( B_{n+1} ) = q P ( A_n ) + p P ( B_n ) + q P (C_n) \\  P ( C_{n+1} ) = q P(A_n) + q P ( B_n ) + p P (C_n ) \end{array} \right. \ \ \ \text{ donc } \ \ \ \begin{smatrix} P ( A_{n+1} ) \\ P ( B_{n+1} ) \\ P ( C_{n+1} ) \\ \end{smatrix} = \Delta \begin{smatrix} P( A_n ) \\ P (B_n ) \\ P ( C_n ) \\ \end{smatrix} 
     \]

     Une récurrence immédiate donne alors : 
     \[
     \begin{smatrix} P( A_n ) \\ P (B_n ) \\ P ( C_n ) \\ \end{smatrix} = \Delta^{n-1} \begin{smatrix} P( A_1 ) \\ P (B_1 ) \\ P ( C_1 ) \\ \end{smatrix} = P D^{n-1} P^{-1} \begin{smatrix} 1 \\ 0 \\ 0 \\ \end{smatrix} 
     \]

     Pour aller plus loin il faut la matrice $P$ et $P^{-1}$ : en calculant les sous-espaces propres de $A$ puis en inversant la matrice $P$ on obtient : 
     \[
     P = \begin{smatrix} -1 & -1 & 1 \\ 1 & 0 & 1 \\ 0 & 1 & 1 \\ \end{smatrix} \ \ \ \text{ puis } \ \ \ P^{-1} = \frac{1}{3} \begin{smatrix} -1 & 2 & -1 \\ -1 & -1 & 2 \\ 1 & 1 & 1 \\ \end{smatrix} 
     \]

     qui donne finalement : 
     \[
     \begin{smatrix} P( A_n ) \\ P (B_n ) \\ P ( C_n ) \\ \end{smatrix} = \frac{1}{3} P D^{n-1} \begin{smatrix} -1 \\ -1 \\ 1 \\ \end{smatrix} = \frac{1}{3} P \begin{smatrix} - (p-q)^{n-1} \\ - (p-q)^{n-1} \\ 1 \\ \end{smatrix} = \frac{1}{3} \begin{smatrix} 1 + 2 (p-q)^{n-1} \\ 1 - (p-q)^{n-1} \\ 1 - (p-q)^{n-1} \\ \end{smatrix} 
     \]

   \item Soit $T$ la variable aléatoire égale au rang du premier dimanche où le couple retourne au restaurant $R_1$, s'il y retourne, et 0 sinon. \begin{enumerate}

     \item Le premier retour au restaurant se fait au minimum lors du 2e jour, et la valeur 0 est rajoutée artificiellement donc : 
       \[
       T ( \Omega ) = \llb 2 ; +\infty \llb \cup \{ 0 \} . 
       \]

       Pour $k=2$, on a :
       \[
       \Ev{T = 2 } = A_1 \cap A_2 \ \ \text{ donc } \ \ P \Ev{ T = 2 } = 1 \times p = p . 
       \]

       Pour tout $k \geq 3$, en a de plus : 
       \[
       \Ev{T = k } = A_1 \cap \overline{A_2 } \cap \dots \cap \overline{ A_{k-1} } \cap A_k  . 
       \]

       Or lorsqu'on est à un restaurant autre que le premier, la probabilité d'aller au premier à l'instant suivant est $q$, donc de ne pas y aller est $1-q = p + q$, ce qui donne : 
       \[
       P \Ev{ T = k } = 1 \times (2q ) \times (p+q ) \times \dots \times (p+q) \times q = 2q (p+q)^{k-3} q = 2 q^2 (p+q)^{k-3} . 
       \]

       Enfin on calcule la dernière probabilité à partir des autres : 
       \begin{eqnarray*}
         P \Ev{ T = 0 } & = & 1 - \Sum{k=2}{+\infty} P \Ev{ T = k } = 1 - p - 2 q^2 \Sum{k=3}{+\infty} (p+q)^{k-3} = 1 - p - 2q^2 \Sum{k=0}{+ \infty }  (p+q)^k \\ \\
         & = & 1 - p - 2 q^2 \times \frac{ 1 }{ 1 - (p+q) } = 1 - p - 2 q^2 \times \frac{ 1 }{ q } = 1 - p - 2q = 1 - ( p + 2q ) = 1 - 1 = 0 .
       \end{eqnarray*}

     \item On a vu que la valeur 0 n'arrive qu'avec une probabilité 0, on peut donc la retirer. On considère les séries : 
       \[
       \Sum{k=2}{+\infty} k P \Ev{ T = k } = 2 p + 2 q^2 \Sum{k=3}{+\infty} k (p+q)^{k-3} 
       \]

       et
       \[
       \Sum{k=2}{+\infty} k^2 P \Ev{ T = k } = 4 p + 2 q^2 \Sum{k=3}{+\infty} k^2 (p+q)^{k-3} 
       \]

       Comme $0 < p + q = 1 - q < 1$, ces séries géométriques dérivées et dérivées secondes convergent absolument donc $T$ admet une espérance, un moment d'ordre deux, et enfin une variance et : 
       \begin{eqnarray*}
         E ( T ) & = & 2 p + \frac{ 2 q^2 }{ (p+q)^2 } \left( \Sum{k=1}{+\infty} k (p+q)^{k-1} - 1 - 2 (p+q) \right) = 2 p + \frac{ 2 q^2 }{ (1-q)^2 } \left( \frac{ 1 }{ [ 1 - (p+q) ]^2 } - 1 - 2 (1-q) \right) \\ \\
         & = & 2 p + \frac{ 2 q^2 }{ (1-q)^2 } \left( \frac{ 1 }{ q^2 } - 1 - 2 (1-q) \right) = 2 p + \frac{ 2 q^2 }{ (1-q)^2 } \times \frac{ 1 - q^2 - 2 q^2 (1-q) }{ q^2 } \\ \\
         & = & 2 p + 2 \times \frac{ (1-q) (1+q) - 2 q^2 (1-q) }{ (1-q)^2 } = 2 \left( p + \frac{ 1 + q - 2 q^2 }{ 1- q } \right) .
       \end{eqnarray*}

       puis
       \begin{eqnarray*}
         E ( T^2 ) & = & 4 p +  2 q^2  \Sum{k=3}{+\infty} [ k (k-1) + k ] (p+q)^{k-3}  = 2 p +  2 q^2  \Sum{k=3}{+\infty}  k (k-1)  (p+q)^{k-3} +  2 q^2  \Sum{k=3}{+\infty}  k  (p+q)^{k-3}  \\ \\
         & = & 4 p + 2 \times \frac{ 1 + q - 2 q^2 }{ 1 - q } + \frac{ 2 q^2 }{ p+q } \left( \Sum{k=2}{+\infty}  k (k-1)  (p+q)^{k-2} - 2 \right) \\ \\
         & = & 4 p + 2 \times \frac{ 1 + q - 2 q^2 }{ 1 - q } + \frac{ 2 q^2 }{ 1-q } \left( \frac{ 2 }{ [ 1 -(p+q) ]^3 } - 2 \right) \\ \\
         & = & 4 p + 2 \times \frac{ 1 + q - 2 q^2 }{ 1 - q } + \frac{ 2 q^2 }{ 1-q } \left( \frac{ 2 }{ q^3 } - 2 \right) = 4 p + 2 \times \frac{ 1 + q - 2 q^2 }{ 1 - q } + 4 \frac{ 1 - q^3 ) }{ (1-q) q }
       \end{eqnarray*}

       et enfin :
       \[
       \V( T ) = 4 p + 2 \times \frac{ 1 + q - 2 q^2 }{ 1 - q } + 4 \frac{ 1 - q^3 ) }{ (1-q) q } - 4 \left( p + \frac{ 1 + q - 2 q^2 }{ 1- q } \right)^2 
       \] 

     \end{enumerate}

   \item On réalise l'expérience sur 52 dimanches, en rajoutant un compteur qui devra être incrémenté ) chaque visite de $R_1$ et qui sera divisé par 52 à la fin de la boucle pour avoir la fréquence de visite (on rassemble les restaurants 1 et 2 en un seul restaurant, noté 2) : 

\begin{verbatim}
x=1
c=1
for k=2:52 do
   if x=1 then 
      if rand()<p then c=c+1
          else x=2
      end
          else if rand()<q then
                  x=1 , c=c+1
               end
   end
end
f=c/52
\end{verbatim}

   \end{enumerate}

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $n \in \N^*$. On définit la fonction réelle $f_n$ par : $\forall x \in \R$, $f_n (x) = x + 1 - \frac{ e^x }{ n }$.
   \begin{enumerate}

   \item On étudie la fonction $f_n$ sur $\R_-$ : elle y est dérivable, avec
     \[
     f_n '(x) = 1 - \frac{ e^x }{ n } = \frac{ n - e^x }{ n } > 0 
     \]

     puisque $x \leq 0$, donc $e^x \leq 1$, et enfin $ n - e^x \geq n - 1 \geq 0$ avec $n \geq 1$. On en déduit que $f_n$ est continue et strictement croissante sur $\R_-$, avec par opérations élémentaires
     \[
     \dlim{ x \rightarrow - \infty } f_n (x) =  - \infty \ \ \text{ et } \ \ f_n (0 ) = 1 - \frac{ 1 }{ n } = \frac{ n - 1 }{ n } \geq 0 
     \]

     On en déduit que $f_n$ réalise une bijection de $\R_-$ dans $ \left] - \infty ; \frac{n-1}{n} \right]$, avec $0 \in \left] - \infty ; \frac{n-1}{n} \right]$, donc il existe un unique $x_n \in \R_-$ tel que $f_n (x_n ) = 0$. \\

   \item \begin{enumerate}

     \item Pour comparer $x_n$ et $x_{n+1}$, on va comparer leurs images par $f_n$ : on sait que $f_n (x_n ) = 0$ et on a : 
       \[
       f_n (x_{n+1} ) = x_{n+1} - 1 - \frac{ e^{ x_{n+1} } }{ n } 
       \]

       Or on sait que $f_{n+1} (x_{n+1} ) = x_{n+1} + 1 - \frac{ e^{ x_{n+1} } }{ n + 1 } = 0$ donc on obtient : 
       \[
       f_n ( x_{n+1} ) = \frac{ e^{ x_{n+1} } }{ n + 1 } - \frac{ e^{ x_{n+1} } }{ n } = e^{ x_{ n+1 } } \left( \frac{ 1 }{ n+1 } - \frac{ 1 }{ n} \right) = - \frac{ e^{ x_{ n+1 } } }{ n (n+1) } < 0 
       \]

       et on en déduit par croissance de $f_n$ sur $\R_-$ que :
       \[
       f_n (x_{n+1} ) < 0 = f_n (x_n ) \ \ \text{ donc } \ \ x_{n+1} < x_n 
       \]

       et ce pour tout $n \in \N$, donc la suite $(x_n)$ est décroissante. \\

       On cherche alors l'existence d'un minorant à la suite. 0 n'a aucune chance d'être un minorant puisque la suite est négative, essayons $-1$. Pour comparer $-1$ et $x_n$, on compare les images par $f_n$ : 
       \[
       f_n (-1) = -1 + 1 - \frac{ e^{-1} }{ n } = - \frac{ 1 }{ n e } < 0 = f_n ( x_n ) 
       \]

       donc par croissance de $f_n$, pour tout $n \in \N$, $x_n \geq -1$ et la suite $(x_n)$ est décroissante et minorée par $-1$ : elle converge. \\

     \item On sait que pour tout $n \in \N$ : 
       \[
       f_n (x_n ) = x_n + 1 - \frac{ e^{ x_n } }{ n } = 0 
       \]

       donc en passant à la limite cette égalité, on obtient immédiatement : 
       \[
       \ell + 1 - \frac{ e^{ \ell } }{ + \infty } = 0 \ \ \text{ donc } \ \ \ell + 1 = 0 \ \ \text{ et enfin } \ \ \ell = -1 . 
       \]

     \end{enumerate}

   \item On pa donc $y_n = x_n + 1$, et d'après la définition de $x_n$ on a : 
     \[
     f_n (x_n ) = y_n - \frac{ e^{ x_n } }{ n } = 0 \ \ \text{ donc } \ \ y_n = \frac{ e^{ x_n } }{ n } . 
     \]

     Or le numérateur converge par composition vers $\frac{ 1 }{ e }$, qui n'est pas nul donc c'est un équivalent de ce numérateur. Par quotient on en déduit que : 
     \[
     y_n \underset{ + \infty }{ \sim } \frac{ 1 }{ n e } . 
     \]

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une matrice est diagonalisable lorsque l'une des conditions suivantes est vérifiées : \begin{itemize}

     \item Elle est symétrique (condition suffisante).

     \item Elle est d'ordre $n$ et admet $n$ valeurs propres distinctes (condition suffisante).

     \item Elle est d'ordre $n$ et la somme des dimensions de ses sous-espaces propres vaut $n$ (condition nécessaire et suffisante).

     \item Il existe une base de $\mathcal{M}_{n,1} ( \R )$ constituée de vecteurs propres de cette matrice (nécessaire et suffisante).

     \end{itemize}


     Soit $A$ la matrice de $\mathcal{M}_3 (\R)$ définie par : $A = \begin{smatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ -2 & 1 & 2 \\ \end{smatrix}$.

   \item \begin{enumerate}

     \item $A X = \lambda X$ possède des solutions non nulles si et seulement si $A - \lambda I$ n'est pas inversible, et on en cherche une réduite triangulaire : 
       \begin{eqnarray*}
         \begin{smatrix} -\lambda  & 1 & 0 \\ 0 & - \lambda & 1 \\ -2 & 1 & 2 - \lambda \\ \end{smatrix} & \Longleftrightarrow L_1 \leftrightarrow L_3 & \begin{smatrix} -2 & 1 & 2 - \lambda \\ 0 & - \lambda & 1 \\ -\lambda  & 1 & 0 \\ \end{smatrix} \\ \\
         & \Longleftrightarrow L_3 \leftrightarrow 2 L_3 - \lambda L_1 & \begin{smatrix} -2 & 1 & 2 - \lambda \\ 0 & - \lambda & 1 \\ 0 & 2 - \lambda & \lambda ( \lambda - 2 ) \\ \end{smatrix} \\ \\
         & \Longleftrightarrow L_2 \leftrightarrow  L_2 -  L_3 & \begin{smatrix} -2 & 1 & 2 - \lambda \\ 0 & -2 & 1 - \lambda ( \lambda - 2 ) \\ 0 & 2 - \lambda & \lambda ( \lambda - 2 ) \\ \end{smatrix} \\ \\
         & \Longleftrightarrow L_3 \leftrightarrow  2 L_3 + (2- \lambda)  L_2 & \begin{smatrix} -2 & 1 & 2 - \lambda \\ 0 & -2 & 1 - \lambda ( \lambda - 2 ) \\ 0 & 0 & P (\lambda) \\ \end{smatrix} 
       \end{eqnarray*}

       avec 
       \[
       P ( \lambda ) = 2 \lambda ( \lambda - 2 ) + (2 - \lambda ) [ 1 - \lambda ( \lambda - 2 ) ] 
       \]

       On simplifie alors $P ( \lambda)$, en espérant faire apparaître le polynôme annoncé : 
       \begin{eqnarray*}
         P ( \lambda ) & = & ( \lambda - 2 ) \left[ \rule{0cm}{0.4cm} 2 \lambda - 1 + \lambda ( \lambda - 2 ) \right] = ( \lambda - 2 ) \left[ \rule{0cm}{0.4cm} 2 \lambda - 1 +  \lambda^2 - 2 \lambda \right] \\ \\
         & = & ( \lambda - 2 ) ( \lambda^2 - 1 ) .
       \end{eqnarray*}

       On en déduit sans difficulté que la valeurs de $\lambda$ correspondantes sont $-1 , 1$ et 2, qui sont donc les valeurs propres de $A$. On cherche alors les sous-espaces propres associés en se servant de la réduite triangulaire précédente : \\ \begin{itemize}

       \item Pour $\lambda = -1$, avec $X = \begin{smatrix} x \\ y \\ z \\ \end{smatrix}$, on obtient : 
         \begin{eqnarray*}
           A X = - X & \Longleftrightarrow & ( A + I ) X = 0 \Longleftrightarrow \left\{ \begin{array}{r} -2x + y + 3 z = 0 \\ -2 y -2 z = 0 \\ 0 = 0 \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{r} x = z \\ y = - z \\ \end{array} \right. \\ \\
           & \Longleftrightarrow & X = z \begin{smatrix} 1 \\ -1 \\ 1 \\ \end{smatrix} 
         \end{eqnarray*}

         donc $E_{ -1 } ( A ) = \Vect { \begin{smatrix} 1 \\ -1 \\ 1 \\ \end{smatrix} }$. 

       \item Pour $\lambda = 1$, avec $X = \begin{smatrix} x \\ y \\ z \\ \end{smatrix}$, on obtient : 
         \begin{eqnarray*}
           A X =  X & \Longleftrightarrow & ( A - I ) X = 0 \Longleftrightarrow \left\{ \begin{array}{r} -2x + y +  z = 0 \\ -2 y +2 z = 0 \\ 0 = 0 \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{r} x = z \\ y = z \\ \end{array} \right. \\ \\
           & \Longleftrightarrow & X = z \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix} 
         \end{eqnarray*}

         donc $E_1 ( A ) = \Vect { \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix} }$.

       \item Pour $\lambda = 2$, avec $X = \begin{smatrix} x \\ y \\ z \\ \end{smatrix}$, on obtient : 
         \begin{eqnarray*}
           A X =  2 X & \Longleftrightarrow & ( A - 2I ) X = 0 \Longleftrightarrow \left\{ \begin{array}{r} -2x + y \ \ \ \ = 0 \\ -2 y + z = 0 \\ 0 = 0 \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{r} x = 1/4 z \\ y = 1/2 z \\ \end{array} \right. \\ \\
           & \Longleftrightarrow & X = z \begin{smatrix} 1/4 \\ 1/2 \\ 1 \\ \end{smatrix} 
         \end{eqnarray*}

         donc $E_2 ( A ) = \Vect { \begin{smatrix} 1/4 \\ 1/2 \\ 1 \\ \end{smatrix} } = \Vect { \begin{smatrix} 1 \\ 2 \\ 4 \\ \end{smatrix} } $.

       \end{itemize}

     \item $A$ est diagonalisable car elle est d'ordre 3 et admet 3 valeurs propres distinctes. On en déduit que la concaténation des bases des sous-espaces propres forme une base de vecteurs propres de $A$, donc en posant : 
       \[
       P = \begin{smatrix} 1 & 1 & 1 \\ -1 & 1 & 2 \\ 1 & 1 & 4 \\ \end{smatrix} \ \ \ \text{ et } \ \ \ D = \begin{smatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2 \\ \end{smatrix} 
       \]

       $P$ est inversible et la formule de changement de base donne $A = P D P^{-1}$. \\

     \end{enumerate}

   \item Soit $(x_n)_{ n \in \N}$ une suite réelle définie par : pour tout $n \in \N$, $x_{n+3} = 2 x_{n+2} + x_{n+1} - 2 x_n$. \\

     On pose pour tout $n \in \N$ : $X_n = \begin{smatrix} x_n \\ x_{n+1} \\ x_{n+2} \\ \end{smatrix}$ et $Y_n = P^{-1} X_n$. \begin{enumerate}

     \item On obtient sans difficulté que :
       \[
       X_{ n + 1 } = A X_n . 
       \]

     \item On en déduit que 
       \[
       Y_{ n + 1 } = P^{-1 } A X_n = P^{-1} P D P^{-1} X_n = D P^{-1} X_n = D Y_n 
       \]

       donc par une récurrence immédiate ou par itération de la relation,
       \[
       Y_n = D^n Y_0 . 
       \]

     \item On va calculer $x_n$ pour tout $n$ : on commence par calculer $Y_n$ : il faut pour cela calculer $Y_0 = P^{-1} X_0$, et donc $P^{-1}$. Par méthode de Gauss-Jordan on obtient sans difficulté : 
       \[
       P^{-1} = \frac{1}{6} \begin{smatrix} 2 & - 3 & 1 \\ 6 & 3 & -3 \\ -2 & 0 & 2 \\ \end{smatrix} \ \ \text{ donc } \ \ Y_0 = \frac{1}{6} \begin{smatrix} 2 x_0 - 3 x_1 + x_2 \\ 6 x_0 + 3 x_1 - 3 x_2 \\ -2 x_0 + 2 x_2 \\ \end{smatrix} 
       \]

       puis
       \[
       Y_n = \begin{smatrix} (-1)^n & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2^n \\ \end{smatrix} Y_0 = \frac{1}{6} \begin{smatrix} (-1)^n [ 2x_0 - 3 x_1 + x_2 ] \\ 3 ( 2 x_0 +  x_1 -  x_2 ) \\ 2^{n+1} ( x_2 - x_0 ) \\ \end{smatrix} 
       \]

       et enfin $x_n$ est la première ligne de $X_n = P Y_n$, donc : 
       \[
       x_n = \frac{1}{6} \left[  (-1)^n [ 2x_0 - 3 x_1 + x_2 ] + 3 ( 2 x_0 +  x_1 -  x_2 ) + 2^{n+1} ( x_2 - x_0 ) \rule{0cm}{0.4cm} \right] 
       \]

       Pour que cette suite converge, il faut que les coefficients devant les suites géométriques divergentes soient nuls. En effet si $x_2 - x_0 \neq 0$, on montre que : 
       \[
       x_n = \frac{1}{6} (x_2 - x_0 ) 2^{ n+1} \left( 1 + o (1) \right) \underset{ + \infty }{ \sim } (x_2 - x_0 ) 2^{n+1} 
       \]

       diverge, donc il faut que $x_2 - x_0 = 0$. Ensuite puisque l'autre partie est constante, il faut que $[ (-1)^n ( 2 x_0 - 3 x_1 + x_2 ) ]$ converge, ce qui n'est possible que si la constante est nulle puisque $[ (-1)^n]$ diverge. On en déduit que $(x_n)_{ n \in \N}$ converge si et seulement si : 
       \[
       \left\{ \begin{array}{c} 2 x_0 + - 3 x_1 + x_2 = 0 \\ x_0 = x_2 \\ \end{array} \right. \Longleftrightarrow  x_0 = x_1 = x_2 . 
       \]

       Enfin pour que la série de terme général $(x_n)$ converge, il faut que la suite $x_n$ converge (vers 0) donc que les conditions précédentes soient vérifiées. On obtient alors : 
       \[
       \forall n \in \N , \ x_n = \frac{ 3 }{ 6 } ( 2x_0 + x_1 - x_2 ) = x_0  
       \]

       est une suite constante, et la série de terme général $(x_n)$ ne peut converger que si $x_n = x_0 = 0$, donc si et seulement si : 
       \[
       x_0 = x_1 = x_2 = 0 . 
       \]

     \end{enumerate}

   \item On pose $B = \begin{smatrix} 5 & 0 & -2 \\ 4 & 3 & -4 \\ 8 & 0 & -5 \\ \end{smatrix}$ et pour tout $(a,b) \in \R^2$, $M( a, b ) = \begin{smatrix} 5b & a & -2b \\ 4 b & 3b & a - 4 b \\ -2 a + 8 b & a & 2a - 5 b \\ \end{smatrix}$.\begin{enumerate}

     \item On a montré que les vecteurs propres de $A$ sont les vecteurs de la forme : 
       \[
       \lambda \begin{smatrix} 1 \\ -1 \\ 1 \\ \end{smatrix} \ \ , \ \ \lambda \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix} \ \ , \ \ \lambda \begin{smatrix} 1 \\ 2 \\ 4 \\ \end{smatrix} 
       \]

       et on les teste tous : 
       \[
       B \lambda \begin{smatrix} 1 \\ -1 \\ 1 \\ \end{smatrix} = \lambda \begin{smatrix} 3 \\ -3 \\ 3 \\ \end{smatrix} = 3 \lambda \begin{smatrix} 1 \\ -1 \\ 1 \\ \end{smatrix} 
       \]

       donc les vecteurs propres de $A$ associé à la valeur propre $-1$ sont vecteurs propres de $B$ associés à la valeur propre 3. De même,
       \[
       B \lambda \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix} = \lambda \begin{smatrix} 3 \\ 3 \\ 3 \\ \end{smatrix} = 3 \lambda \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix} 
       \]

       donc les vecteurs propres de $A$ associé à la valeur propre $1$ sont vecteurs propres de $B$ associés à la valeur propre 3. Enfin
       \[
       B \lambda \begin{smatrix} 1 \\ 2 \\ 4 \\ \end{smatrix} = \lambda \begin{smatrix} -3 \\ -6 \\ -12 \\ \end{smatrix} = -3 \lambda \begin{smatrix} 1 \\ 2 \\ 4 \\ \end{smatrix} 
       \]

       donc les vecteurs propres de $A$ associé à la valeur propre $2$ sont vecteurs propres de $B$ associés à la valeur propre $-3$. \\

       La réciproque n'est pas vraie, car on a ici testé tous les vecteurs propres de $A$. Or, comme on a trouvé deux fois la même valeur propre pour $B$, on peut construire par combinaison d'autres vecteurs propres de $B$. Par exemple,
       \[
       \begin{smatrix} 1 \\ -1 \\ 1 \\ \end{smatrix} + \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix} = \begin{smatrix} 2 \\ 0 \\ 2 \\ \end{smatrix} 
       \]

       est vecteur propre de $B$ (associé à la valeur propre 3) mais pas vecteur propre de $A$ : 
       \[
       B \begin{smatrix} 2 \\ 0 \\ 2 \\ \end{smatrix}  = \begin{smatrix} 6 \\ 0 \\ 6 \\ \end{smatrix} = 3 \begin{smatrix} 2 \\ 0 \\ 2 \\ \end{smatrix}  \ \ \ \text{ mais } \ \ \ A \begin{smatrix} 2 \\ 0 \\ 2 \\ \end{smatrix}  = \begin{smatrix} 0 \\ 2 \\ 0 \\ \end{smatrix} 
       \]

       qui n'est pas colinéaire à $\begin{smatrix} 2 \\ 0 \\ 2 \\ \end{smatrix} $. \\

     \item On en déduit que la base de vecteurs propres de $A$ précédente est aussi base de vecteurs propres de $B$ : avec la même matrice $P$ que précédemment et avec $D' = \begin{smatrix} 3 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & -3 \\ \end{smatrix}$, la formule de changement de base donne $B = P D' P^{-1}$ puis : 
       \[
       M (a,b) = a A + b B = a P D P^{-1} + b P D' P^{-1} = P ( a D + b D' ) P^{-1} = P \begin{smatrix} 3b - a & 0 & 0 \\ 0 & a + 3 b & 0 \\ 0 & 0 & 2 a - 3 b \\ \end{smatrix} P^{-1} 
       \]

       On en déduit que $M(a,b)$ est diagonalisable et que ses valeurs propres sont $3b-a , a+3b$ et $2a-3b$. \\

     \item On en déduit que 
       \[
       M( a,b)^n = P \begin{smatrix} (3b - a)^n & 0 & 0 \\ 0 & (a + 3 b)^n & 0 \\ 0 & 0 & (2 a - 3 b)^n \\ \end{smatrix} P^{-1} 
       \]

       qui aura pour limite $P D_{\infty} P^{-1}$, où $D_{ \infty}$ est la limite de $D^n$, si elle existe (et si elle n'existe pas, $M(a,b)^n$ n'a pas de limite. Comme $P$ et $P^{-1}$ sont inversibles, ce produit est nul si et seulement si $D_{ \infty } = 0$, et donc si et seulement si les trois suites géométriques convergent vers 0, donc si et seulement si : 
       \[
       -1 < 3 b - a < 0 \ \ \text{ et } \ \ -1 < a + 3 b < 1 \ \ \text{ et } \ \ -1 < 2a - 3 b < 1 . 
       \]

     \end{enumerate}

   \end{enumerate}

   \indent

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $p \in ]0;1[$. Soit $(X_n)_{ n \in \N^* }$ une suite de variables aléatoires définies sur un espace probabilisé $(\Omega , \mathcal{A} , P )$ indépendantes et de même loi donnée par : 
   \[
   \forall n \in \N^* , \ P \Ev{X_n = -1} = p \ \ \text{ et } \ \ P \Ev{X_n = 1} = 1 - p . 
   \]

   \noindent On pose pour tout $n \in \N^*$, $Z_n = \prod\limits_{i=1}^n X_i$.
   \begin{enumerate}

   \item Les $(X_i)$ sont indépendantes donc : 
     \[
     E ( Z_n ) = E \left( \prod\limits_{i=1}^n X_i \right) = \prod\limits_{i=1}^n E ( X_i ) = \prod\limits_{i=1}^n \left( 1 \times (1-p) - 1 \times p \right) = ( 1 - 2 p)^n . 
     \]

     Or on sait que
     \[
     0 < p < 1 \ \ \text{ donc } \ \ -2 < -2p < 0 \ \ \text{ et enfin } \ \ -1 < 1 - 2p < 1 
     \]

     et donc $E ( Z_n)$ converge vers 0. \\

   \item $Z_n$ est un produit de nombres qui valent 1 ou $-1$, elle vaut donc 1 et ou $-1$ : 
     \[
     Z_n ( \Omega ) = \{ -1 ; 1 \} 
     \]

     De plus $\Ev{Z_n = 1}$ signifie que le nombre de valeurs $X_i$ qui valent $-1$ est pair, donc en posant $S$ la variable égale au nombre de $X_i$ qui valent $-1$ (qui suit une loi binomiale de paramètres $n$ et $p$), on a : 
     \[
     \Ev{Z_n = 1 } = ( S \text{ est paire } ) = \bigcup\limits_{ k=0 }{ \lfloor \frac{n}{2} \rfloor } \Ev{ S = 2 k } . 
     \]

     On en déduit par incompatibilité que : 
     \[
     P \Ev{ Z_n = 1 } = \Sum{k=0}{ \lfloor \frac{ n }{ 2 } \rfloor } \binom{n}{2k} p^{2k} (1-p)^{n-2k} . 
     \]

     Cependant on ne sait pas calculer cette somme. On va alors trouver la valeur des probabilités de manière beaucoup plus astucieuse, en se servant de l'espérance précédemment calculée. Posons $q$ la probabilité de $\Ev{Z_n = 1}$, alors l'autre probabilité vaut $1-q$, et on a :
     \[
     E ( Z_n ) = 1 \times q - 1 \times (1-2q) = 2 q -1 
     \]

     ce qui donne : 
     \[
     2 q - 1 = ( 1 - 2 p)^n \ \ \text{ et } \ \ q = \frac{ (1-2p)^n + 1 }{ 2 } 
     \]

     donc on obtient finalement : 
     \[
     P \Ev{ Z_n = 1 } = \frac{ (1-2p)^n + 1 }{ 2 } \ \ \text{ et } \ \ P \Ev{ Z_n = -1 } = 1 - q = \frac{ 1 - (1-2p)^n }{ 2 } . 
     \]

   \item On connaît les lois de $Z_1$ et $Z_2$, on cherche la loi du couple : 
     \[
     \Ev{Z_1 = 1 } \cap \Ev{Z_2 = 1 } = \Ev{X_1 = 1 } \cap \Ev{X_2 = 1 } \ \ \text{ donc } \ \ P [ \Ev{Z_1 = 1 } \cap \Ev{Z_2 = 1 } = P \Ev{X_1 = 1 } \cap \Ev{X_2 = 1 } ] = (1-p)^2 . 
     \]

     \[
     \Ev{Z_1 = 1 } \cap \Ev{Z_2 = -1 } = \Ev{X_1 = 1 } \cap \Ev{X_2 = -1 } \ \ \text{ donc } \ \ P [ \Ev{Z_1 = 1 } \cap \Ev{Z_2 = -1 } = P \Ev{X_1 = 1 } \cap \Ev{X_2 = -1 } ] = p (1-p) . 
     \]

     \[
     \Ev{Z_1 = -1 } \cap \Ev{Z_2 = 1 } = \Ev{X_1 = -1 } \cap \Ev{X_2 = -1 } \ \ \text{ donc } \ \ P [ \Ev{Z_1 = -1 } \cap \Ev{Z_2 = 1 } = P \Ev{X_1 = -1 } \cap \Ev{X_2 = -1 } ] = p^2 . 
     \]

     \[
     \Ev{Z_1 = -1 } \cap \Ev{Z_2 = -1 } = \Ev{X_1 = -1 } \cap \Ev{X_2 = 1 } \ \ \text{ donc } \ \ P [ \Ev{Z_1 = -1 } \cap \Ev{Z_2 = -1 } = P \Ev{X_1 = -1 } \cap \Ev{X_2 = 1 } ] = p (1-p) . 
     \]

     Enfin $Z_1$ et $Z_2$ sont indépendantes si on a : 
     \[
     \left\{ \begin{array}{c} (1-p)^2 = (1-p) \times \frac{ (1-2p)^2 + 1 }{ 2 } \\ p (1-p) = (1-p) \times \frac{ 1 - (1-2p)^2 }{ 2 } \\ p^2 = p \times \frac{ (1-2p)^2 + 1 }{ 2 } \\ p (1-p) = p \times \frac{ 1 - (1-2p)^2 }{ 2 } \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{c} 2 (1-p) =   (1-2p)^2 + 1  \\ 2 p =  1 - (1-2p)^2  \\ 2p = (1-2p)^2 + 1 \\ 2 (1-p) =  1 - (1-2p)^2 \\ \end{array} \right. 
     \]

     Ceci impose que $p = 1-p$ donc que $p=1/2$, on vérifie alors les valeurs : 
     \[
     (1-2 \times 1/2 )^2 + 1 = 0^2 + 1 = 1 = 2 p = 2 (1-p) \ \ \ \text{ et } \ \ \ 1 - (1 - 2 \times 1/2)^2 = 1 - 0^2 = 1 =  2p = 2 (1-p) 
     \]

     et les quatre égalités sont bien vérifiées. On en déduit que $Z_1$ et $Z_2$ sont indépendantes si et seulement si $p = \frac{1}{2}$.

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Un point critique de $f$ est un couple $(a,b)$ tel que $\partial_1 (f) (a,b) = \partial_2 (f) (a,b) = 0$. \\

     De plus on sait que lorsque la matrice Hessienne au point $(a,b)$, $\nabla^2 (f) (a,b) = \begin{smatrix} \partial_{1,1}^2 (f) (a,b) & \partial_{1,2}^2 (f) (a,b) \\ \partial_{2,1}^2 (f) (a,b) & \partial_{2,2}^2 (f) (a,b) \\ \end{smatrix} $ admet des valeurs propres de même signe (strict), $f$ admet un extremum local au point $(a,b)$. \\

     Soit $X$ une variable aléatoire discrète finie définie sur un espace probabilisé $(\Omega , \mathcal{A} , P)$. \\
     On pose pour tout $n \in \N^*$ : $X ( \Omega ) = \{ x_1 , \dots , x_n \} \subset \R$ et on suppose que $\forall i \in \llb 1 ; n \rrb$, $P \Ev{X=x_i} \neq 0$. \\

     On définit l'entropie de $X$ par : $ H(X) = - \frac{ 1 }{ \ln 2 } \Sum{i=1}{n} P \Ev{X=x_i} \ln \big(  \Prob(\Ev{X=x_i}) \big)$.

   \item Soient $x_1 , x_2 , x_3 , x_4$ quatre réels distincts. On considère un jeu de 32 cartes dont on tire une carte au hasard. Soit $X$ la variable aléatoire prenant les valeurs suivantes : \begin{itemize}

     \item $x_1$ si la carte tirée est rouge (coeur ou carreau),

     \item $x_2$ si la carte tirée est un pique,

     \item $x_3$ si la carte tirée est le valet, la dame, le roi ou l'as de trèfle,

     \item $x_4$ dans les autres cas.

     \end{itemize}

     On tire une carte notée $C$ et un enfant décide de déterminer la valeur $X(C)$ en posant dans l'ordre les questions suivantes auxquelles il lui est répondu par "oui" ou par "non". LA carte $C$ est-elle rouge? La carte $C$ est-elle un pique? La carte $C$ est-elle le valet, la dame, le roi ou l'as de trèfle? \\
     Soit $N$ la variable aléatoire égale au nombre de questions posées (l'enfant cesse de poser des questions dès qu'il a obtenu une réponse "oui"). \begin{enumerate}

     \item On obtient sans difficulté que $X ( \Omega ) = \{ x_1 ; x_2 ; x_3 ; x_4 \}$ et : 
       \[
       P \Ev{ X = x_1 } = \frac{1}{2} \ \ , \ \ P ( X = x_2 = \frac{1}{4} \ \ , \ \ P \Ev{ X = x_3 } = P \Ev{ X = x_4 } = \frac{ 1 }{ 8 } . 
       \]

       On peut alors calculer : 
       \begin{eqnarray*}
         H ( X ) & = & - \frac{ 1 }{ \ln 2 } [ 1/2 \ln (1/2) + 1/4 \ln (1/4) + 2 \times 1/8 \ln (1/8) ] = - \frac{ 1 }{ 4 \ln 2 } [ - 2 \ln 2 - \ln 4 - \ln 8 ] \\ \\
         & = & \frac{ 2 \ln 2 + \ln 2^2 + \ln 2^3 }{ 4 \ln 2 } = \frac{ (2+2+3) \ln 2 }{ 4 \ln 2 } = \frac{ 7 }{ 4 } . 
       \end{eqnarray*}

     \item Il faut au minimum 1 question, et au maximum trois questions pour déterminer $X ( C )$. De plus on remarque que : 
       \[
       \Ev{N = 1 } = \Ev{ X = x_1 } \ \ , \ \ \Ev{ N = 2 } = \Ev{ X = x_2} \ \ , \ \ \Ev{ N = 3 } = \Ev{ X= x_3 } \cup ( \Ev{X = X_4 } 
       \]

       donc : 
       \[
       P \Ev{ N = 1 } = \frac{ 1 }{ 2 } \ \ \text{ et } \ \ P \Ev{ N=2} = P \Ev{ N = 3 } = \frac{ 1 }{ 4 } . 
       \]

       Enfin on calcule : 
       \[
       E ( N ) = \frac{ 1 }{ 2 } + \frac{ 2 }{ 4 } + \frac{ 3 }{ 4 } = \frac{ 7 }{ 4 } = H ( X ) . 
       \]

     \end{enumerate}

   \item Soit $f$ la fonction définie sur $\R^2$ à valeurs réelles telle que : $f(x,y) = x \ln x + y \ln y + (1-x-y) \ln (1-x-y)$. \begin{enumerate}

     \item $f$ est bien définie si et seulement si les $\ln$ le sont, il faut donc : 
       \[
       x > 0 \ \ , \ \ y > 0 \ \ \text{ et } \ \ 1 - x - y > 0 \Longleftrightarrow x + y < 1 \Longleftrightarrow y < 1 - x . 
       \]

       On trace alors dans un repère orthonormé la droite d'équation
       $y = 1-x$, et on hachure la partie du plan au-dessus de l'axe
       des abscisses ($y > 0$), à droite de l'axe des ordonnées ($x >
       0$) et en-dessous de la droite tracée ($y < 1 - x $).

     \item $f$ est de classe $C^2$ sur son en semble de définition et : 
       \[
       \partial_1 (f) (x,y) = \ln x + 1 - \ln (1-x-y) - 1 = \ln x - \ln (1-x-y) \ \ \text{ et } \ \ \partial_2 (f) (x,y) = \ln y - \ln (1-x-y) . 
       \]

       On résout alors le système, avec $\ln$ bijective : 
       \begin{eqnarray*}
         \left\{ \begin{array}{c} \ln x - \ln (1-x-y)  = 0 \\ \ln y - \ln (1-x-y) = 0 \\ \end{array} \right. & \Longleftrightarrow & \left\{ \begin{array}{c} \ln x = \ln (1-x-y)  \\ \ln y - \ln (1-x-y) = 0 \\ \end{array} \right. \Longleftrightarrow \left\{ \begin{array}{c}  x = 1-x-y  \\ \ln y - \ln (1-x-y) = 0 \\ \end{array} \right. \\ \\
         & \Longleftrightarrow & \left\{ \begin{array}{c}  y = 1- 2 x  \\ \ln (1-2x) - \ln (1-x-1 + 2x) = 0 \\ \end{array} \right. \Longleftrightarrow  \left\{ \begin{array}{c}  y = 1- 2 x  \\ \ln (1-2x) = \ln (x)  \\ \end{array} \right. \\ \\
         & \Longleftrightarrow & \left\{ \begin{array}{c}  y = 1- 2 x  \\ 1-2x =x  \\ \end{array} \right. \Longleftrightarrow  \left\{ \begin{array}{c}  y = 1- 2 x  \\ 3x = 1  \\ \end{array} \right. \Longleftrightarrow  \left\{ \begin{array}{c}  y = 1/3  \\ x = 1/3  \\ \end{array} \right.
       \end{eqnarray*}

       donc le seul point critique de $f$ est $(1/3 ; 1/3)$. On calcule alors les dérivées partielles secondes puis la matrice Hessienne au point $(1/3 ; 1/3)$ : 
       \[
       \partial_{1,1}^2 (f ) ( x,y) = \frac{1}{x} + \frac{ 1 }{ 1 - x - y } \ \ , \ \ \partial_{1,2}^2 (f) (x,y) = \partial_{2,1}^2 (f) (x,y) = \frac{  1 }{ 1 - x - y } \ \ , \ \ \partial_{2,2}^2 (f ) ( x,y) = \frac{1}{y} + \frac{ 1 }{ 1 - x - y } 
       \]

       puis au point $(1/3 ; 1/3)$ : 
       \[
       \partial_{1,1}^2 (f ) ( 1/3,1/3) = 3+ 3 = 6 \ \ , \ \ \partial_{1,2}^2 (f) (1/3,1/3) = \partial_{2,1}^2 (f) (1/3,1/3) = 3 \ \ , \ \ \partial_{2,2}^2 (f ) ( 1/3,1/3) = 6 
       \]

       et on cherche les valeurs propres de la Hessienne : 
       \begin{eqnarray*}
         \begin{smatrix} 6 - \lambda & 3 \\ 3 & 6 - \lambda \\ \end{smatrix} & \Longleftrightarrow L_1 \leftrightarrow L_2 & \begin{smatrix} 3 & 6 - \lambda \\ 6 - \lambda & 3 \\ \end{smatrix} \Longleftrightarrow L_2 \leftarrow 3 L_2 - ( 6 - \lambda) L_1 \begin{smatrix} 3 & 6 - \lambda \\ 0 & 9 - (6 - \lambda)^2 \\ \end{smatrix}
       \end{eqnarray*}

       donc les valeurs propres sont les solutions de l'équation : 
       \[
       9 - ( 6 - \lambda)^2 = 0 \Longleftrightarrow ( 3 + 6 - \lambda) ( 3 - 6 + \lambda ) = 0 \Longleftrightarrow ( 9 - \lambda ) ( \lambda - 3 ) = 0 
       \]

       donc 3 et 9, qui sont strictement positives donc $f$ admet un minimum local au point $(1/3,1/3)$. \\

     \item On calcule sans difficulté : 
       \[
       H ( X ) =  - \frac{ 1 }{ \ln 2 } [ p_1 \ln (p_1 ) + p_2 \ln (p_2 ) + p_3 \ln (p_3 ) ] = - \frac{ 1 }{ \ln 2 } f( p_1 , p_2 ) 
       \]

       car on sait que $p_1 + p_2 + p_3 = 1$, donc $p_3 = 1 - p_1 - p_2$. On en déduit avec $ - \frac{ 1 }{ \ln 2 } $ que $f$ et $H(X)$ atteignent leurs extrema locaux aux mêmes points, mais que les natures sont inversées. \\

       Finalement $H(X)$ admet un maximum local au point $p_1 = p_2 = \frac{1}{3}$, donc $p_ 3 = 1 - p_1 - p_2 = \frac{1}{3}$. 

     \end{enumerate}
     
   \end{noliste}

   \indent

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   On rappelle l'identité remarquable $a^3 + b^3 = (a+b) (a^2 - a b + b^2)$. \\ 
   \\
   On vérifie à l'aide de la commutativité de $A$ et $B$ que l'identité remarquable est toujours valable sur les matrices : 
   \[
   (A + B) ( A^2 - A B + B^2 ) = A^3 - A^2 B + A B^2 + B A^2 - B A B + B^3 = A^3 - A^2 B + A B^2 + A^2 B - A B^2 + B^3 = A^3 + B^3 
   \]

   \noindent et puisque $A^3 = 0$, on en déduit que
   \[
   B^3 = A^3 + B^3 = (A + B ) ( A^2 - A B + B^2 ) . 
   \]

   \noindent De plus puisque $B$ est inversible on peut multiplier par $B^{-1}$ (on le fait trois fois, à droite obligatoirement pour obtenir $(A+B) \times ...$) : 
   \[
   B^3 (B^{-1} )^3 = (A + B ) (A^2 - A B + B^2 ) (B^{-1} )^3 \ \ \text{ et enfin } \ \ ( A + B ) [ (A^2 - A B + B^2 ) (B^{-1})^3 ] = I 
   \]

   \noindent donc $A+B$ est inversible, et son inverse est la matrice qui la multiplie pour donner $I$.

 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Dans le cas général, cette intégrale est dite convergente si l'intégrale $\int_a^x f(t) \ dt $ converge lorsque $x$ tend vers $+\infty$. \\

     Si la fonction intégrée est positive, on a alors 4 critères possibles pour obtenir la convergence : \\ \begin{itemize}

     \item Si l'intégrale partielle est bornée (condition nécessaire et suffisante). 

     \item Si la fonction intégrée est majorée par une fonction dont l'intégrale converge (condition suffisante).

     \item Si la fonction intégrée est négligeable en l'infini devant une fonction dont l'intégrale converge (condition suffisante).

     \item Si la fonction intégrée est équivalente en l'infini à une fonction dont l'intégrale converge (condition nécessaire et suffisante).

     \end{itemize}

   \item Soit $x \in \R_+^*$. \begin{enumerate}

     \item Cette intégrale n'est généralisée qu'en $ +\infty$, et comme $ \frac{ 1 }{ t }$ tend vers 0 on a : 
       \[
       e^{ -t }{ x+t } = \frac{ e^{-t} }{ t \left( 1 + \frac{ x }{ t } \right) } \underset{ + \infty }{ \sim } \frac{ e^{ - t } }{ t } = o_{ + \infty } ( e^{ - t } ) 
       \]

       Or les deux fonctions sont à termes positifs, et l'intégrale de 0 à $+\infty$ de $e^{-t}$ converge (densité de la loi exponentielle), donc par théorème de comparaison l'intégrale de la question converge. On pose alors 
       \[
       f(x) = \int_0^{ + \infty } \frac{ e^{ - t } }{ x + t } \ dt . 
       \]

     \item Question difficile par manque d'habitude. Cette fonction n'a rien à voir avec les fonctions intégrales habituellement étudiées, car la dépendance en $x$ ne se trouve pas sur la borne : on ne peut pas poser une primitive de la fonction à l'intérieur fixée (elle devrait dépendre de $x$). \\

       Il faut alors s'inspirer les suites intégrales, qui, sur le même modère que $f$, dépendent en général de $n$ avec le $n$ à l'intérieur de l'intégrale et pas sur la borne, et on revient à la définition de la monotonie : \\

       Soient $x$ et $y$ deux réels strictement positifs, tels que $x < y$. On obtient alors pour tout $t \in \R_+$ : 
       \[
       0 < x + t < y + t \ \ \text{ donc (inverse strictement décroissante sur } \R+^*) \ \ \frac{ 1 }{ y + t } < \frac{ 1 }{ x + t } 
       \]

       On multiplie par $e^{-t} > 0$ et on intègre avec des bornes dans l'ordre croissant, on obtient : 
       \[
       x < y \Longrightarrow f(y) \leq f(x) 
       \]

       et la fonction $f$ est décroissante.

     \end{enumerate}

   \item Soit $g$ et $h$ les fonctions définies sur $\R_+^*$ à valeurs réelles telles que : 
     \[
     g(x) = \int_0^1 \frac{ e^{ -t } - 1 }{ x + t } \ dt \ \ \ \text{ et } \ \ \ h(x) = \int_1^{+\infty} \frac{ e^{ -t } }{ x + t } \ dt . 
     \]

     \begin{enumerate}

     \item $\varphi$ est continue sur $] 0 ; 1]$ par opérations élémentaires, et en 0 on a $-t$ qui  tend vers 0 donc par DL : 
       \[
       \varphi (t) = \frac{ e^{ -t } - 1 }{ t } = \frac{ 1 - t  + o(t) - 1  }{ t } = -1 + o(1) \xrightarrow[ t \rightarrow 0]{} -1 = \varphi (0) 
       \]

       donc $\varphi$ est continue en 0, et donc sur $[0 ; 1]$. \\

     \item Pour tout $x > 0$ et $t \in ] 0 ; 1]$, on a : 
       \[
       x + t > t > 0 \ \ \text{ donc } \ \ 0 <  \frac{1}{ x + t } < \frac{ 1 }{ t } 
       \]

       puis en multipliant par $e^{ - t } - 1 < 0 $ (avec $-t < 0$ donc $e^{ - t } < 1 $) : 
       \[
       \varphi (t) \leq \frac{ e^{ - t } - 1 }{ x + t } \leq 0 
       \]

       et cette inégalité reste vraie en $t=0$, car $\frac{ e^0 - 1 }{ x + 0 } = 0$, donc en intégrant avec des bornes dans l'ordre croissant (avec des intégrales qui convergent car aucune n'est généralisée) on obtient : 
       \[
       \int_0^1 \varphi (t) \ dt \leq g(x) \leq 0 
       \]

       et la fonction $g$ est bien bornée (l'intégrale à gauche est une constante, elle ne dépend pas de $x$). \\

     \item Avec une preuve strictement identique à la fonction $f$ (sauf que cette fois-ci la fonction $h$ est définie en $x=0$, et qu'on intègre entre 1 et $+\infty$), la fonction $h$ est décroissante et vérifie : 
       \[
       h(x) \leq h(0) = \int_1^{+\infty} \frac{ e^{ - t } }{ t } \ dt 
       \]

       avec l'intégrale de droite qui est constante (ne dépend pas de $x$) donc $h$ est majorée. \\

       De plus $h$ est minorée par 0 car c'est l'itnégrale d'une fonction positive avec des bornes dans l'ordre croissant, elle est donc positive. On en déduit finalement que $h$ est bornée. \\

     \item On sépare l'intégrale avec la relation de Chasles pour faire apparaître $h(x)$ : 
       \[
       f(x) = \int_0^1 \frac{ e^{ - t } }{ x + t } \ dt + \int_1^{+\infty} \frac{ e^{ -t } }{ x + t } \ dt = \int_0^1 \frac{ e^{ - t } }{ x + t } \ dt + h(x) . 
       \]

       Ensuite on fait apparaître $g(x)$ : 
       \[
       f(x) = \int_0^1 \frac{ e^{-t} - 1 }{ x + t } \ dt + \int_0^1 \frac{ 1 }{ x + t } \ dt + h(x) = g(x) + h(x) + \int_0^1 \frac{ 1 }{ x + t } \ dt . 
       \]

       Enfin on calcule l'intégrale restante : 
       \[
       f(x) = g(x) + h(x) + \left[ \rule{0cm}{0.4cm} \ln (x+t) \right]_0^1 = g(x) + h(x) + \ln (1+ x ) - \ln (x) . 
       \]

       Au voisinage de 0, les fonctions $g$ et $h$ sont bornées et $\ln (1+x)$ tend vers 0 donc le terme prépondérant est $- \ln x$ qui tend vers $-\infty$ : 
       \[
       f(x) = - \ln x + o ( \ln x ) \underset{ x \rightarrow 0^+ }{ \sim } - \ln x . 
       \]

     \end{enumerate}

   \item Cet encadrement s'obtient immédiatement en mettant les deux fractions au même dénominateur. On va alors s'en servir pour faire apparaître un équivalent de $f(x)$, donc en obtenant un encadrement concernant $f(x)$. \\

     On multiplie donc par $e^{-t} > 0$ et on intègre avec des bornes dans l'ordre croissant (toutes les intégrales sont convergentes) : 
     \[
     0 \leq \frac{1}{x} \int_0^{ + \infty} e^{ - t } \ dt - f(x) \leq \frac{1}{x^2} \int_0^{+\infty} t e^{-t} \ dt . 
     \]

     On reconnaît la densité et l'espérance d'une loi exponentielle, on obtient donc : 
     \[
     0 \leq \frac{ 1 }{ x } - f(x) \leq \frac{ 1 }{ x^2 } 
     \]

     puis on encadre $f$ : 
     \[
     - \frac{1}{x} \leq - f(x) \leq - \frac{1}{x} + \frac{1}{x^2} \ \ \text{ donc } \ \ \frac{ 1 }{ x } - \frac{ 1 }{ x^2 } \leq f(x) \leq \frac{1}{ x } 
     \]

     Enfin en multipliant par $x > 0$ : 
     \[
     1 - \frac{ 1 }{ x } \leq \frac{ f(x) }{ \frac{ 1 }{ x } } \leq 1 
     \]

     et par encadrement (les termes de gauche et droite convergent facilement vers 1), on obtient : 
     \[
     \frac{ f(x) }{ \frac{ 1 }{ x } } \xrightarrow[ x \rightarrow + \infty ]{} 1 \ \ \text{ et enfin } \ \ f(x) \underset{ x \rightarrow + \infty }{ \sim } \frac{ 1 }{ x } . 
     \]

   \end{enumerate}

   \indent

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Les variables aléatoires sont définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$. \\
   Soit $X$ une variable aléatoire qui suit la loi de Poisson de paramètre $\lambda > 0$ et soit $Y$ une variable aléatoire indépendante de $X$ telle que : $ Y ( \Omega ) = \{ 1 ; 2 \} , P \Ev{ Y = 1 } = P \Ev{ Y = 2 } = \frac{ 1 }{ 2 }$. On pose $Z = X Y$.
   \begin{enumerate}

   \item Lorsque $Y=1$, $Z = X Y$ prend toutes les valeurs de $\N$; lorsque $Y = 2$, $Z = X Y$ prend toutes les valeurs paires de $\N$. Finalement on obtient : 
     \[
     Z ( \Omega ) = \N . 
     \]

     Si $k = 2 j + 1$ est impair, il ne peut être atteint qu'avec $Y=1$ donc (avec $X$ et $Y$ indépendantes) : 
     \[
     \forall j \in \N , \ ( Z = 2 j + 1 ) = (X = 2 j + 1 ) \cap \Ev{Y = 1 } \ \ \text{ et } \ \ P ( Z = 2 j + 1 ) = \frac{ 1 }{ 2 } P ( X = 2 j + 1 ) = \frac{ e^{ - \lambda } \lambda^{ 2j + 1 } }{ 2 (2j+1)! } 
     \]

     Si  $k = 2j$ est pair, il peut être atteint avec $Y=1$ ou 2, donc : 
     \[
     \forall j \in \N , \ \Ev{ Z = 2 j } = [ \Ev{ X = 2j } \cap \Ev{ Y = 1 } ] \cup [ \Ev{ X = j } \cap \Ev{ Y = 2 } ] 
     \]

     et
     \[
     P \Ev{ Z = 2j } = \frac{ P \Ev{ X = 2j } + P \Ev{ X = j } }{ 2 } = \frac{ e^{ - \lambda } }{ 2 } \left( \frac{ \lambda^{ 2j } }{ (2j)! } + \frac{ \lambda^j }{ j! } \right) . 
     \]

   \item On décompose : 
     \[
     (Z \text{ est paire } ) = \bigcup\limits_{j=0}^{+\infty} \Ev{Z = 2j } 
     \]

     avec une union incompatible donc : 
     \begin{eqnarray*}
       P ( Z \text{ est paire } ) & = & \Sum{j=0}{+\infty} \left[ \frac{ e^{ - \lambda } }{ 2 } \left( \frac{ \lambda^{ 2j } }{ (2j)! } + \frac{ \lambda^j }{ j! } \right) \right] \\ \\
       & = & \frac{ e^{ - \lambda } }{ 2 } \left[ \Sum{j=0}{+\infty} \frac{ \lambda^{ 2j } }{ (2j)! } + \Sum{j=0}{+\infty} \frac{ \lambda^j }{ j! } \right] \\ \\
       & = & \frac{ e^{ - \lambda } }{ 2 } \left[ \frac{ e^{ \lambda } + e^{ - \lambda } }{ 2 } + e^{ \lambda } \right] = \frac{ 1 + e^{ - 2 \lambda } + 2 }{ 4 } = \frac{ 3 + e^{ - 2 \lambda } }{ 4 } . 
     \end{eqnarray*}

   \end{noliste}
 \end{exercice}

 \newpage
 
 \section{\underline{Annales 2012}}
 
 %\setcounter{exercice}{0}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item La série de terme général $(u_n)$ converge si la suite $(S_n)$ des sommes partielles : $S_n= \sum \limits_{k=1}^n u_k$ admet une limite finie lorsque $n$ tend vers $+\infty$.\\
     La série de terme général $(\ln x)^n$ est géométrique donc converge si et seulement si $|\ln x|<1 \Leftrightarrow -1< \ln x < 1 \Leftrightarrow \frac{1}{e} < x < e$  par stricte croissance de la fonction exponentielle. On a alors : \\
     \[
     \sum_{n=0}^{+\infty} (\ln x)^n = \frac{1 }{1-\ln x}
     \]
   \item 
     \begin{enumerate}
     \item $f_n$ est de classe $\mathcal{C}^2$ sur $]0,+\infty[$ comme somme de produits de fonctions usuelles de classe $\mathcal{C}^2$ sur $]0,+\infty[$.\\
       $\forall x > 0$, $f_n^{'}(x)= \frac{n}{x}(\ln x)^{n-1}-1$.\\
       Ainsi, si $n=1$, $f_1^{'}(x)= \frac{1}{x}-1$ donc $f_1^{''}(x)= -\frac{1}{x^2}$; \\
       si $n \geq 2$, $f_n^{''}(x)= -\frac{1}{x^2} n (\ln x) ^{n-1}+\frac{1}{x^2} n (n-1) (\ln x)^{n-2}= \frac{n}{x^2} (\ln x)^{n-2}[-\ln x + n-1]$.
     \item \noindent


       \begin{center} \begin{tikzpicture}
           \tkzTabInit{$x$/0.8,signe de $f_1^{''}(x)$/1,variations de $f_1^{'}$/2,signe de $f_1^{'}(x)$/1,variations de $f_1$/2}{$0$, $1$, $+\infty$}
           \tkzTabLine{d,- ,t,- , }
           \tkzTabVar{D+/ , R, -/}
           \tkzTabIma{1}{3}{2}{0}
           \tkzTabLine{d,+ ,z,- , }
           \tkzTabVar{D-/ , +/$-1$, -/}
         \end{tikzpicture} \end{center} 
       \vspace{0.3cm}
     \item $\lim \limits_{ x \to 0} f_2=-\infty$ et $f_2(1)= -1$ donc d'après le théorème des valeurs intermédiaires ($f_2$ est continue) il existe $a \in ]0,1[$ tel que $f_2(a)=0$.
     \end{enumerate}
   \item
     \begin{enumerate}
     \item $\frac{n}{x^2} (\ln x)^{n-2}$ est strictement positif sur $]1,+\infty[$ donc $f_n^{''}(x)$ est du signe de $-\ln x+n-1$.\\
       $-\ln(x)+n-1 \geq 0 \Leftrightarrow \ln x \leq n-1 \leftrightarrow x \leq e^{n-1}$ par croissance de exp.
       \begin{center} \begin{tikzpicture}
           \tkzTabInit{$x$/0.8,signe de $f_n^{''}(x)$/1,variations de $f_n^{'}$/2}{$1$, $e^{n-1}$, $+\infty$}
           \tkzTabLine{,+ ,z,- , }
           \tkzTabVar{-/ $-1$, +/ $>0$, -/ $-1$}
           \tkzTabVal{1}{2}{0.5}{$\alpha_n$}{0}
           \tkzTabVal{2}{3}{0.5}{$\beta_n$}{0}
         \end{tikzpicture} \end{center} 
       On a $f_n^{'}(e^{n-1}) >0$, en effet : $f_n^{'}(e^{n-1})= n \left(\frac{n-1}{e} \right)^{n-1}-1$ avec : si $n \geq 4$, $n-1 \geq e$ donc $\frac{n-1}{e} >1$ donc $n \frac{n-1}{e} >1$ d'où $f_n^{'}(e^{n-1}) >0$ et si $n=3$, $f_{3}^{'}(e^2)= 3\frac{2^2}{e^2}-1= \frac{12}{e^2}-1 >0$. \\
       On cherche maintenant à étudier le signe de $f_n'$ : \\
       $f_{n}^{'}$ est continue et strictement croissante sur $]1, e^{n-1}]$ donc définit une bijection de  $]1, e^{n-1}]$ dans $]-1, f_{n}(e^{n-1})] $ qui contient 0 donc il existe un unique $\alpha_n \in ]1, e^{n-1}[$ tel que $f_n^{'}(\alpha_n)=0$.\\
       De même, il existe un unique $\beta_n$ sur $]e^{n-1}, +\infty[$ tel que $f_{n}^{'}(\beta_n)=0$.\\
       \begin{center} \begin{tikzpicture}
           \tkzTabInit{$x$/0.8,signe de $f_n^{'}(x)$/1,variations de $f_n$/2}{$1$, $\alpha_n$, $\beta_n$, $+\infty$}
           \tkzTabLine{,-,z,+ ,z,- , }
           \tkzTabVar{+/ $-1$, -/ $<-1$ ,+/ $>0$,  -/ $-\infty$}
           \tkzTabVal{2}{3}{0.5}{$u_n$}{0}
           \tkzTabVal{3}{4}{0.5}{$v_n$}{0}
           \tkzTabVal{2}{3}{0.75}{$e^{n-1}$}{}
         \end{tikzpicture} \end{center} 
       En effet, $f_n(\beta_n) > f_n(e^{n-1})$ avec $f_n(e^{n-1}) = (n-1)^n-e^{n-1} > (n-1)^{n-1}-e^{n-1} >0$ si $n \geq 4 $ car $n-1 >e$ et la fonction puissance est strictement croissante sur $R_{+}$. Et on vérifie que $f_3(e^2) >0$.\\  \\
       $f_n$ est décroissante donc majorée par $-1$ sur $]1, \alpha_n]$ donc ne s'annule pas sur cet intervalle.\\
       Par deux théorèmes de la bijection sur $[\alpha_n, \beta_n]$ et $]\beta_n ,+\infty[$ on démontre l'existence de deux racines $u_n$ et $v_n$ sur $]1, +\infty[$.
     \item $v_n > \beta_n > e^{n-1}$ or $\lim \limits_{n \to +\infty} e^{n-1}=+\infty$ donc par comparaison, $\lim  \limits_{n \to +\infty} v_n= +\infty$.
     \end{enumerate}
   \item Cette question est une vraie question de recherche. Il faut utiliser les méthodes habituelles d'étude des suites implicites : \\
     $\bullet$ \textbf{On étudie les variations de la suite $u_n$ en comparant $f_n(u_n)$ et $f_n(u_{n+1})$ :} \\
     $f_{n}(u_{n+1})= ( \ln u_{n+1})^n-u_{n+1} $ or $f_{n+1}(u_{n+1})=0$ donc $u_{n+1}= (\ln u_{n+1})^{n+1}$ d'où : \\
     $f_n(u_{n+1})=  ( \ln u_{n+1})^n- ( \ln u_{n+1})^{n+1}=  ( \ln u_{n+1})^n[ 1-\ln(u_{n+1})]$. Avec $u_{n+1} \geq 1$ donc $\ln(u_{n+1}) \geq 0$.\\
     Etudions le signe de $1-\ln u_{n+1} $ :  pour cela, comparons $u_{n+1}$ et $e$: \\
     $\forall n \in \N$, $f_n(e)=( \ln e)^n - e = 1-e<0 $ donc d'après le tableau de variations de $f_n$, $e \in ]1, u_n[$ ou $e \in ]v_n, +\infty[$ or $v_n > e^{n-1} > e$ donc $\forall n \in \N, \ e < u_n$.\\
     Ainsi, $e < u_{n+1}$ donc $1-\ln u_{n+1} <0$ \\
     Donc $f_n(u_{n+1}) <0 = f_n(u_n)$ , ainsi $u_{n+1} \in ]-\infty, u_n[$. 
     \[
     \boxed{\text{La suite $(u_n)$ est décroissante}}
     \]
     $\bullet$ \textbf{On conclut que la suite $(u_n)$ converge vers un certain réel l }  : \\
     $(u_n)$ est décroissante et minorée par 1 donc converge vers un réel $l \geq 1$ \\
     $\bullet$ \textbf{On trouve $l$ en passant à la limite dans la relation $f_n(u_n)=0$}: \\
     $u_n= (\ln u_n)^n$ : $u_n^{\frac{1}{n}}= \ln u_n$ donc $e^{\frac{1}{n} \ln u_n}= \ln u_n$. \\
     avec $\ln u_n \to \ln l$ donc $\frac{1}{n} \ln u_n \to 0$ donc $e^{\frac{1}{n} \ln u_n} \to 1$ \\ En passant à la limite on obtient donc $1= \ln l$ c-a-d $l=e$ .\\ \\
   \end{enumerate}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item
     \begin{enumerate}
     \item $cov(Y_k, Y_{k+1})= \frac{1}{2} \left(\V(Y_k+Y_{k+1})-\V(Y_k)-\V(Y_{k+1}) \right)$.\\
       Or $\V(Y_k+Y_{k+1})= \V(X_k+2X_{k+1}+X_{k+2})= \V(X_k)+4\V(X_{k+1})+\V(X_{k+2})$ par indépendance des $X_k$; \\
       $\V(Y_k+Y_{k+1})=6\V(X_1)$. et, de même, $\V(Y_k)= 2\V(X_1)= \V(Y_{k+1})$ d'où  : \\
       $cov(Y_k, Y_{k+1})= \V(X_1)=pq$. 
     \item On étudie $p \to p(1-p)$ sur $]0,1[$. 
     \end{enumerate}
   \item si $ l \geq k+ 2$ alors $Y_k$ et $Y_l$ sont fonctions de
     variables $X_n$ distinctes : c'est variables étant indépendantes,
     $Y_k$ et $Y_l$ sont indépendantes donc $cov(Y_k, Y_l)=0$ si
     $l=k+1$ alors $cov(Y_k, Y_l)= pq$ d'après 1.a).
   \item On note $Y= \frac{1}{n} \sum \limits_{k=1}^n Y_k$ alors
     d'après l'inégalité de Bienaymé-Tchebychev : $P([|Y-\E(Y)| >
     \varepsilon) \leq \frac{\V(Y)}{ \varepsilon^2}$ c-a-d $\mathcal{P}
     \left( \left[ \left| \frac{1}{n} \sum \limits_{k=1}^n Y_k-2p
         \right|> \varepsilon\right] \right) \leq V \left( \frac{1}{n}
       \sum \limits_{k=1}^n Y_k \right) / \varepsilon^2$.\\ 
     Avec $V \left( \frac{1}{n} \sum \limits_{k=1}^n Y_k \right)=
     \frac{1}{n^2} V \left(\sum \limits_{k=1}^n Y_k
     \right)=\frac{1}{n^2} [\sum \limits_{k=1}^n \V(Y_k) + 2 \sum
     \limits_{k=1}^n \sum \limits_{l=k+1}^n cov(Y_k,
     Y_l)]=\frac{1}{n^2} [\sum \limits_{k=1}^n \V(Y_k) + 2 \sum
     \limits_{k=1}^{n-1} cov(Y_k, Y_{k+1})] = \frac{4n-2}{n^2} \V(X_1)
     \to 0$ .
   \end{noliste}
 \end{exercice}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Soit $(A_i)_{i \in I}$ un système complet d'évènements (c-a-d deux à deux incompatibles et dont la réunion fait l'univers) \textbf{de probabilités non nulles},  alors pour tout évènement $B$ : \\
     \[
     \Prob(B)=\sum \limits_{i \in I} \Prob(B \cap A_i)=\sum \limits_{i \in I} P_{A_i}(B ) \Prob(A_i)
     \]
   \item
     \begin{enumerate}
     \item $I_{n,0}= \int_0^1 x^n dx = \frac{1}{n+1}$.
     \item On pose $u$ et $v$ les fonctions définies sur $[0,1]$ par $u(x)= \frac{x^{n+1}}{n+1}$ et $v(x) = (1-x)^{p+1}$. $u$ et $v$ sont de classe $\mathcal{C}^1$ sur $[0,1]$  de dérivées, $u'(x)= x^n$ et $v'(x)= -(p+1)(1-x)^p$ \\
       Par intégration par parties, on obtient : \\
       \[
       \int_0^1 x^n (1-x)^{p+1} dx = [ \frac{x^{n+1}}{n+1}(1-x)^{p+1}]_0^1+  \int_0^1 \frac{x^{n+1}}{n+1}(p+1)(1-x)^p dx
       \]
       soit: 
       \[
       I_{n,p+1}= \frac{p+1}{n+1} I_{n+1,p}
       \]
     \item On connait $I_{n,0}$ , on part donc de $I_{n,p}$ et on se ramène par récurrence à un $I_{m,0}$ : \\
       On démontrer par récurrence que $I_{n,p}= \frac{p! n!}{(n+p)!} I_{n+p,0}= \frac{p! n!}{(n+p+1)!}$.
     \end{enumerate}
   \item On note $U_k$ l'évènement "choisir l'urne $U_k$". Il existe $\lambda$ tel que pour tout $k\in \llb 1,N \rrb$, $\Prob(U_k)= \lambda k$ (probabilité proportionnelle au nombre de boules rouges. \\
     Or $\sum \limits_{k=1}^N \Prob(U_k)=1$ (les $(U_k)_{1 \leq k \leq N}$ forment un s.c.e) donc $\lambda \sum \limits_{k=1}^N k= 1$ , $\lambda = \frac{2}{N(N+1)}$.
     \[
     \boxed{\forall k \in \llb, 1,N \rrb, \ \Prob(U_k)= \frac{2k}{N(N+1)}}
     \]
   \item
     \begin{enumerate}
     \item La probabilité d'obtenir une boule rouge dépend de l'urne choisie, on décompose donc sur le système complet d'évènements $(U_k)_{ 1 \leq k \leq N}$ : \\
       \[
       \Prob(E_n)= \sum \limits_{k=1}^N \Prob(U_k) P_{U_k}(E_n)
       \]
       Sachant que l'on tire dans l'urne $U_k$, Les tirages étant indépendants, le nombre de boules rouges obtenues au cours de 2n tirages suit une loi binomiale de paramètres 2n et $p = \frac{k}{N}$ donc $P_{U_k}(E_n)=  \binom{2n}{n} \left(\frac{k}{N} \frac{N-k}{N}\right)^n$. Ainsi, 
       \[
       \Prob(E_n)= \sum \limits_{k=1}^N \frac{2k}{N(N+1)} \binom{2n}{n}
       \frac{k^n(N-k)^n}{N^{2n}}
       \]
     \item $P_{E_n}(R_{2n+1})= \frac{\Prob(E_n \cap R_{2n+1})}{\Prob(E_n)}$ avec \\
       $\Prob(E_n \cap R_{2n+1})= \sum \limits_{k=1}^N \Prob(U_k) P_{U_k}(E_n
       \cap R_{2n+1})= \sum \limits_{k=1}^N \Prob(U_k) P_{U_k}(E_n
       )P_{U_k}(R_{2n+1}) = \sum \limits_{k=1}^N \frac{2k}{N(N+1)}
       \binom{2n}{n} \left(\frac{k}{N} \right)^{n} \left(\frac{N-k}{N}
       \right)^n\frac{k}{N}$.\\
       Ainsi, 
       \[ 
       \Prob(E_n \cap R_{2n+1})=\frac{\sum \limits_{k=1}^N \left(
           \frac{k}{N}\right)^{n+2} \left(1-\frac{k}{N} \right)^n
       }{\sum \limits_{k=1}^N \left( \frac{k}{N}\right)^{n+1}
         \left(1-\frac{k}{N} \right)^n}
       \]
       \[
       \Prob(E_n \cap R_{2n+1})= \frac{\frac{1}{N}\sum \limits_{k=1}^N
         \left( \frac{k}{N}\right)^{n+2} \left(1-\frac{k}{N} \right)^n
       }{\frac{1}{N}\sum \limits_{k=1}^N \left(
           \frac{k}{N}\right)^{n+1} \left(1-\frac{k}{N} \right)^n}
       \]
       On reconnait les sommes de Riemann des fonctions $x \mapsto
       x^{n+2} (1-x)^n$ et $x \mapsto x^{n+1}(1-x)^n $ continues sur
       $[0,1]$ donc : \\ 
       $\bullet$ $\frac{1}{N}\sum \limits_{k=1}^N \left(
         \frac{k}{N}\right)^{n+2} \left(1-\frac{k}{N} \right)^n \to
       \int_{0}^1 x^{n+2} (1-x)^n dx$\\ 
       $\bullet$ $\frac{1}{N}\sum \limits_{k=1}^N \left(
         \frac{k}{N}\right)^{n+1} \left(1-\frac{k}{N} \right)^n \to
       \int_{0}^1 x^{n+1} (1-x)^n dx$\\ 
       $\Prob(E_n \cap R_{2n+1}) \to \frac{I_{n+2,n}}{I_{n+1,n}}=
       \frac{n!(n+2)!}{(2n+3)!} \frac{(2n+2)!}{n!(n+1)!}=
       \frac{n+2}{2n+3}$.
     \end{enumerate}
   \item
   \end{enumerate}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item On donne la base canonique de $\mathcal{M}_2(\R)$ : $\left( \begin{smatrix} 1 & 0 \\ 0 & 0 \end{smatrix} , \begin{smatrix} 0 & 1 \\ 0 & 0 \end{smatrix}, \begin{smatrix} 0 & 0 \\ 1 & 0 \end{smatrix}, \begin{smatrix} 0 & 0 \\ 0 & 1 \end{smatrix}\right)$
   \item on prend des matrices inversibles (colonnes non colinéaires)  les plus simples possibles  en faisant attention qu'elles forment une famille libre : $\left( \begin{smatrix} 1 & 0 \\ 0 & 1 \end{smatrix} , \begin{smatrix} 1 & 0 \\ 0 & -1 \end{smatrix}, \begin{smatrix} 0 & 1 \\ 1 & 0 \end{smatrix}, \begin{smatrix} 0 & 1 \\ -1 & 0 \end{smatrix}\right)$
   \item On prend une base des matrices symétriques :  $\left( \begin{smatrix} 1 & 0 \\ 0 & 0 \end{smatrix} , \begin{smatrix} 0 & 1 \\ 1 & 0 \end{smatrix}, \begin{smatrix} 0 & 0 \\ 0 & 1 \end{smatrix}\right)$. On la complète en une base en prenant une matrice triangulaire à deux valeurs propres distinctes , exemple: $ \begin{smatrix} 1 & 1 \\ 0 & 2 \end{smatrix}$.
   \end{noliste}
 \end{exercice}


\newpage


 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Un estimateur $T$ de $\theta$ admettant une espérance est dit sans biais si $\E(T)= \theta$. \\
     Si $T$ admet un moment d'ordre 2, son risque quadratique est défini par $r(T)= E((T- \theta)^2)$.
   \item $X$ est une variable bornée donc admet des moments de tous ordres\\
     $\E(X)= \int_{-\infty}^{+\infty} x f(x) dx= \int_{0}^{\theta} 2\frac{x^2}{\theta^2}dx= \frac{2}{3} \theta$.\\
     $\E(X^2)= \int_{-\infty}^{+\infty} x^2f(x)dx= \int_0^{\theta} 2\frac{x^3}{\theta^2}dx= \frac{1}{2} \theta^2$. Ainsi, d'après Koenig-Huygens, $\V(X)= \E(X^2)-(\E(X))^2=\frac{1}{18} \theta^2$. 
   \item 
     \begin{enumerate}
     \item $\forall x \in \R$, $F(x)= \Prob(\Ev{X \leq x }) = \int_{-\infty}^x f(t)dt$ .\\
       $\bullet$ si $x < 0$, $F(x)=0$ \\
       $\bullet$ si $ 0 \leq x \leq \theta$, $F(x)= \int_{0}^x f(t)dt= \frac{x^2}{\theta^2}$.\\
       $\bullet$ si $x > \theta$, $F(x)= 1$ 
     \item
     \end{enumerate}
   \item \begin{enumerate}
     \item $\E(\overline{X_n})= \frac{1}{n} \sum_{k=1}^n \E(X_k)= \E(X)= \frac{2}{3} \theta$ (linéarité de l'espérance). \\
       Soit alors $T_n = \frac{3}{2} \overline{X_n}$. Par linéarité de l'espérance, $T_n$ est un estimateur sans biais de $\theta$.
     \item $r(T_n)= \V(T_n)= \frac{c^2}{n^2} \V(\sum \limits_{k=1}^n X_k)= \frac{c^2}{n^2} \sum   \limits_{k=1}^n \V(X_k)=\frac{c^2}{n} \V(X)$ par indépendance des $X_k$. \\
       $r(T_n)=\frac{\theta^2}{8n} $.\\
       $r(\overline{X_n})= \V(\overline{X_n})+ \left( b(\overline{X_n})\right)^2 =\frac{(2n+1) \theta^2}{18n} $ 
     \end{enumerate}
   \item \begin{enumerate}
     \item $\forall x \in \R$, $G_n(x)= \Prob(\Ev{M_n \leq x }) = P( \Ev{X_1 \leq x } \cap \Ev{X_2 \leq x} \cap ... \cap X_n \leq x)= \Prob(\Ev{X_1 \leq x}) \Prob(\Ev{X_2 \leq x })...\Prob(\Ev{X_n \leq x})= \Prob(\Ev{X \leq x})^n= \left\{\begin{array}{lr} 0 & \text{ si } x < 0\\ \frac{x^{2n}}{\theta^{2n}} & \text{ si } 0 \leq x \leq \theta\\ 1 & \text{ si } x > \theta\end{array} \right.$.\\
       $G_n$ est de classe $\mathcal{C}_1$ sur $\R$ sauf en $0$ et $\theta$, on vérifie aisément qu'elle est continue en 0 et $\theta$ donc $M_n$ est une variable à densité de densité : $g_n(x)=\left\{\begin{array}{lr} 0 & \text{ si } x < 0\\ \frac{2n x^{2n-1}}{\theta^{2n}} & \text{ si } 0 \leq x \leq \theta\\ 0 & \text{ si } x > \theta\end{array} \right.$
     \item $M_n$ est une variable finie donc admet des moments de tous ordres : \\
       $\E(M_n)= \int_{0}^{\theta} 2n x^{2n}{\theta^{2n}}dx= \frac{2n}{2n+1} \theta$. On choisit donc $W_n= \frac{2n+1}{2n} M_n$ comme estimateur sans biais de $\theta$
     \item Comparons leur risque quadratique : calculer $r(W_n)= \V(W_n)= \frac{(2n+1)^2}{(2n)^2} \V(M_n)$ avec,\\
       $\E(M_n^2)= \frac{n}{n+1} \theta^2$ donc $\V(M_n)= \frac{n}{(n+1)(2n+1)^2} \theta^2$ d'où $r(W_n)= \frac{\theta^2}{4n(n+1)}$. \\
       $W_n$ est meilleur estimateur que $T_n$ car son risque quadratique est en $\frac{1}{n^2}$ au voisinage de $+\infty$ alors que celui de $T_n$ est en $\frac{1}{n}$ (son risque quadratique tend plus vite vers 0).
     \end{enumerate}
   \item
     \begin{enumerate}
     \item On résout l'équation $G_n(a \theta)= \frac{\alpha}{2} \Leftrightarrow a= \left( \frac{\alpha}{2}\right)^{\frac{1}{2n}}$ et on résout l'équation $G_n(\theta)- G_n(b \theta)= \frac{\alpha}{2} \leftrightarrow b = \left( 1-\frac{\alpha}{2}\right)^{\frac{1}{2n}}$ 
     \item On isole $\theta$ dans les inégalité en renversant ces inégalités : \\
       On a $\Prob(M_n \leq a \theta)= \frac{\alpha}{2} \Leftrightarrow \Prob( \theta \geq \frac{M_n}{a})= \frac{\alpha}{2}$. \\
       $\Prob( b \theta \leq M_n)= \Prob( b \theta \leq M_n \leq \theta)= \frac{\alpha}{2}$ donc $\Prob( \theta \leq \frac{M_n}{b})= \frac{\alpha}{2}$.\\
       Ainsi, $\Prob( \frac{M_n}{b} \leq \theta \leq \frac{M_n}{a})=  1- P((\theta \leq \frac{M_n}{b} ) \cup (\theta \geq \frac{M_n}{a})  ) = 1-\Prob(\theta \leq \frac{M_n}{b} )- P (\theta \geq \frac{M_n}{a}) =1-\alpha.$
     \end{enumerate}
   \end{enumerate}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item $A(A^2+A+I)=0$. Si $A$ est inversible, on peut simplifier par $A$ en multipliant par $A^{-1}$ à gauche, on obtient: $A^2+A+I=0 $ soit $A(-A-I)=(-A-I)A= I$ donc $A^{-1}=-A-I$.
   \item  Si $A$ est symétrique alors elle est diagonalisable c'est-à-dire il existe $P$ inversible et $D$ diagonale tels que $A= PDP^{-1}$ où $D$ a sur sa diagonale des valeurs propres de $A$.\\
     Or $X^3+X^2+X$ est un polynôme annulateur de $A$ donc si $\lambda$ est valeur propre de $A$ alors $\lambda^3+\lambda^2+\lambda=0$ donc $\lambda( \lambda^2+\lambda+1)= 0$ or l'équation de degré 2 n'a pas de solutions donc $\lambda=0$ donc $D=0$ d'où $A=P0P^{-1}=0$.
   \end{noliste}
 \end{exercice}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item $X$ et $Y$ sont indépendantes si $\forall i \in X(\Omega)$, $\forall j \in Y(\Omega)$, $\Prob([X=i] \cap [Y=j])= \Prob(\Ev{[X=i]}) \Prob(\Ev{[Y=j]})$.
   \item \begin{enumerate}
     \item faire un graphique pour représenter les différents déplacements possibles. On obtient : \\
       $T_n(\Omega)= \{-1,0,1\}$ , $\Prob(\Ev{[T_n=-1]})= \Prob(\Ev{[T_n=1]})= \frac{1}{4}$ et $\Prob(\Ev{[T_n=0]})= \frac{1}{2} $ (équiprobabilité des quatre déplacements)
       $\E(T_n)= -1 \Prob(\Ev{[T_n=-1]})+0\Prob(\Ev{[T_n=0]})+1\Prob(\Ev{[T_n=1]})= 0$.\\
       $\E(T_n^2)= (-1)^2 \Prob(\Ev{ [T_n=-1]})+ 1^2 \Prob(\Ev{[T_n=1]})= \frac{1}{2}$ donc par K-H $\V(T_n)= \E(T_n^2)= \frac{1}{2}$. 
     \item $\sum \limits_{k=1}^n T_k= \sum \limits_{k=1}^n (X_{k}-X_{k-1})= X_n-X_0=X_n$ (télescopage) .
     \item $\E(T_k)= -1 \Prob(\Ev{[T_k=-1]})+0\Prob(\Ev{[T_k=0]})+1\Prob(\Ev{[T_k=1]})= 0$ et donc par linéarité de l'espérance $\E(X_n)= \sum \limits_{k=1}^n \E(T_k)= 0$.
     \item Commençons par calculer $\V(X_n)$ facile à calculer car somme de variables indépendantes donc $\V(X_n)= \sum \limits_{k=1}^n \V(T_k)= \frac{n}{2}$. Et par K-H : $\E(x_n^2)= \V(X_n)+ \E(X_n)^2$ donc :
       \[
       \E(X_n^2)= \frac{n}{2}
       \]
     \end{enumerate}
   \item \begin{enumerate}
     \item $\Prob([X_n=n] \cap [Y_n=n])=0$ car  à chaque déplacement, l'abscisse et l'ordonnée ne se modifient pas simultanément donc il faudrait 2n déplacements et non n déplacements pour avoir cette configuration.
     \item Il faut utiliser l'inégalité $\E(Z_n)^2 \leq \E(Z_n^2)$ vraie pour toute variable aléatoire : en effet, $\V(Z_n) \geq 0 $ donc $\E(Z_n^2)-\E(Z_n)^2 \geq0$. \\
       Ainsi, $\E(Z_n)^2  \leq \E(Z_n^2)$ avec $Z_n^2= X_n^2+Y_n^2$ donc par linéarité de l'espérance $\E(Z_n^2)= \E(X_n^2)+ \E(Y_n^2)= n$ par symétrie de $X_n$ et $Y_n$. \\
       Ainsi, $\E(Z_n)^2 \leq n $ donc $\E(Z_n) \leq  \sqrt{n}$ ($Z_n$ distance donc positive )
     \end{enumerate}
   \item \begin{enumerate}
     \item Soit $k$ le nombre total de déplacements à l'est et $l$ le nombre total de déplacements au nord. Pour revenir à l'origine la puce doit faire $k$ déplacements à l'ouest et $l$ déplacements au sud soit au total $2(k+l)$ déplacements (nombre pair de déplacements) ainsi, si $n$ impair, $p_n=0$ 
     \item D'après l'explication précédente, si la puce fait $2m$ déplacements alors la somme des déplacements à l'ouest et au nord vaut $m$. \\
       Notons $N$ le nombre de déplacements au nord, $E$ le nombre de déplacements à l'est, $O$ le nombre de déplacements à l'ouest et $S$ le nombre de déplacements au sud.\\
       $\Ev{[N=k]}_{0 \leq k \leq m}$ est un système complet d'évènements donc d'après la formule des probas totales: 
       \[
       \Prob(\Ev{M_n=0})= \sum \limits_{k=0}^n \Prob([M_n=0] \cap [N=k])= \sum \limits_{k=0}^n \Prob( [N=k]\cap [S=k] \cap [O=m-k] \cap [E=m-k]) \text{ (d' après l'explication précédente.)}
       \]
       \underline{ Calculons $\Prob( [N=k]\cap [S=k] \cap [O=m-k] \cap [E=m-k])$} :\\
       -on choisit la place des déplacements vers le nord au cours de $2m$ déplacements : on a $\binom{2m}{k}$ choix.\\
       - ces emplacements étant choisis, on choisit la place des déplacements vers l'est parmi les places restantes : on a $\binom{2m-k}{m-k}$ choix.\\
       - on choisit  l'emplacement des déplacements vers le sud parmi les places restantes : $\binom{m}{k}$ choix. et il ne reste plus de choix pour les déplacements vers l'est. \\
       La probabilité de chacun de ces déplacements possibles, par indépendance des déplacements est $\frac{1}{4^{2m}}$ , on a donc au final : 
       \[
       \Prob( [N=k]\cap [S=k] \cap [O=m-k] \cap [E=m-k])= \binom{2m}{k} \binom{2m-k}{m-k} \binom{m}{k} \frac{1}{4^{2m}}
       \]
       Or on montre facilement que $\binom{2m}{k} \binom{2m-k}{m-k}= \binom{2m}{m} \binom{m}{k}$ donc 
       \[
       \Prob( [N=k]\cap [S=k] \cap [O=m-k] \cap [E=m-k])= \binom{2m}{m}  \binom{m}{k}^2 \frac{1}{4^{2m}}
       \]
       Ainsi, $\Prob(\Ev{M_n=0})=  \binom{2m}{m}   \frac{1}{4^{2m}} \sum \limits_{k=0}^n \binom{m}{k}^2$.
     \end{enumerate}
   \end{enumerate}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item On se sait rien faire lorsque le premier indice tend vers l'infini, on va donc se ramener à ce que l'on connait (les séries) : \\
     Soit $M >n$, $\sum \limits_{k=n}^M \frac{1}{k^3}=\sum \limits_{k=1}^M \frac{1}{k^3}-\sum \limits_{k=1}^n \frac{1}{k^3}$  or la première somme converge lorsque $M$ tend vers $+\infty$ comme série de Riemann avec $\alpha=3 >1$ donc $v_n$ existe bien et vaut : 
     \[
     v_n=\sum \limits_{k=n}^{+\infty} \frac{1}{k^3}=\sum \limits_{k=1}^{+\infty} \frac{1}{k^3}-\sum \limits_{k=1}^n \frac{1}{k^3}
     \]
     On fait tendre $n$ vers $+\infty$ dans la seconde expression, on obtient : 
     \[
     \lim \limits_{n \to + \infty} v_n =\sum \limits_{k=1}^{+\infty} \frac{1}{k^3}-\sum \limits_{k=1}^{+\infty} \frac{1}{k^3}=0 
     \]
   \item On est clairement dans un exercice de comparaison série-intégrale.
     \begin{enumerate}
     \item La fonction $x \mapsto \frac{1}{x^3}$ est décroissante sur $\R_{+}^*$ donc $\forall k \geq 1$, soit $ x \in [k, k+1]$, alors : $\frac{1}{(k+1)^3}\leq \frac{1}{x^3} \leq \frac{1}{k^3}$ . La fonction étant de plus continue, on peut intégrer l'inégalité sur $[k, k+1]$.  (bornes croissantes )  :
       \[
       \int_k^{k+1} \frac{1}{(k+1)^3}dx\leq \int_k^{k+1}\frac{1}{x^3}dx \leq\int_k^{k+1} \frac{1}{k^3}dx 
       \]
       \[
       \frac{1}{(k+1)^3} \leq \int_k^{k+1}\frac{1}{x^3}dx \leq \frac{1}{k^3}
       \]
       On somme alors l'encadrement pour $k$ allant de $n$ à $n+m$ et on obtient le résultat.
     \item Encadrons $v_n$ grâce à la question précédente : 
       $\bullet$ on a $\sum \limits_{k=n}^{n+m} \frac{1}{k^3} \geq  \int_n^{n+m+1}\frac{1}{x^3}dx= \frac{1}{2n^2}-\frac{1}{2(n+m+1)^2}$, on fait tendre $m$ vers $+\infty$, on obtient : 
       \[
       v_n \geq \frac{1}{2n^2}
       \]
       $\bullet$ La seconde inégalité est :  $ \sum \limits_{k=n}^{n+m} \frac{1}{(k+1)^3} \leq  \int_n^{n+m+1}\frac{1}{x^3}dx$ or par changement d'indice on a : 
       $ \sum \limits_{k=n}^{n+m} \frac{1}{(k+1)^3}= \sum \limits_{k=n+1}^{n+m+1} \frac{1}{k^3} = \sum \limits_{k=n}^{n+m+1} \frac{1}{k^3} - \frac{1}{n^3}$.\\
       On passe alors à la limite dans l'inégalité, on obtient : 
       \[
       v_n - \frac{1}{n^3} \leq \frac{1}{2n^2} \Leftrightarrow v_n \leq \frac{1}{2n^2}+ \frac{1}{n^3} 
       \]
       D'où 
       \[
       \frac{1}{2n^2} \leq v_n \leq \frac{1}{2n^2}+ \frac{1}{n^3}
       \]
       Grâce à cet encadrement, on montre par le théorème d'encadrement que $\frac{v_n}{\frac{1}{2n^2}} \to 1$ donc $v_n \sim \frac{1}{2n^2}$. 
     \end{enumerate}
   \end{noliste}
 \end{exercice}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Deux matrices $A$ et $B$ de $\mathcal{M}_n(\R)$ sont semblables s'il existe une matrice $P$ de $\mathcal{M}_n(\R)$ inversible telle que $A=PBP^{-1}$. 
   \item
     \begin{enumerate}
     \item  $f^2(i)= f (f(i))= f(i-j+k)= f(i)-f(j)+f(k)= 2j-2k$ (linéarité de $f$) donc $(f^2-2f+2Id)(i)=f^2(i)-2f(i)+2i=0$ \\
       ainsi, $(2Id-f)((f^2-2f+2Id)(i))= (2Id-f)(0)=0$ par linéarité de $2Id-f$. \\
       De même, on trouve : $(f^2-2f+2Id)(j)=i+j+k$ et $(2Id-f)((f^2-2f+2Id)(j))= (2Id-f)(i+j+k)=0$ et $(f^2-2f+2Id)(k)=i+j+k$ et $(2Id-f)((f^2-2f+2Id)(k))= (2Id-f)(i+j+k)=0$.
     \item Montrons que $f$ est surjective $\Leftrightarrow Im(f)= \R^3$.\\
       $Im(f)= Vect(f(i), f(j), f(k))= Vect(i-j+k, i+2j, j+k)$. Vérifions la liberté de la famille $(i-j+k, i+2j, j+k)$: \\
       $a(i-j+k) + b(i+2j)+c(j+k)= 0 \Leftrightarrow (a+b)i+(-a+2b+c)j+(a+c)k=0$ $\Leftrightarrow \left\{\begin{array}{ccccc} a+ & b & &= & 0\\ -a+ & 2b+ & c & = & 0\\ & -b+ & c & = & 0\end{array} \right.$ car $(i,j,k)$ est une base de $E$ donc une famille libre.\\
       On résout le système par pivots, on obtient :$ a=b=c=0$. La famille est libre et génératrice de $Im(f)$ donc c'est une base de $Im(f)$ et $dim(Im(f)=3$.\\
       $Im(f)$ est un sous-espace de $E$ de dimension 3 donc $Im(f)=E$.\\
       $f$ est une endomorphisme de $E$ ( $E$ de dim finie) surjectif donc bijectif .
       \[
       \text{$f$ est un automorphisme de $E$}
       \]
     \item La matrice de $f$ dans la base $(ij,k)$ est $A= \begin{smatrix}1 & 1 & 0\\-1 & 2 & 1\\1 & 0 & 1 \end{smatrix}$. 
     \item D'après la question 2.a) $(2-X)(X^2-2X+2)$ est un polynôme annulateur de $f$. Donc si $\lambda$ est valeur propre de $f$ alors $(2-\lambda)(\lambda^2-2\lambda+2)=0$ donc $\lambda=2$ .\\
       On résout l'équation $(A-2I)X=0$ on obtient $E_2(f)= Vect(i+j+k)$. 
     \end{enumerate}
   \item La somme des dimensions des sous-espaces propres de $f$ est égale à 1 et non à 3 donc $f$ n'est pas diagonalisable.
     \begin{enumerate}
     \item On résout l'équation $\alpha(-b,a,0)+ \beta (0,c,-b)+ \gamma (-c,0,a)= (0,0,0)$ en distinguant les cas $b\neq 0$ et $b=0$ (alors $a$ ou $c \neq 0$), et on obtient dans tous les cas que la famille est liée or les vecteurs ne sont pas colinéaires deux à deux donc $Vect(U,V,W)= Vect(U,V)$ où $(U,V)$ est une base de $Vect(U,V,W)$. 
     \item  On remarque que $U$, $V$ et $W$ appartiennent à $P$ car ils vérifient l'équation de $P$ donc comme $P$ espace vectoriel alors $Vect(U,V,W) \subset P$. On vérifie facilement en trouvant une base de $P$ que $P$ est de dimension 2 donc : 
       \[
       P= Vect(U,V,W)
       \]
       \underline{Calculons $f(U)$, $f(V)$ et $f(W)$ :}\\
       $f(U)= f(-bi+aj)= -bf(i)+af(j)= -b(i-j+k)+a(i+2j)= (a-b)i+(b+2a)j-bk$ .\\
       $f(V)=ci+(2c-b)j-bk$ et $f(W)= -ci+(c+a)j+(a-c)k$. comme $f(P)$ est un espace vectoriel alors $f(P) \subset P$ si et seulement si $f(U)$, $f(V)$ et $f(W) \in P$, si et seulement si $\left\{ \begin{array}{c}a(a-b)+b(b+2a)-cb=0\\ ac+b(2c-b)-cb=0\\ -ac+b(c+a)+c(a-c)=0 \end{array}\right.$ .\\
       Il reste à résoudre ce système d'équations (bonne chance...)
     \end{enumerate}
   \end{enumerate}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item $x (1-\Phi(ax))= x\Prob(\Ev{X >ax}) (clairement positif)= x \int_{ax}^{+\infty} \frac{1}{\sqrt{2 \pi}} e^{-t^2}{2}dt= \frac{1}{a} \int_{ax}^{+\infty} \frac{ax}{\sqrt{2 \pi}} e^{-t^2}{2}dt \leq  \frac{1}{a} \int_{ax}^{+\infty} \frac{t}{\sqrt{2 \pi}} e^{-t^2}{2}dt= \frac{1}{\sqrt{2 \pi a}}e^{-a^2x2/2} \leq \sqrt{\frac{2}{\pi}} e^\frac{-ax^2}{2}$. 
   \item La densité d'une variable centrée de variance $\frac{1}{a}$ est $f(x)= \sqrt{\frac{a}{2 \pi}}e^{-ax^2/2}$ . Cette densité étant paire, on a : \\
     \[
     \int_{0}^{+\infty}\sqrt{\frac{a}{2 \pi}}e^{-ax^2/2}dx= \frac{1}{2} \Leftrightarrow \int_0^{+\infty} \sqrt{\frac{2}{\pi}} e^\frac{-ax^2}{2}dx = \frac{1}{\sqrt{a}} 
     \]
     Ainsi, par croissance des bornes : \\
     \[
     0 \leq \int_0^{+\infty}x (1-\Phi(ax))dx \leq \frac{1}{\sqrt{a}}
     \]
     Et par le théorème d'encadrement, on obtient : $\lim \limits_{ a \to +\infty} \int_0^{+\infty}x (1-\Phi(ax))dx =0$. 
   \end{noliste}
 \end{exercice}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une fonction $f$ définie sur $\R$ est convexe si elle est au
     dessus de toutes ses tangentes.\\ 
     Caractérisation: une fonction $f$ définie sur $\R$ est convexe si et seulement si elle est au dessous de toutes ses cordes.\\
     Si $f$ est de classe $\mathcal{C}^1$ sur $\R$ alors elle est convexe si et seulement si sa dérivée $f'$ est croissante.\\
     Si $f$ est de classe $\mathcal{C}^2$ sur $\R$ alors elle est convexe si et seulement si sa dérivée seconde $f''$ est positive.\\
   \item \begin{enumerate}
     \item Si $x  \geq 0$, $t \mapsto e^{t^2}$ est continue sur $[0,x]$ donc $\int_0^x e^{t^2}dt$ existe; \\
       Si $x  \leq 0$, $t \mapsto e^{t^2}$ est continue sur $[x,0]$ donc $\int_0^x e^{t^2}dt$ existe.
     \item $g : t \mapsto e^{t^2}$ est continue sur $\R$ donc admet une primitive $G$ sur cet intervalle. Ainsi, $\forall x \in \R$, $f(x)= G(x)-G(0)$ or $G$ est de classe $\mathcal{C}^2$ sur $\R$ en tant que primitive de $g$, fonction de classe $\mathcal{C}^1$ sur $\R$ donc $f$ est aussi de classe $\mathcal{C}_2$ sur $\R$. \\
       $\bullet$ $\R$ est centré en 0.\\
       $\bullet$ Soit $x \in \R$, $f(-x)= \int_{0}^{-x} e^{t^2} dt$ . On pose le changement de variable $u=-t$ de classe $\mathcal{C}^1$ sur $[0,x]$ ($t \mapsto -t$ affine),  alors $f(-x)= \int_0^x e^{(-u)^2} -du= -\int_0^x e^{u^2} du = -f(x)$ \\
       \[
       \boxed{\text{$f$ est impaire}}
       \].\\
       $\forall x \in \R$, $f'(x)= g(x)= e^{x^2}$ et $f''(x)= 2x e^{x^2}$ est du signe de $2x$ car $\exp>0$ donc $f'' >0$ sur $]-\infty, 0[$ et $f''>0$ sur $]0, +\infty[$. \\
       \[
       \boxed{\text{$f$ est concave sur $ ]-\infty,0]$ et convexe sur $[0, +\infty[$}}
       \]
     \item $f' >0$ sur $\R$ donc $f$ est strictement croissante sur $\R$. 
     \end{enumerate}
   \item \begin{enumerate}
     \item si $t >1$ alors $t^2 >t$ donc $\exp(t^2) >  \exp(t)$ donc $\int_1^x e^{t^2}dt > \int_1^x e_t = e^x - e \to +\infty $ lorsque $x \to +\infty$.\\ Ainsi, $\lim \limits_{x  \to +\infty } f(x)= +\infty$ et par imparité, $\lim \limits_{ x \to -\infty } f(x)= -\infty$.\\
       $f$ est continue et strictement croissante sur $\R$ donc réalise une bijection de $\R$ dans $f(\R)= \R$ ainsi,pour tout $ n \in \N^*$, il existe un unique réel $u_n$ tel que $f(u_n)= \frac{1}{n}$. 
     \item $f(u_n)= \frac{1}{n}$ et $f(u_{n+1})= \frac{1}{n+1}$ or $\frac{1}{n+1} < \frac{1}{n}$ par stricte décroissante de la fonction inverse sur $\R_{+}^{*}$ donc $f(u_{n+1}) < f(u_n)$ d'où $u_{n+1} < u_n$ en composant par $f^{-1}$ strictement croissante.
       \[
       \boxed{\text{$(u_n)$ est décroissante}}
       \]
       De plus, $f(x) < f(0)=0$ si $x <0$ donc $u_n >0$ . \\
       La suite $(u_n)$ est décroissante et minorée (par 0) donc convergente vers un certain réel $l$. 
     \item On passe à la limite lorsque $n \to +\infty$ dans l'égalité $f(u_n)= \frac{1}{n}$. Par continuité de $f$ sur $\R$ donc en $l$, on obtient $f(l)=0$. Or l'unique antécédent par $f$ de 0 est 0 donc $l=0$.
     \end{enumerate}
   \item \begin{enumerate}
     \item La fonction $\exp$ est convexe sur $\R$ donc au dessus de sa tangente en $0$ donc $\forall  u\in \R$, $\exp(u) \geq 1+u$. \\
       Pour démontrer l'autre inégalité, on étudie la fonction $u  \mapsto 1+2u - e^u$ sur $[0, \ln(2)]$.
     \item Soit $t \in [0,  \sqrt{\ln(2)}]$, alors $t^2 \in [0, \ln(2)]$ donc en appliquant l'inégalité précédente pour $u=t^2$, on obtient $ 1+t^2 \leq e^{t^2} \leq 1+2t^2$. \\

       Pour pouvoir alors intégrer l'encadrement sur $[0, u_n]$, il faut que $u_n \leq \sqrt{\ln(2)}$ or $\lim \limits_{n \to +\infty } u_n=0$ donc en prenant la définition de la limite pour $ \epsilon= \sqrt{\ln(2)}$, on sait qu'il existe $n_0$ tel que pour tout $n \geq n_0$, $|u_n| = u_n \leq \sqrt{\ln(2)}$. \\
       Pour $n \geq n_0$, par croissance des bornes on obtient : \[
       \int_0^{u_n} (1+t^2)dt \leq \int_0^{u_n} e^{t^2} dt=f(u_n)= \frac{1}{n} \leq \int_0^{u_n} (1+2t^2)dt
       \]
     \item On calcule les intégrales de cette inégalité, on obtient : $u_n + \frac{u_n^3}{3} \leq \frac{1}{n} \leq u_n + \frac{2u_n^3}{3}$. \\ 
       c-a-d $nu_n + \frac{nu_n^3}{3} \leq 1 \leq nu_n + \frac{2n u_n^3}{3}$ .\\ 
       $\bullet$ En divisant par $n u_n$, on obtient déjà que $ n u_n \sim 1$ car par le théorème d'encadrement $\lim \limits_{n \to +\infty} \frac{1}{nu_n}=1$ \\
       Ainsi, $\lim \limits_{n \to +\infty} nu_n=1$. Au passage, on a montré que $u_n \sim \frac{1}{n}$.\\ 
       On renverse alors l'encadrement pour encadrer $n u_n^3$ : 
       \[
       \frac{3}{2}(1-n u_n) \leq n u_n^3 \leq 3(1-n u_n)
       \]
       Par encadrement, on obtient $ n u_n^3 \to 0$.\\ \\
     \end{enumerate}
   \end{noliste}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   $\bullet$ $Y$ prend la valeur 0 et lorsque $X$ prend toutes les valeurs paires supérieures à 2 alors $\frac{X}{2}$ prend toutes les valeurs entières supérieures à 1 donc 
   \[
   \boxed{Y(\Omega)=\N}
   \]
   $\bullet$ Soit $k \geq 1$, $\Prob(\Ev{Y=k})= \Prob( \frac{X}{2}=k)= \Prob(\Ev{X=2k})= p q^{2k-1}$.\\
   $\Prob(\Ev{Y=0})= \Prob( \bigcup \limits_{k=0}{+\infty} [X=2k+1])= \sum \limits_{k=0}^{+\infty} \Prob(\Ev{[X=2k+1]}) $ (incompatibilité deux à deux des évènements de l'union) $=  \sum \limits_{k=0}^{+\infty}pq^{2k} =\frac{p}{1-q^2}=\frac{1}{1+q}$. \\
   Pour tout $k \in \N$, $k\Prob(\Ev{[Y=k]}) \geq 0$ donc la série de terme général $k \Prob(\Ev{[Y=k]})$ converge absolument si et seulement si elle converge. \\
   Sous réserve de convergence , on a : \\
   $\E(Y)= 0 \Prob(\Ev{ [Y=0]})+ \sum \limits_{k=1}^{+\infty} k \Prob(\Ev{[Y=k]})= \sum \limits_{k=1}^{+\infty} k pq^{2k-1}= pq \sum \limits_{k=1}^{+\infty} k(q^2)^{k-1}$. On reconnaît une série géométrique dérivée de raison $q^2$ avec $|q^2| = q^^2 <1$ donc la série converge et 
   \[
   \E(Y)= \frac{pq}{(1-q^2)^2}= \frac{q}{(1+q)^2(1-q)} 
   \]
 \end{exercice}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Soient $X$ et $Y$ deux variables aléatoires discrètes.  La loi du couple $(X,Y)$ est déterminée par : \\
     $ \bullet$  La donnée de $(X,Y)(\Omega)$ : l'ensemble des valeurs prises par le couple.\\
     $\bullet$ La donnée, pour tout $(i,j) \in (X,Y)(\Omega)$ de $\Prob([X=i] \cap [Y=j])$ .\\
     Les lois marginales sont les lois de $X$ et de $Y$. On déterminer la loi d'une variable en utilisant la formule des probas totales sur le sce des valeurs prises par l'autre variable:  
     \[
     \Prob(\Ev{[X=i]})= \sum \limits_{j \in Y(\Omega)} \Prob([X=i] \cap [Y=j])
     \]
     \[
     \Prob(\Ev{[Y=j]})=\sum \limits_{i \in X(\Omega)} \Prob([X=i] \cap [Y=j]) 
     \]
     Pour tout $j  \in Y(\Omega)$, la loi de $X$ sachant $[Y=j]$ est définie par : \\
     Pour tout $i \in X(\Omega)$, $P_{[Y=j]} \Ev{[X=i]} =\frac{\Prob([X=i] \cap [Y=j])} {\Prob(\Ev{[Y=j]})}$
   \item $\bullet$ $(X,Y)( \Omega)= \{(i,j) \in \N^2 | j \leq i\}$. \\
     $\bullet$ $\forall (i,j) \in (X,Y)(\Omega)$, $\Prob([X=i] \cap [Y=j])= \Prob(\Ev{[X=i]})P_{[X=i]}\Ev{ [Y=j]}= e^{-\lambda} \frac{\lambda^i}{i!} \binom{i}{j} p^j (1-p)^{i-j}=e^{-\lambda} \frac{1}{j!(i-j)!}(\lambda p)^j (1-p)^{i-j} $
   \item $\forall j \in \N$, $\Prob(\Ev{[Y=j})= \sum \limits_{i=0}^{+\infty} \Prob([X=i] \cap [Y=j])= \sum \limits_{i=j}^{+\infty} \Prob([X=i] \cap [Y=j])=\sum \limits_{i=j}^{+\infty} e^{-\lambda} \frac{1}{j!(i-j)!}(\lambda p)^j (1-p)^{i-j} = e^{-\lambda} \frac{(\lambda p)^j}{j!} \sum \limits_{i=j}^{+\infty} \frac{(1-p)^{i-j }}{(i-j)!}$. On pose le changement d'indice $k= i-j$: \\
     $\Prob(\Ev{[Y=j]}) =  e^{-\lambda} \frac{(\lambda p)^j}{j!} \sum \limits_{k=0}^{+\infty} \frac{(1-p)^{k }}{k!}= e^{-\lambda} \frac{(\lambda p)^j}{j!} e^{\lambda(1-p)}= e^{-\lambda p} \frac{(\lambda p)^j}{j!}$.\\
     $Y$ suit une loi de Poisson de paramètre $\lambda p$.
   \item $\bullet$ $(X-Y)(\Omega)= \N$ ($X$ peut prendre n'importe quelle valeur entière et de nombre de succès de la binomiale a tjrs une proba non nulle de valoir 0). \\
     $\bullet$ Soit $n \in \N$, $\Prob(\Ev{[X-Y]=n})= \sum \limits_{i=0}^{+\infty} \Prob([X=i] \cap [Y=i-n])= \sum \limits_{i=n}^{+\infty} \Prob([X=i] \cap [Y=i-n])= \sum \limits_{i=n}^{+\infty}e^{-\lambda} \frac{1}{(i-n)!}{n!}(\lambda p)^{i-n}(1-p)^n= e^{-\lambda}\frac{(1-p)^n}{n!}\sum \limits_{i=n}^{+\infty} \frac{1}{(i-n)!}(\lambda p)^{i-n}$ . On pose le changement d'indice $k=i-n$ : \\
     $\Prob(\Ev{[X-Y]=n})=e^{-\lambda}\frac{(1-p)^n}{n!}\sum \limits_{k=0}^{+\infty} \frac{1}{k!}(\lambda p)^{k}=e^{\lambda(1-p)}\frac{(1-p)^n}{n!}$
   \item\begin{enumerate}
     \item $\forall (j,n) \in \N^2$, $\Prob([Y=j ] \cap [X-Y=n])= \Prob([Y=j] \cap [X=j+n])= e^{-\lambda} \frac{1}{j!n!} (\lambda p)^j(1-p)^n= e^{-\lambda p} \frac{(\lambda p)^j}{j!} \times e^{\lambda(1-p)} \frac{(1-p)^n}{n!}= \Prob(\Ev{[Y=j]})\Prob(\Ev{[X-Y=n]})$ .\\
       Les variables $Y$ et $X-Y$ sont indépendantes.
     \item $Y$ et $X-Y$ étant indépendantes, on a $cov(Y, X-Y)=0$ cad $cov(Y,X)- Cov(Y,Y)= Cov(X,Y)-\V(Y)=0$ (linéarité à droite et symétrie de la covariance). \\
       Ainsi, $Cov(X,Y)= \V(Y) $ donc $\frac{Cov(X,Y)}{\sigma(X) \sigma(Y)}= \frac{ \sigma(Y)}{\sigma(X)}= \sqrt{p}$. 
     \end{enumerate}
   \end{noliste}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   $A$ est diagonalisable donc il existe $P \in \mathcal{M}_n(\R)$ inversible et $D\in \mathcal{M}_n(\R) $ diagonale constituée de valeurs propres de $A$ telles que $A=PDP^{-1}$. \\
   Or $X^k-1$ est un polynôme annulateur de $A$ donc si $\lambda$ est valeur propre de $A$ alors $\lambda^k=1$. \\
   $\bullet$ Si $k$ est impair alors $\lambda=1$ donc $D=I_n$ donc $A= PI_n P^{-1}=I_n$ donc $A^2= I_n$.\\
   $\bullet$ Si $k$ est pair alors $\lambda= \pm 1$. Donc $D$ est diagonale de coefficients diagonaux tous égaux à $\pm 1$ donc $D^2=I_n$.\\
   Ainsi, $A^2= PDP^{-1} PDP^{-1}= PD^2 P^{-1}= PI_n P^{-1}= I_n$. 
 \end{exercice}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une fonction est de classe $\mathcal{C}^p$ sur un intervalle $I$ si elle est $p$ fois dérivable et que sa dérivée $p$-ième est continue. \\
     Toute fonction de classe  $\mathcal{C}^p$ admet une formule de Taylor à l'ordre $p-1$ cad pour tout $a$, $b$ $\in I$, $f(b)= \sum_{k=0}^{(p-1)} \frac{(b-a)^k}{k!} f^{(k)}(a)+\int_a^b (b-t)^{p-1} f^{(p)}(t)dt$.
   \item 
     \begin{enumerate}
     \item Montrons que $(f_1,f_2)$ est libre : \\
       Soit $a$ et $b$ deux réels tels que $\forall x \in \R$, $a f_1(x)+b f_2(x)=0$. Alors, en prenant des valeurs particulières ($x=0$ et $x=1$) on obtient $a=0$ et $a+b=0$ cad $a=b=0$. \\
       La famille est libre et génératrice de $E$ donc est une base de $E$.
     \item La dérivation est linéaire donc $\Delta$ est linéaire.\\
       $\forall x \in \R$, $\Delta(f_1)(x)= f_1'(x)= \alpha f_1(x)$ donc $\delta(f_1)= \alpha f_1 \in E$.\\
       $\forall x \in \R$, $\Delta(f_2)(x)=e^{\alpha x}+ \alpha x e^{\alpha(x)}$ donc $\Delta(f_2) = f_1+ \alpha f_2 \in E$ . Par linéarité der $\Delta$ et par stabilité de $E$ par combinaisons linéaires, on obtient, $\Delta(E) \subset E$. \\
       \[
       \boxed{\text{$\Delta$ est un endomorphisme de $E$}}
       \] 
       La matrice de $\Delta$ dans la base $(f_1,f_2)$ est $A= \begin{smatrix} \alpha & 1\\ 0 & \alpha\end{smatrix}$
     \item $A$ est inversible car trig sup avec des coeff diag non nuls donc $\Delta$ est bijective et comme $A$ est triangulaire, son unique valeur propre est $\alpha$. \\
       Si $A$ était diagonalisable alors il existerait $P$ inversible telle que $A=P \alpha I_n P^{-1}= \alpha I_n$ : contradiction donc $\Delta$ n'est pas diagonalisable.
     \end{enumerate}
   \item $A^{-1}= \begin{smatrix}  \frac{1}{\alpha} & -\frac{1}{\alpha^2}\\ 0 & \frac{1}{\alpha}\end{smatrix}$.\\
     Soit $f \in E$, si $\Delta^{-1}(f)=g$ alors $\Delta(g)=f$ cad $g'=f$ cad $g$ est une primitive de $f$. $\Delta^{-1}$ associe donc à $f$ une primitive de $f$ (celle qui est dans $E$). \\
     $A^{-1} \begin{smatrix}-3\\ 2 \end{smatrix}= \begin{smatrix}-\frac{3
         \alpha +2}{\alpha^2} \\ \frac{2}{\alpha}\end{smatrix}$ donc $\Delta^{-1}(-3f_1+2f_2)= -\frac{3
       \alpha +2}{\alpha^2}f_1+\frac{2}{\alpha}$. \\
     Les primitive de $f$ sont donc les fonctions de la forme $-\frac{3 \alpha +2}{\alpha^2}f_1+\frac{2}{\alpha}+ constante$
   \item\begin{enumerate}
     \item On calcule les premières puissances, on trouve $A^2= \begin{smatrix} \alpha^2 & 2 \alpha\\ 0 & \alpha^2\end{smatrix}$,  $A^3= \begin{smatrix} \alpha^3 & 3 \alpha^2\\ 0 & \alpha^3\end{smatrix}$,  $A^4= \begin{smatrix} \alpha^4 & 4 \alpha^3\\ 0 & \alpha^4\end{smatrix}$. On fait alors l'hypothèse que $\forall n \in \N$, $A^n= \begin{smatrix} \alpha^n & n \alpha^{n-1}\\ 0 & \alpha^n\end{smatrix}$ et on démontrer cette relation par récurrence. 
     \item $A^n \begin{smatrix}-3\\ 2 \end{smatrix}= \begin{smatrix}-3 \alpha^n +2n \alpha^{n-1}\\ 2\alpha^n\end{smatrix}$ donc $\Delta^{n}(f)= \alpha^{n-1}[ (-3\alpha +2n)f_1+2 \alpha f_2]$. \\
     \end{enumerate}
   \end{enumerate}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item si $X$ pair alors $(-1)^X=1$ et si $X$ impair alors $(-1)^X=-1$ donc $Y(\Omega)=\{-1,1\}$.\\
     D'après le théorème de transfert, sous réserve de convergence absolue, on a : 
     \[
     \E(Y)=\sum \limits_{k=0}^{+\infty} (-1)^k \Prob(\Ev{X=k}) 
     \]
     Montrons la convergence absolue de cette série : la série de terme général $|(-1)^k \Prob(\Ev{[X=k]})|= \Prob(\Ev{X=k})$ converge (sce) donc la série converge bien absolument. \\
     \[
     \E(Y)=\sum \limits_{k=0}^{+\infty} (-1)^k e^{-\lambda}\frac{\lambda^k}{k!}= e^{-\lambda} \sum \limits_{k=0}^{+\infty}\frac{(-\lambda)^k}{k!}=e^{-2\lambda}
     \]
   \item on a $\E(Y)= 1 \Prob(\Ev{ [Y=1]})-1\Prob(\Ev{[Y=-1]})= e^{-2\lambda}$ et $\Prob(\Ev{[Y=1]})+\Prob(\Ev{[Y=-1]})=1$. On résout ce système de deux équations à deux inconnues, on obtient : $\Prob(\Ev{[Y=1]})=\frac{ e^{-2\lambda}+1}{2}$ et $\Prob(\Ev{[Y=-1]})= \frac{1-e^{-2\lambda}}{2 }$
   \end{noliste}
 \end{exercice}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Si $\int_a^b f(t)dt$ converge absolument, alors elle converge.\\
     Théorèmes de comparaison si $f$ est positif et continue par morceaux sur $]a,b]$ : \\
     $\bullet$ Si $f \leq g$ avec $\int_a^b g(t)dt$ converge alors $\int_a^b f(t) dt $ converge.\\ 
     $\bullet$ Si $f =o_a( g)$ avec $\int_a^b g(t)dt$ converge alors $\int_a^b f(t) dt $ converge.\\ 
     $\bullet$ Si $f \underset{a}{\sim} g$alors $\int_a^b g(t)dt$ converge si et seulement si $\int_a^b f(t) dt $ converge.
   \item Si $x \leq 0$ alors $t \to t^{-x} \sqrt{1+t}$ est continue sur $[0,1]$ donc l'intégrale converge. \\
     Si $x>0$,la fonction intérieure est positive et l'intégrale est impropre en 0 or $\frac{1}{t^x} \sqrt{1+t} \underset{t  \to 0}{sim} \frac{1}{t^x}$ avec $\int_0^1  \frac{1}{t^x}dt$ converge si et seulement si $x<1$ (intégrale de Riemann) donc d'après les théorèmes de comparaison, $f(x)$ existe si et seulement si $x <1$.
   \item On ne sait pas dériver $f$ donc on revient à la définition des variations d'une fonction : \\
     soient $a<b <1$, alors $-a > -b $ donc pour tout $t  \in ]0,1[$, $-a \ln(t) < -b \ln(t) $ car $\ln(t) < 0$ donc $e^{-a \ln(t) } < e^{-b \ln(t) }$ par stricte croissance de l'exponentielle.\\
     On a donc $t^{-a} < t^{-b}$ donc $t^{-a} \sqrt{1+t} < t^{-b} \sqrt{1+t}$. On intègre l'inégalité sur $]0,1[$, par croissance des bornes, on obtient $f(a) < f(b)$.\\
     $f$ est strictement croissante sur $D$.
   \item\begin{enumerate}
     \item $\forall t \in ]0,1]$, $1 \leq 1+t \leq 2$ et $1 \leq \sqrt{1+t} \leq \sqrt{2}$ par croissance de la fct racine sur $\R_+$. d'où $t^{-x} \leq t^{-x} \sqrt{1+t} \leq \sqrt{2} t^{-x}$. \\
       En intégrant cet encadrement sur $t \in ]0,1]$, on obtient (croissance des bornes) : 
       \[
       \frac{1}{1-x} \leq f(x) \leq\frac{\sqrt{2}}{1-x} 
       \]
     \item $\lim \limits_{x \to -\infty} \frac{1}{1-x}=0$ donc d'après le théorème d'encadrement, $\lim \limits_{x \to -\infty } f(x)=0$.\\
       $\lim \limits_{x \to 1^-}   \frac{1}{1-x}=+\infty$ donc d'après le théorème de comparaison, $\lim \limits_{x \to 1^-} f(x)= +\infty$.
     \end{enumerate}
   \item \begin{enumerate}
     \item $f(0)= \int_0^1 t^0 \sqrt{1+t}dt= \int_{0}^1 (1+t)^{\frac{1}{2}}dt= \frac{2}{3} (2^\frac{3}{2}-1)$.
     \item $f(x)= \int_0^1 t^{-x} \sqrt{1+t}dt$. \\
       On pose $u$ et $v$ définies sur $[0,1]$ par  $u(t)=t^{-x}$ et $v(t)=\frac{2}{3}(1+t)\sqrt{1+t}$. $u$ et $v$ sont de classe $\mathcal{C}^1$ sur $]0,1]$ de dérivées $u'(t)= x t^{-(x+1)}$ et $v'(t)= \sqrt{1+t}$. Ainsi, pour tout  $0 < M < 1$, on a, par I.P.P.: \\
       \[
       \int_M^1  t^{-x} \sqrt{1+t}dt= [ \frac{2}{3}t^{-x}(1+t)\sqrt{1+t}]_M^1 +x\frac{2}{3} \int_M^1 t^{-(x+1)}(1+t)\sqrt{1+t}dt 
       \]
       avec $\int_M^1t^{-(x+1)}(1+t)\sqrt{1+t}dt= \int_M^1  (t^{-x}+t^{-(x+1)})\sqrt{1+t}dt=\int_M^1  t^{-x}\sqrt{1+t}dt+\int_M^1  t^{-(x+1)}\sqrt{1+t}dt  $.\\
       On fait tendre $M$ vers $0$, on obtient : 
       \[
       f(x)=\frac{4 \sqrt{2}}{3}+\frac{2}{3}x(f(x)+f(x+1))
       \]
       On isole $f(x+1)$, on obtient : 
       \[
       f(x+1)= \frac{1}{x}[f(x)(\frac{3}{2}-x)-2\sqrt{2}]
       \]
     \item On obtient $f(x+1)\underset{x \to 0}{\sim}  \frac{1}{x}[f(0)frac{3}{2}-2\sqrt{2}]= -\frac{1}{x}$ à condition que $f$ soit continue en 0 ($\ast$].\\
       Alors $f(X) \underset{X \to 1}{\sim} \frac{1}{1-X}$ en posant $X=x+1$. \\ \\
       $(\ast) $ continuité de $f$ en 0 : \\
       $|f(x)-f(0)| = \left|  \int_0^1 (t^{-x}-1) \sqrt{1+t}dt\right| \leq \int_0^1 |t^{-x}-1|\sqrt{1+t}dt \leq \sqrt{2} \int_0^1|t^{-x}-1|dt = \sqrt{2} | \frac{1}{1-x}-1| \to 0$ lorsque $x$ tend vers 0 d'où $f$ est continue en 0.
     \end{enumerate}
   \item
   \end{enumerate}
   \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item L'univers est l'ensemble des $n$-listes d'urnes , avec répétitions donc $card(\Omega)=n^n$. \\
     On note $A$ l'évènement "chaque urne reçoit exactement 1 boule", alors $A$ est constitué de l'ensemble des permutations des $n$ urnes donc $card(A)=n!$.  \\
     Par équiprobabilité des choix, on : \\
     \[
     p_n= \Prob(A)= \frac{n!}{n^n}
     \]. 
   \item $\forall n  \in  \N^*$, $\frac{p_{n+1}}{p_n} = \frac{(n+1)!n^n}{n! (n+1)^{n+1}}= \frac{(n+1)!}{n!} \frac{n^n}{(n+1)^{n+1}}= (n+1)  \frac{n^n}{(n+1)^{n+1}} = \frac{n^n}{(n+1)^n} = \left(\frac{n}{n+1} \right)^n<1$ \\
     La suite $(o_n)$ est donc décroissante et minorée (par 0 car chaque terme est une proba ) donc convergente .
   \item $p_n= \frac{n}{n} \frac{n-1}{n} \frac{n-2}{n} ....\frac{1}{n} < \frac{1}{n} $ donc $0 \leq p_n \leq \frac{1}{n}$ et par le théorème d'encadrement, on trouve $p_n \to 0$. 
   \end{noliste}
 \end{exercice}
 \section{\underline{Annales 2011}}
 %\setcounter{exercice}{0}

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Soit $f$ la fonction définie sur $\R$ par :
   \[
   \forall x \in \R,\ \ f(x) = \frac{e^{- \vert x \vert} }{2}.
   \]
   \begin{enumerate}
   \item C'est une fonction positive, continue sauf en un nombre fini de points et telle que $\int_{-\infty}^{+\infty} f$ converge et vaut 1. \\
   \item $f$ est positive par positivité de l'exponentielle, continue comme composée de fonctions continues (la valeur absolue est bien continue!) et paire donc $\int_{-\infty}^{+\infty} f$ converge absolument et vaut 1 si et seulement si $\int_0^{+\infty} f$ converge et vaut $\frac{1}{2}$. \\
     Or pour $x > 0$, (à préciser pour pouvoir remplacer la valeur absolue!), $\int_0^x f(t)\ dt = \frac{1}{2} \int_0^x e^{-t} = \frac{1}{2}$ en primitivant ou en utilisant la loi exponentielle de paramètre $\lambda = 1$. \\ \\
     Soit $X$ une variable aléatoire définie sur un espace probabilisé $(\Omega , \mathcal{A} , P)$ dont $f$ est une densité de probabilité. \\
   \item \begin{enumerate} 
     \item L'absolue convergence est équivalente à la convergence sur $[0 ; +\infty[$ par positivité de $t \rightarrow t f(t)$ et sur $]-\infty ; 0]$ par négativité de $t \rightarrow t f(t)$ donc sur $]-\infty ; +\infty[$. \\
       De plus la fonction $t \rightarrow t f(t)$ est impaire donc si $\int_0^{+\infty} t f(t)\ dt$ converge, alors $\int_{-\infty}^{+\infty} t f(t)\ dt$ converge et vaut 0. \\
       Or on a $ t^2 \times t e^{-t} = t^3 e^{-t} \xrightarrow[t \rightarrow +\infty]{} 0$ donc $ t e^{-t} = o \left( \frac{1}{t^2} \right)$ et par théorème de comparaison des intégrales de fonctions positives (on compare ici à une intégrale de Riemann convergente), $\int_0^{+\infty} t f(t)\ dt$ converge, donc $X$ admet une espérance et $\E(X) = 0$. \\
     \item Un peu de calcul ici : il faut calculer la fonction de répartition de $f$ pour faire des calculs : \\
       En traitant bien à part les cas $x < 0$ et $x >0$ on trouve : 
       \[
       F(x) = \begin{cases} \frac{1}{2} e^x \text{ si } x <0 \\ 1 - \frac{1}{2} e^{-x}  \text{ si } x \geq 0 \\ \end{cases} 
       \]
       On peut alors calculer$ = \Prob( [X > t - s]) =  \frac{1}{2} e^{s - t}$ et  $P_{[X > s]} \Ev{ [X > t]} = \frac{ \Prob( [X > s] \cap [X > t] )}{P ([X > s]} = \frac{\Prob(\Ev{ [X > t]}) }{\Prob(\Ev{ [X > s]})} = \frac{ \frac{1}{2} e^{-t} }{ \frac{1}{2} e^{ -s} } = e^{s-t} $ si $s \geq 0$ (car alors $t \geq 0$), ce qui contredit le résultat. \\
     \end{enumerate}
   \item Il faut prouver que $H_n$ est croissante, continue à droite et tend vers 0 en $-\infty$ et $1$ en $+\infty$. \\
     Cependant comme cela ressemble à une variable à densité, on considère plutôt $h_n (t) = f(t) ( 1 + t e^{- n \vert t \vert})$ et on prouver que c'est une densité de probabilité : $H_n$, fonction de répartition associée, sera bien une fonction de répartition. \\ \\
     La fonction est continue sur $\R$ par théorèmes généraux sur les fonctions continues. \\
     Pour tout $t \in \R$, $f(t) \geq 0$ donc il faut prouver que $ 1 + t e^{- n \vert t \vert}$ sur $\R$; sur $\R_+$, c'est évident comme somme de deux quantités positives; sur $\R_-$, étudions la fonction $g_n(t) = 1 + t e^{n t}$ : elle est dérivable et on a $g_n'(t) = n t e^{n t } + e^{nt} = ( 1 + nt) e^{nt} $ qui s'annule en $t =- \frac{1}{n }$ qui est le minimum de la fonction (vérifier les signes éventuellement mais cela paraît évident). Enfin on a $g_n \left( - \frac{1}{n} \right) = 1 - \frac{1}{n} e^{-1} = 1 - \frac{1}{n e} \geq 0$ car $ ne \geq 1$. \\
     La fonction $g_n$ est donc positive sur $\R_-$, et $h_n$ est positive sur $\R$. 
     \newpage
     Ensuite on a $h_n (t) = f(t) + t f(t) e^{-n \vert t \vert}$. \\
     La première fonction vérifie $\int_{-\infty}^{+\infty} f(t)\ dt $ converge absolument et vaut 1. \\
     La deuxième est impaire donc comme tout à l'heure si on obtient la convergence sur $[ 0 ; +\infty[$, $\int_{-\infty}^{+\infty} t f(t) e^{-n \vert t \vert}$ convergera absolument et vaudra 0. \\
     Or on a $t f(t) = o \left( \frac{ 1}{t^2} \right)$ en $+\infty$ et $e^{-n \vert t \vert} \xrightarrow[ t \rightarrow +\infty]{} 0$ donc est négligeable devant 1 donc le produit vérifie $t f(t) e^{-n \vert t \vert} = o \left( \frac{ 1}{t^2} \right)$ et par théorème de comparaison des intégrales de fonctions positives, l'intégrale est convergente. \\
     finalement on obtient bien par somme que $\int_{-\infty}^{+\infty} h_n (t)\ dt$ converge absolument et vaut 1. \\
   \item Ce sont des variables aléatoires à densité donc la fonction de répartition de $X$ est continue sur $\R$; \\
     il faut donc prouver que pour tout $x \in \R$, $\int_{-\infty}^x f(t) \left( 1 + t e^{ - n \vert t \vert} \right)\ dt$ converge vers $\int_{-\infty}^x f(t)\ dt$, donc que $\int_{-\infty}^x t f(t) e^{- n \vert t \vert}\ dt \rightarrow0$, et enfin cela équivaut à  $\int_{-\infty}^x t e^{ - (n+1) \vert t \vert} \rightarrow 0.$ \\ \\
     Ici il faut être précis dans les calculs : on le prouve pour $x \leq 0$ par intégration par parties et calcul de l'intégrale. \\
     Ensuite pour $x \geq 0$ on a $\int_{-\infty}^0  t e^{ - (n+1) \vert t \vert} \rightarrow 0$ donc il suffit de prouver $\int_0^x  t e^{ - (n+1) \vert t \vert} \rightarrow 0$, qu'on prouve également avec une intégration par parties (pas la même, la fonction n'a pas la même expression!) et calcul de l'intégrale. \\
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $n$ un entier supérieur ou égal à 2 et $(a_1 , a_2,\ \dots\ , a_n) \in \R^n - \{ 0 ,\ \dots\ ,0) \}$. \\ 
   On considère la matrice colonne $X = \begin{smatrix} a_1 \\ a_2 \\ \vdots \\ a_n \\ \end{smatrix} \in \mathcal{M}_{n,1} (\R)$. \\
   On pose $B = X\ {}^tX $ et $A=\ {}^tX\ X$. \\
   On désigne par $u$ l'endomorphisme de $\R^n$ canoniquement associé à $B$. \begin{enumerate}
   \item $A = \Sum{i=1}{n} a_i^2$ est un réel et $B = ( b_{i,j} )_{ 1 \leq i,j \leq n}$ est une matrice de $\mathcal{M}_n (\R)$ avec $b_{i,j} = a_i a_j$. \\
   \item $u$ est de rang 1 car les colonnes de $B$ sont toutes multiples de $X$ et au moins une est non nulle (car un au moins des $a_i$ est non nul et le terme $a_i^2$ correspondant est alors non nul donc la colonne correspondante est non nulle). \\
   \item $B$ est diagonalisable car elle est symétrique ($b_{i,j} = b_{j,i} = a_i a_j$). \\
   \item $B^k = (X {}^t X ) (X {}^t X ) \dots (X {}^t X ) = X ({}^t X X) \dots ({}^t X X) {}^t X = \left( \Sum{i=1}{n} a_i^2 \right)^{k-1} X {}^t X = \left( \Sum{i=1}{n} a_i^2 \right)^{k-1} B$.
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Toute suite croissante converge si et seulement si elle est majorée. Sinon elle diverge vers $+\infty$. \\
     Toute suite décroissante converge si et seulement si elle est minorée. Sinon elle diverge vers $-\infty$. \\

   \item Dans cette question seulement, on suppose $\alpha = 1$ et $\beta = 2$. 
     \begin{enumerate} 
     \item $f'(x) = \frac{1+x}{1+2x} + x \frac{1+2x - 2(1+x) }{(1+2x)^2} = \frac{(1+x) (1+2x) -x}{(1+2x)^2} = \frac{2x^2 +2x +1}{(1+2x)^2} >0$ (Au numérateur le discriminant est égal à $-4$ donc le trinôme est du signe de son coefficient dominant, donc positif et le dénominateur est un carré donc toujours positif.) \\
       On ne déduit que $f$ est strictement croissante sur $\R_+$, avec $f(0) = 0$ et $\dlim{+\infty} f = +\infty$. \\
     \item L'intervalle $\R_+$ est stable par $f$ et $u_0 \in \R_+$ donc $u_n \in \R_+$ pour tout $n$. \\
       Avec la croissance de $f$ on peut regarder le signe de $u_0 - u_1$; mais comme $u_0$ est quelconque, autant regarder directement le signe de $f(x) - x$ : \\
       $f(x) - x= x \left( \frac{1 + x}{1+2x} - \frac{1+2x}{1+2x} \right) = x \frac{-x}{1+2x} = \frac{-x^2}{1+2x} < 0$ donc la suite est décroissante ($u_{n+1} - u_n = f(u_n) - u_n < 0$) et minorée par 0 donc converge. \\
       De plus elle ne peut converger que vers un point fixe de $f$, vérifiant donc $f(x) = x \Leftrightarrow \frac{-x^2}{1+2x} = 0 \Leftrightarrow x=0$ donc $(u_n)$ converge vers 0. \\
     \item Question toute simple : je vous laisse faire ce programme vous-même. \\
     \end{enumerate}
   \item On peut reprendre la structure des questions précédentes. Essayons de varier un peu : \\
     Pour montrer que $u_n >0$, on fait une récurrence immédiate avec en hérédité : \\
     $u_n >0$ donc $ 1 + \alpha u_n > 0$ et $1 + \beta u_n > 0$ donc $\frac{1+ \alpha u_n}{1 + \beta u_n} > 0$ et enfin $u_{n+1} = u_n \frac{1 + \alpha u_n}{1+ \beta u_n} > 0$. \\
     Ensuite on étudie $u_{n+1} - u_n = u_n \frac{1 + \alpha u_n - 1 - \beta u_n}{1 + \beta u_n} = \frac{ (\alpha - \beta ) u_n^2}{1 + \beta u_n} < 0$ car $\alpha - \beta <0$, $u_n^2 >0$ et $1 + \beta u_n >0$ donc $(u_n)$ est strictement décroissante, et minorée par 0 donc convergente vers un point fixe donc $l$ vérifie $\frac{(\alpha - \beta) l^2}{1 + \beta l} =0$ et enfin $l=0$, donc $(u_n)$ converge vers 0. \\

   \item $v_{n+1} - v_n = \frac{1}{u_{n+1} } - \frac{1}{u_n} = \frac{1}{u_n} \left( \frac{ 1 + \beta u_n}{1 + \alpha u_n} - \frac{1 + \alpha u_n}{1 + \alpha u_n} \right) = \frac{ (\beta - \alpha) u_n }{ (1 + \alpha u_n) u_n} =\frac{ \beta - \alpha }{ (1 + \alpha u_n)}   \xrightarrow[n \rightarrow +\infty]{} \frac{ \beta - \alpha}{ 1 + \alpha \times 0} = \beta - \alpha$. \\

   \item On en déduit (on pose $w_n = v_{n+1} - v_n$) que la suite $W_n = \frac{1}{n} \left( w_0 + w_1 + \dots + u_{n-1} \right)$ converge vers $\beta - \alpha$. \\
     Or $W_n = \frac{1}{n} \left( v_n - v_0 \right) = \frac{1}{n} \left( \frac{1}{u_n} - \frac{1}{u_0} \right)$. \\
     Or on a $\frac{1}{u_n} \rightarrow +\infty$ et $\frac{1}{u_0}$ est une constante donc $ \left( \frac{1}{u_n} - \frac{1}{u_0} \right) \sim \frac{1}{u_n}$ et enfin $W_n \sim \frac{1}{ n u_n} \sim (\beta - \alpha)$ et enfin $u_n \sim \frac{1}{n (\beta - \alpha)}$. \\
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   $n$ souris (minimum 3) sont lâchées en direction de 3 cages, chaque cage pouvant contenir les $n$ souris et chaque souris allant dans une cage au hasard. 
   \begin{enumerate}
   \item On pose $Y_i$ le nombre de souris dans la cage $i$, on a $Y_i \suit \mathcal{B} \left( n , \frac{1}{3} \right)$ donc $\Prob(\Ev{Y_i = 0}) = \left( \frac{2}{3} \right)^n$. \\
     La probabilité cherchée vaut $\Prob( [ Y_1=0] \cup [Y_2 = 0] \cup [Y_3 = 0] ) = \Prob(\Ev{Y_1 = 0}) + \Prob(\Ev{ Y_2 = 0}) + \Prob(\Ev{ Y_3 = 0}) - \Prob( [Y_1 = 0] \cap [Y_2 = 0]) - \Prob([ Y_1 = 0] \cap [ Y_3 = 0]) - \Prob( [ Y_2 = 0] \cap [Y_3 = 0]) + \Prob( [Y_1 = 0] \cap [Y_2 = 0] \cap [Y_3 = 0])$. \\
     Or on a $\Prob(  [Y_1 = 0] \cap [Y_2 = 0] \cap [Y_3 = 0]) = 0$ (les trois cages ne peuvent être vides en même temps, où seraient passé les souris?) \\
     D'autre part pour calculer $P ([Y_ i = 0] \cap [Y_j = 0])$ on pose $Z_{i,j}$ le nombre de souris dans les cages $i$ et $j$, $Z_{i,j} \suit \mathcal{B} \left( n , \frac{2}{3} \right)$ donc $P (  [Y_ i = 0] \cap [Y_j = 0]) = \Prob( Z_{i,j} = 0) = \left( \frac{1}{3} \right)^n$. \\ 
     Enfin  on obtient $\Prob( [ Y_1=0] \cup [Y_2 = 0] \cup [Y_3 = 0] )  = 3  \frac{2^n - 1}{3^n} = \frac{2^n - 1}{3^{n-1}}$. \\

   \item On pose $X_i$ la variable aléatoire égale à 1 si la cage $i$ reste vide, et 0 sinon, on a $X_i \suit \mathcal{B} \left( \left( \frac{2}{3} \right)^n \right)$ et on a $X = X_1 + X_2 + X_3 $ donc $\E(X) = 3 \E(X_1)$ (les trois variables suivent la même loi et ont donc la même espérance). \\
     Enfin  $\E(X) = \frac{2^n}{3^{n-1}}$.
   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une variable aléatoire $X$ est à densité s'il existe une fonction $f$ positive, continue sauf en un nombre finie de points, telle que pour tout $x \in \R$, $F_X ( x) = \int_{-\infty}^x f(t)\ dt$. \\
     Toute fonction de répartition est croissante, continue à droite en tout point et de limites $0$ en $-\infty$ et $1$ en $+\infty$. \\
     La variable est à densité si et seulement si $F_X$ est de plus continue sur $\R$, de classe $C^1$ sauf en un nombre fini de points. \\
   \item $F$ continue sur $\R$ donc admet des primitives, et donc une unique primitive sur $\R$ s'annulant en 0, notée $H_f$. \\
     De plus $H_f$ est dérivable sur $\R$, de dérivée $F$ continue donc $H_f$ est de classe $C^1$ sur $\R$. \\
   \item Donner $H_f$ dans les cas suivants : 
     \begin{enumerate} 
     \item On a alors $F(x) = 0$ si $x \leq 0$ et $F(x) = 1 - e^{-x}$ si $x >0$, puis $H_f(x) = 0$ si $x \leq 0$ et $H_f(x) =   x + e^{-x} - 1$ si $x > 0$, d'asymptote oblique $y = x -1$ en $+\infty$. \\
     \item On a alors $F(x) = 0$ si $x \leq 0$ et $F(x) = 1 - \frac{1}{1+x}$ si $x >0$, puis $H_f(x) = 0$ si $x \leq 0$ et $H_f(x) = x - \ln (1+x)$ si $x > 0$, de direction asymptotique $y=x$ en $+\infty$, mais qui n'a pas d'asymptote en $+\infty$. \\ 
     \item On a alors $F(x) = 0$ si $x \leq 0$ et $F(x) = 1 - \frac{1}{\sqrt{1+x} }$ si $x > 0$ puis $H_f(x) = 0$ si $x \leq 0$ et $H_f(x) = x - 2 \sqrt{1+x} + 2$ si $x > 0$, de direction asymptotique $y = x$, mais qui n'a pas d'asymptote en$+\infty$. \\
     \end{enumerate}
   \item On suppose que $X$ admet une espérance $l$. \begin{enumerate}
     \item On intègre par parties avec $u = t$ et $v = F(t)$ de classe $C^1$ sur $[ 0 ; x]$ et on a : \\
       $\int_0^x t f(t)\ dt = [ t F(t) ]_0^x - \int_0^x F(t)\ dt = x F(x) - H_f (x) + H_f(0) = x F(x) - H_f (x)$. \\
       Comme $X$ admet une espérance, on a $\int_0^x t f(t)\ dt \rightarrow \E(X)$ donc $H_f (x) = x F(x) - \int_0^x t f(t)\ dt = x \left( F(x) - \frac{\int_0^x t f(t)\ dt }{x} \right) \sim x F(x) \sim x $ car $\dlim{+\infty} F(x) = 1$.
       \\ \\
       On en déduit que $\frac{H_f(x) }{x } \sim 1 \xrightarrow[ x \rightarrow +\infty]{} 1$ donc on a une direction asymptotique $y = x$. \\
     \item Difficile de répondre : les cas de la question 3 ne sont pas concluants (les deux cas où il n'y a pas d'asymptote proviennent de variables sans espérance). Il est probable qu'avec une rédaction similaire, on montrer que si $X$ admet une variance, il y a bien une asymptote (en intégrant par parties l'intégrale menant au moment d'ordre 2). \\
       Je ne vois pas comment répondre à la question posée intégralement (ce n'est peut-être pas le but recherché) : il faudrait arriver à obtenir un développement asymptotique de $x (F(x) - 1)$. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $E$ l'ensemble des matrices $M_{a,b} = \begin{smatrix} a & b & b \\ b & a & b \\ b & b & a \\ \end{smatrix}$ où $(a,b)$ prend toute valeur de $\R^2$. \begin{enumerate}
   \item Evident. \\

   \item On peut calculer les premières puissances pour essayer de voir une relation simple : cela échoue. \\
     La matrice est symétrique donc diagonalisable, on va la diagonaliser. \\
     Pour simplifier on utilise le fait que $M(a,b) = a I + b A$ et comme $I = P I P^{-1}$ pour tout $P$, si on diagonalise $A$ on aura $A = P D P^{-1}$ puis $M(a,b) = a P I P^{-1} + b P D P^{-1} = P ( a I + b D) P^{-1}$ et on aura la diagonalisation de $M(a,b)$. \\ 
     Enfin l'étude des valeurs propres et des sous-espaces propres de $A$ donne $A = P D P^{-1}$ avec \\ $P = \begin{smatrix} 1 & 1 & 1 \\ -1 & 0 &  1 \\ 0 & -1 & 1 \\ \end{smatrix}$ et $D = \begin{smatrix} -1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 2 \\ \end{smatrix}$ donc $M(a,b) = P \begin{smatrix} a -b &  0 & 0 \\ 0 & a-b & 0 \\ 0 & 0 & a +2b \\ \end{smatrix}$ donc $M(a,b)^n = P \begin{smatrix} (a -b)^n &  0 & 0 \\ 0 & (a-b)^n & 0 \\ 0 & 0 & (a +2b)^n \\ \end{smatrix} P^{-1}$.

   \end{noliste}
 \end{exercice}

 \newpage


 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Toutes les variables aléatoires de cet exercice sont définies sur
   un espace probabilisé $(\Omega , \mathcal{A} , P)$. Soit $p \in ]0
   ; 1[$ et $q = 1-p$.
   \begin{enumerate}
   \item $n$ variables discrètes $(X_1 ,\ \dots\ , X_n$ sont mutuellement indépendantes ou indépendantes dans leur ensemble si pour tout $(x_1 ,\ \dots\ , x_n) \in \R^n$, $P \left( \bigcap\limits_{i=1}^n X_i = x_i \right) = \prod\limits_{i=1}^n \Prob(\Ev{ X = x_i})$. \\
     Bien évidemment il suffit de le vérifier pour des $x_i$ toujours dans $X_i (\Omega)$ pour tout $i$. \\

   \item 
     \begin{enumerate} 
     \item $X_1$ et $X_2$ suivent des lois géométriques de paramètre $p$ donc on a $\Prob(\Ev{X_1 = 0}) = \Prob(\Ev{ X_2 = 0}) = 0$. \\
     \item Déjà fait; l'indépendance des deux lois est obtenue par indépendance des lancers pairs et des lancers impairs. \\
     \item $Y(\Omega) = \N$, $\Prob(\Ev{ Y=0}) = 0$ et pour tout $k \geq 0$ : \\
       $\Prob(\Ev{ Y > k}) = \Prob( [X_1 > k] \cap [X_2 > k] ) = \Prob(\Ev{ X_1 > k})^2$ par indépendance et même loi. \\
       D'où $\Prob(\Ev{Y > k}) = ( q^k)^2$ et $\Prob(\Ev{ Y \leq k}) = 1 - (q^2)^k$. \\
       Enfin pour $k \geq 1$, $\Prob(\Ev{Y= k}) = \Prob(\Ev{ Y \leq k}) - \Prob(\Ev{ Y \leq k-1}) = (q^2)^{k-1} - (q^2)^k = (q^2)^{k-1} ( 1 - q^2)$ et $Y$ suit la loi géométrique de paramètre $1 - q^2$. \\
     \end{enumerate}
   \item Soit $X$ une variable aléatoire suivant une loi géométrique de paramètre $p$. 
     \begin{enumerate}
     \item On a $Y (\Omega) = \N^*$ : $Y \subset \N^*$ est évident et pour $k \neq 0$, $Y=k$ est atteint pour $X = 2k$ donc on a bien $\N^* \subset Y(\Omega)$. \\
       Ensuite on a plus précisément $\Ev{ Y = k} = \Ev{ X = 2k} \cup (X = 2k - 1)$ qui sont incompatibles donc $\Prob(\Ev{ Y = k}) = P \Ev{X = 2k} + \Prob(\Ev{ X = 2k-1}) = p \left( q^{2k-1} + q^{2k-2} \right) = p q^{2k-2} ( q + 1) = [ (1 + q) (1-q) ] (q^2)^{k-1} = (1-q^2) (q^2)^{k-1}$
       donc $Y \suit \mathcal{G} ( 1 - q^2)$. \\
     \item L'étude de la partie entière montre que $ (2Y - X) (\Omega) = \{ 0 ; 1 \}$ et on a : \\
       $\Prob( 2Y - X =0) = P (X$ pair)$= \Sum{k=1}{+\infty} \Prob(\Ev{ X = 2k}) = p q^{-1} \Sum{k=1}{+\infty} (q^2)^k = \frac{ p q^2}{q ( 1 -q^2) } = \frac{ q}{1+q}$. \\
       $\Prob( 2Y - X =1) = P (X$ impair)$= \Sum{k=0}{+\infty} \Prob(\Ev{ X = 2k+1}) = p \Sum{k=0}{+\infty} (q^2)^k =\frac{p}{1-q^2} = \frac{ 1}{  1 +q }$. \\ 
       Enfin on a $ P( \Ev{Y = k } \cap (2 Y - X =0) ) = \Prob(\Ev{ X = 2k}) = p q^{2k-1}$ et $\Prob(\Ev{ Y=k}) \Prob( 2 Y - X = 0) = (1-q^2) (q^2)^{k-1} \frac{q}{1+q} = (1-q) q^{2k-1} = p q^{2k-1}$. \\
       De même on a $ P( \Ev{Y = k } \cap (2 Y - X =1) ) = \Prob(\Ev{ X = 2k-1}) = p q^{2k-2}$ et $\Prob(\Ev{ Y=k}) \Prob( 2 Y - X = 1) = (1-q^2) (q^2)^{k-1} \frac{1}{1+q} = (1-q) q^{2k-2} = p q^{2k-2}$, et les variables sont indépendantes. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   On note $E_4$ l'espace vectoriel des fonctions polynomiales de degré inférieur ou égal à 4 et on considère l'application $\Delta$ qui à un polynôme $P$ de $E_4$ associe le polynôme $Q = \Delta (P)$ défini par : $Q(x) = P(x+2) - P(x)$. \begin{enumerate}
   \item La linéarité est évidente. \\
     De plus on a $\deg P(x+2) = \deg P \times \deg (X+2) = \deg P$ donc $\deg \Delta (P) \leq \min (\deg P , \deg P) = \deg P \leq 4$ donc $\Delta (P) \in E_4$ et $\Delta$ est un endomorphisme. \\ \\
     Avec le binôme de Newton on trouve : $Mat_{\mathcal{B} } ( \Delta) = \begin{smatrix} 0 & 2 & 4 & 8 & 16 \\ 0 & 0 & 4 & 12 & 32 \\ 0 & 0 & 0 & 6 & 24 \\ 0 & 0 & 0 & 0 & 8 \\ 0 & 0 & 0 & 0 & 0 \\ \end{smatrix}$.
   \item Il est beaucoup plus simple d'utiliser la matrice : on trouve $\ker \Delta = \Vect{ e_0} = \R_0 [X]$. \\
     Sinon avec l'indication on suppose que $P(x+2) = P(x)$, alors on a $P(2) = P(0)$, puis $P(4) = P(2) = P(0)$ et par une récurrence simple, pour tout $n$, $P(2n) = 0$ donc $P(x) - P(0)$ a une infinité de racines, donc $P(x) - P(0) = 0$ et enfin $P(x) = P(0)$ est une constante. D'où $\ker \Delta \subset \R_0 [X]$. \\
     Enfin on trouve facilement que $\R_0[X] \subset \ker \Delta$ en prenant un polynôme constant, qui vérifie trivialement $P(x+2)= P(x)$ et on obtient le résultat. \\
   \item La matrice de $\Delta$ est triangulaire, on obtient que 0 est l'unique valeur propre. \\
     Si $\Delta$ était diagonalisable, il existerait $P$ inversible telle que $M = P 0 P^{-1} = 0$, ce qui est absurde. \\
   \item Soit $Q$ un polynôme admettant un antécédent, on a $\Delta (P) = Q$. \\
     Alors l'équation $\Delta (R) = Q$ est équivalente à $\Delta (R) - \Delta (P) = \Delta (R - P) = 0$ donc tout polynôme de la forme $R = P + cste$ est solution, et la réponse à la question est non.

   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Dans tout l'exercice, $n$ désigne un entier naturel non nul et $\R_n[X]$ l'espace vectoriel des polynômes à coefficients réels, de degré inférieur ou égal à $n$. On note $M (m_{i,j})_{1 \leq i,j \leq n+1}$ la matrice de $\mathcal{M}_{n+1} (\R)$ de terme général : 
   \[
   m_{i,j} = \left\{ \begin{array}{cc} i & \text{ si } j = i+1 \\ n + 1 - j & \text{ si } i = j + 1 \\ 0 & \text{ dans tous les autres cas } \\ \end{array} \right.
   \]
   et $u$ l'endomorphisme de $\R_n[X]$ dont la matrice dans la base canonique $(1 , X ,\ \dots\ , X^n)$ est égale à $M$.
   \begin{enumerate}
   \item Soit $u$ un endomorphisme d'un espace vectoriel $E$, on appelle vecteur propre de $u$ tout vecteur $X$ non nul tel qu'il existe un réel $\lambda$ vérifiant $u(X) = \lambda X$. On dit alors que $\lambda$ est une valeur propre de $u$ et $X$ un vecteur propre de $u$ associé à la valeur propre $\lambda$. \\
     Une famille de vecteurs propres associés à des valeurs propres distinctes est libre. \\
   \item 
     \begin{enumerate} 
     \item D'après la matrice, $u(X^k) = k X^{k-1} + (n+1 - (k+1)) X^{k+1} = k X^{k-1} + (n-k) X^{k+1}$ si $n-1 \geq k \geq 1$,  $u(1) = n X$ et $u(X^n) = n X^{n-1}$. \\
     \item On voit qu'en posant $e_k = X^k$, on a $u (e_k) = (1 - X^2) e_k' + n X e_k$. \\
       Comme c'est vrai sur une base de $\R_n [x]$ on a pour tout $P \in \R_n [X]$, $u(P) = (1 - X^2) P' + n X P$. \\
     \end{enumerate}
   \item Pour $k \in \llb 0 ; n \rrb$, on pose $P_k (X) = (X-1)^k (X+1)^{n-k}$.
     \begin{enumerate} \item $u(P_k) = (1-X^2) \left[ k (X-1)^{k-1} (X+1)^{n-k} + (n-k) (X-1)^k (X+1)^{n-k-1} \right] + n X (X-1)^k (X+1)^{n-k} = (X-1)^{k-1} (X+1)^{n-k-1} \left[ (1-X^2) [ k (X+1) + (n-k) (X-1) ] + n X  (X-1) (X+1) \right] \\ \\= (X-1)^{k-1} (X+1)^{n-k-1} \left[ (1-X^2) [ n X + 2k - n ] + n X  (X-1) (X+1) \right] \\ \\ = (X-1)^{k-1} (X+1)^{n-k-1} \left[ n X + 2k -n - n X^3 + (n - 2k) X^2 + n X^3 - n X \right] \\ \\ =(n - 2k) (X-1)^{k-1} (X+1)^{n-k-1} (X^2 - 1) = (n-2k)  (X-1)^{k} (X+1)^{n-k} = (n - 2k) P_k$. \\
     \item C'est une famille de vecteurs propres de $u$ associés à des valeurs propres distinctes donc elle est libre; de plus elle est de cardinal $n+1$ donc c'est une base. \\
     \item Il existe une base de vecteurs propres de $u$ donc $u$ est diagonalisable. \\
     \end{enumerate}
   \item Dans cette question, on suppose que $n = 3$.
     \begin{enumerate} \item $M = \begin{smatrix} 0 & 1 & 0 & 0 \\ 3 & 0 & 2 & 0 \\ 0 & 2 & 0 & 3 \\ 0 & 0 & 1 & 0 \\ \end{smatrix}$ et la base de vecteur propres est : \\
       $(X+1)^3 = X^3 + 3 X^2 + 3 X + 1$ associé à la valeur propre $3$, \\
       $(X-1)(X+1)^2 = X^3 +  X^2 - X - 1$ associé à la valeur propre $1$, \\
       $(X-1)^2 (X+1) = X^3 - X^2 - X + 1$ associé à la valeur propre $-1$, \\
       $(X-1)^3 = X^3 - 3 X^2 + 3 X - 1$ associé à la valeur propre $-3$, \\
       donc $P = \begin{smatrix} 1 & -1 & 1 & -1 \\ 3 & -1 & -1 & 3 \\ 3 & 1 & -1 & -3 \\ 1 & 1 & 1 & 1 \\ \end{smatrix}$ et $D = \begin{smatrix} 3 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & -3 \\ \end{smatrix}$.
     \item $ \begin{smatrix} a & b & c & d \\ e & f  & g & h \\ i & j & k & l \\ m & n & o & p \\ \end{smatrix} D = D  \begin{smatrix} a & b & c & d \\ e & f  & g & h \\ i & j & k & l \\ m & n & o & p \\ \end{smatrix} \Leftrightarrow  \begin{smatrix} 3a & b & -c & -3d \\ 3e & f  & -g & -3h \\ 3i & j & -k & -3l \\ 3m & n & -o & -3p \\ \end{smatrix} =  \begin{smatrix} 3a & 3b & 3c & 3d \\ e & f  & g & h \\- i & -j & -k & - l \\-3 m & -3n & -3o & -3p \\ \end{smatrix} \Leftrightarrow b =c=d=z=g=h=i=j=l=m=n=o=0$ donc les matrices commutant avec $D$ sont les matrices diagonales. \\
     \item On se place dans la base de vecteurs propres, on appelle $N$ la matrice de $v$. \\
       Alors $v \circ v = u \Leftrightarrow N^2 = D$. On a alors $N D = N N^2 = N^3 = N^2 N = D N$ donc $N$ commute avec $D$, et elle est diagonale. \\
       De plus $\begin{smatrix} a & 0 & 0 & 0 \\ 0 & b & 0 & 0 \\ 0 & 0 & c & 0 \\ 0 & 0 & 0 & d \\ \end{smatrix}^2 = \begin{smatrix} 3 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & -3 \\ \end{smatrix} \Leftrightarrow a^2 = 3,\ b^2 = 1,\ c^2 = -1$ et $d^2 = -3$ et les deux dernières équations sont impossibles donc il n'y a pas de solution. \\
     \end{enumerate} 
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soient $X$ et $Y$ deux variables aléatoires définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$ à valeurs dans $\N^*$, indépendantes et telles que : 
   \[
   \forall i \in \N^*,\ \Prob(\Ev{X=i}) = \Prob(\Ev{Y=i}) = \frac{1}{2^i}
   \] \begin{enumerate}
   \item On reconnaît la loi géométrique de paramètre $\frac{1}{2}$. \\
   \item $Z(\Omega) = \llb 2 ; +\infty \llb $ et avec la formule des probabilités totales (système complet d'évènements $\Ev{X=i}_{i \in \N^*}$ ) on a pour tout $k \geq 2$ : \\
     $\Prob(\Ev{ Z=k}) = \Sum{i=1}{+\infty} P ( [X=i] \cap [Z=k] ) = \Sum{i=1}{+\infty} P ( [X=i] \cap [X+Y=k] ) = \Sum{i=1}{+\infty} P ( [X=i] \cap [Y=k-i] ) = \Sum{i=1}{+\infty} P \Ev{ [X=i] } \Prob(\Ev{ [Y=k-i] }) = \Sum{i=1}{k-1} \frac{1}{2^i} \times \frac{1}{2^{k-i}} =\Sum{i=1}{k-1}  \frac{1}{2^{k} } = (k-1) \left( \frac{1}{2} \right)^k$. \\ \\
     D'autre part pour tout $i \geq k$ on a $P_{\Ev{X+Y = k}} \Ev{X = i} = 0$ et pour $1 < i < k-1$ on a : \\
     $P_{\Ev{X+Y = k}} \Ev{X = i} = \frac{ \Prob( [X+Y = k] \cap [X=i] ) }{P \Ev{X+Y = k}} = \frac{ \Prob( [ X=i] \cap [Y = k-i] ) }{(k-1) \left( \frac{1}{2} \right)^k} = \frac{1}{k-1}$. \\
   \item $\Prob(\Ev{ X = Y}) =  \Sum{i=1}{+\infty} \Prob( [X=i] \cap [Y=i] ) =  \Sum{i=1}{+\infty} \left( \frac{1}{4} \right)^{i} = \frac{1}{4} \times \frac{1}{ 1 -\frac{1}{4} } = \frac{1}{3}$. \\ \\
     Par symétrie $\Prob(\Ev{X< Y}) = P \Ev{X > Y}$ et $\Prob(\Ev{X<Y}) + \Prob(\Ev{X > Y}) + \Prob(\Ev{X=Y}) = 1$ (somme des probabilités sur un système complet) donc $\Prob(\Ev{X < Y}) = \Prob(\Ev{X > Y}) = \frac{1 - \frac{1}{3} }{2} = \frac{1}{3}$. \\
   \item $\Prob(\Ev{ X \geq 2 Y}) = \Sum{i=1}{+\infty} \Prob( [Y=i] \cap [X > 2i - 1] ) = \Sum{i=1}{+\infty} \frac{1}{2^i} \frac{1}{2^{2i - 1}} = \Sum{i=1}{+\infty}  \frac{1}{2^{3i -1} } = 2 \Sum{i=1}{+\infty} \left( \frac{1}{8} \right)^i = 2 \frac{1}{8} \frac{1 }{1 - \frac{1}{8} } = \frac{2}{7}$. \\ \\
     Enfin $P_{[X \geq Y]} \Ev{ X \geq 2 Y} = \frac{ P ([X \geq Y] \cap [X \geq 2 Y])}{\Prob(\Ev{ X \geq Y})} = \frac{ \Prob(\Ev{ X \geq 2 Y})}{P \Ev{X \geq Y}} = \frac{ \frac{2}{7} }{ \frac{2}{3} } = \frac{3}{7}$.

   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Soit $(X_n)_{n \in \N}$ une suite de variables aléatoires indépendantes définie sur un espace probabilisé $(\Omega , \mathcal{A} , P)$ telles que, pour tout $n \in \N^*$, $X_n$ suit la loi exponentielle de paramètre $\frac{1}{n}$ (d'espérance $n$). \\
   Pour tout $x$ réel on note $\lfloor x \rfloor$ sa partie entière. \\
   Pour $n \in \N^*$ soient :
   \[
   Y_n = \lfloor X_n \rfloor \ \ \ \text{ et } \ \ \ Z_n = X_n - \lfloor X_n \rfloor 
   \]
   \begin{enumerate}
   \item Une suite $(X_n)$ de variables aléatoires converge en loi vers $X$ si pour tout $x$ telle que $F_X$ est continue en $x$, $\dlim{n \rightarrow +\infty} F_{X_n} (x) = F_X(x)$. \\
   \item $Y_n (\Omega) = \lfloor \R_+ \rfloor = \N$. \\
     Pour tout $k \in \N$, $\Prob(\Ev{Y_n =k}) = \Prob( k \leq X_n < k+1) = F_{X_n} (k+1) - F_{X_n} (k) = 1 - e^{ - \frac{k+1}{n} } - 1 + e^{ - \frac{k}{n} } = \left( e^{- \frac{1}{n} } \right)^k ( 1 - e^{ - \frac{1}{n} })$. \\ \\
     On remarque que pour $k \in \N^*$, $\Prob(\Ev{ Y+1 = k}) = \Prob(\Ev{ Y = k-1}) = \left( e^{- \frac{1}{n} } \right)^{k-1} ( 1 - e^{ - \frac{1}{n} })$ et $Y+1$ suit la loi géométrique de paramètre $ 1 - e^{ - \frac{1}{n} }$, d'espérance $ \frac{1}{ 1 - e^{ - \frac{1}{n} }}$ et enfin $\E(Y) = \E(Y+1-1) = \E(Y+1) - 1 = \frac{1}{ 1 - e^{ - \frac{1}{n} }} - 1 = \frac{1 - ( 1 - e^{ - \frac{1}{n} }) }{ 1 - e^{ - \frac{1}{n} }} = \frac{  e^{ - \frac{1}{n} } }{ 1 - e^{ - \frac{1}{n} }}$. \\
   \item On $Y_n \leq X_n < Y_n + 1$ donc $0 \leq Z_n < 1$ et $Z_n (\Omega) = [ 0 ; 1[$. \\
     Avec le système complet $\Ev{Y_n =k}_{k \in \N}$ on a $\Prob(\Ev{Z_n \leq t}) = \Sum{k=0}{+\infty} P ( \Ev{Y_n = k} \cap \Ev{Z_n \leq t} ) =\Sum{k=0}{+\infty} P (k \leq X_n \leq k + t) = \Sum{k=0}{+\infty} F_{X_n} (k + t) - F_{X_n} (k) = \Sum{k=0}{+\infty} e^{-  \frac{k}{n} } - e^{ - \frac{ k+t}{n} } =(1 -  e^{- \frac{t}{n} }) \Sum{k=0}{+\infty} \left( e^{ - \frac{1}{n} } \right)^k = \frac{1 -  e^{- \frac{t}{n} } }{1 -  e^{- \frac{1}{n} }} $. \\
   \item Pour tout $t \in [ 0 ; 1]$, on obtient $F_{Z_n} (t) \sim \frac{ \frac{-t}{n} }{ \frac{-1}{n} } = t \xrightarrow[ n \rightarrow +\infty]{} t$. \\
     De plus on a pour $t \leq 0$, $F_{Z_n} (t) = 0 \xrightarrow[ n \rightarrow +\infty]{} 0$ et pour $t \geq 1$, $F_{Z_n} (t) = 1 \xrightarrow[ n \rightarrow +\infty]{} 1$; donc $(Z_n)$ converge en loi vers une variable aléatoire $Z$ suivant la loi uniforme sur $[ 0 ; 1]$. \\
   \item Soit $n \in \N^*$ et $N_n$ la variable aléatoire définie par :
     \[
     N_n = \Card \left\{ k \in \llb 1 ; n \rrb \text{ tel que } X_k \leq \frac{k}{n} \right\}
     \]
     où $\Card (A)$ désigne le nombre d'éléments de l'ensemble fini $A$.
     \begin{enumerate} 
     \item On a $N_n (\Omega) = \llb 0 ; n \rrb$ et $N_n$ compte le nombre de succès dans une succession de $n$ épreuves de Bernouilli indépendantes de même paramètre $P \left( X_n \leq \frac{k}{n} \right) = 1 - e^{ - \frac{ \frac{k}{n} }{k} } = 1 - e^{ - \frac{1}{n} }$ (qui est bien indépendant de $k$) donc $N_n \suit \mathcal{B} \left( n , 1 - e^{ - \frac{1}{n} } \right)$. \\
     \item Pour tout $i \in \N$, dès que $n \geq i$ on a : \\
       $\Prob(\Ev{ N_n = i}) = \binom{n}{i} \left( 1 - e^{- \frac{1}{n} } \right)^{i} \left( e^{ - \frac{1}{n} } \right)^{n-i} \sim  \binom{n}{i} \left( \frac{1}{n} \right)^{i} \left( e^{ - \frac{1}{n} } \right)^{n-i} = \frac{n}{n} \times \dots \times \frac{n-i+1}{n} \times \frac{1}{i!} \times e^{- \frac{n-i}{n} } \\ \Prob(\Ev{ N_n = i}) \sim \frac{e^{ -1}}{i!} = \frac{1^i e^{-1} }{i!}$ et on reconnaît une loi de Poisson de paramètre 1. \\
       D'où $(N_n)$ converge en loi vers une variable aléatoire $N$ qui suit la loi de Poisson de paramètre 1. \\
     \end{enumerate} 
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $E$ l'ensemble des matrices $M_{a,b} = \begin{smatrix} a & b & b \\ b & a & b \\ b & b & a \\ \end{smatrix}$ où $(a,b)$ prend toute valeur de $\R^2$. \begin{enumerate}
   \item Evident. \\
   \item Si $a=b$, $M_{a,b}$ a trois colonnes égales donc n'est pas inversible. \\
     Si $a = -2b$ on a $C_1 + C_2 + C_3 = 0$ donc $M$ n'est pas inversible. \\
     Sinon on a avec la méthode du pivot complet $M_{a,b}$ est inversible et : \\
     $M_{a,b}^{-1} = \frac{1}{(b-a) (a+2b) } \begin{smatrix} - (a+b) & b & b \\ b & -(a+b) & b \\ b & b & -(a+b) \\ \end{smatrix} \in E$. \\ 
   \item On peut essayer les premières puissances; on n'obtient rien de probant. \\
     Il faut alors diagonaliser; les valeurs propres peuvent être déduites de la deuxième question : \\
     En effet $M_{a,b} - \lambda I = M_{a -\lambda , b}$ n'est pas inversible si et seulement si $a - \lambda = b \Leftrightarrow \lambda = a-b$ ou $a - \lambda = -2b \Leftrightarrow \lambda = a + 2b$. \\
     Pour chacune de ces valeurs on cherche les sous-espaces propres : \\
     $(M_{a ,b} - (a-b) I)X = 0 \Leftrightarrow \begin{smatrix} b & b & b \\ b & b & b \\ b & b & b \\ \end{smatrix} \begin{smatrix} x \\ y \\ z \\ \end{smatrix} = 0 \Leftrightarrow b (x+y+z) = 0 \Leftrightarrow b= 0$ ou $x = -y-z$. \\ \\
     1er cas : $b = 0$, alors on a en fait $M_{a,b} = aI$ donc $M_{a,b}^n = a^n I$. \\
     2er cas : $b \neq 0$, on obtient alors un sous-espace propre de dimension 2, engendré par $[ ( -1 , 1 , 0) , (-1 , 0 , 1) ]$. \\ \\
     $(M_{a ,b} - (a+2b) I)X = 0 \Leftrightarrow \begin{smatrix} -2b & b & b \\ b & -2b & b \\ b & b & -2b \\ \end{smatrix} \begin{smatrix} x \\ y \\ z \\ \end{smatrix} = 0 \Leftrightarrow \left\{ \begin{array}{l} - 2 x + y + z = 0 \\ x - 2y + z = 0 \\ x + y -2z = 0 \\ \end{array} \right.$ (car $b \neq 0$) $\Leftrightarrow  \left\{ \begin{array}{l} x + y -2z = 0 \\  - 3y + 3z = 0 \\ -2x + y +z = 0 \\ \end{array} \right. \Leftrightarrow \left\{ \begin{array}{l} x  = y \\  y=z \\ 2y =2x \\  \end{array} \right. \Leftrightarrow (x,y,z) = x (1,1,1) $ ce qui donne une base du sous-espace propre. \\ \\
     On obtient avec $P = \begin{smatrix} -1 & -1 & 1 \\ 1 & 0 & 1 \\ 0 & 1 & 1 \\ \end{smatrix} $ et $D = \begin{smatrix} a-b & 0 & 0 \\ 0 & a-b & 0 \\ 0 & 0 & a+2b \\ \end{smatrix}$ que $M_{a,b} = P D P^{-1}$ puis $M_{a,b} = P D^n P^{-1}$ avec $D^n = \begin{smatrix} (a-b)^n & 0 & 0 \\ 0 & (a-b)^n & 0 \\ 0 & 0 & (a+2b)^n \\ \end{smatrix}$.

   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Un estimateur d'un paramètre $\theta$ de la loi $P_X$ d'une variable aléatoire $X$ dont on dispose d'un échantillon $(X_n)$ est une suite de variables aléatoires $(T_n)$ où pour tout $n$, $T_n$ est une fonction des variables $X_1 ,\ \dots\ , X_n)$. \\ \\
     Soient $a,\ b$ et $c$ trois réels strictement positifs et soit $f$ la fonction définie sur $\R$ par : 
     \[
     f(x) = 0 \text{ si } x < 0,\ \ \ f(x) = c \text{ si } x \in [ 0 ; a[,\ \ \ f(x) = \frac{b}{x^4} \text{ si } x \in [ a ; +\infty[.
     \]
   \item La fonction est continue sauf éventuellement en $0$ et en $a$ et positive, il reste à vérifier $\int_{-\infty}^{+\infty} f = 1$. \\
     Or $\int_{-\infty}^{+\infty} f = \int_{-\infty}^0 0\ dx + \int_0^a c\ dx +\int_a^{+\infty} \frac{b}{x^4}\ dx = a c +b \int_a^{+\infty} \frac{1}{x^4}\ dx$. \\
     Cette dernière intégrale est convergente (intégrale de Riemann) et $\int_a^y \frac{1}{x^4}\ dx = \left[ \frac{-1}{3x^3} \right]_a^y \xrightarrow[y \rightarrow +\infty]{} \frac{1}{3a^3}$. \\
     Il faut donc avoir $ a c + \frac{ b}{3 a^3} = 1$. \\
     De plus pour la continuité sur $\R_+$ il faut la continuité en $a$, qui donne $c = \frac{b}{a^4}$. \\
     On injecte dans la première égalité : $ \frac{b}{a^3} +  \frac{b}{3 a^3} = 1$ donc $\frac{4b }{3a^3} = 1$, $b = \frac{3a^3}{4}$ et $c = \frac{3}{4a}$. \\ \\
     On prend $a=1$, la courbe est constante égale à 0 jusqu'à $x=0$, constante égale à $\frac{3}{4}$ sur $[0;1[$ et décroissante et convexe de $\frac{3}{4}$ à 0 sur $[ 1 ; +\infty[$. \\
   \item Toutes les autres intégrales étant clairement absolument convergentes (intégrale de 0 ou intégrale sur un segment), il reste à vérifier que $ \frac{x^k}{x^4}$ est intégrable en $+\infty$, ce qui est vrai si et seulement si $4-k >1$, donc si et seulement si $k < 3$, c'est-à-dire $k \leq 2$. \\
   \item $\E(X) = c \frac{a^2}{2} + \frac{b}{2a^2} = \frac{3a}{8} + \frac{3a}{8} = \frac{3a}{4}$. \\
     De même $\E(X^2) = c \frac{a^3}{3} + \frac{b}{a} = \frac{a^2}{4} + \frac{3 a^2}{4} = a^2$. \\
     Enfin $\V(X) = a^2 - \frac{9}{16} a^2 = \frac{7}{16} a^2$. \\
   \item Soit $(X_n)$ une suite de variables aléatoires indépendantes de même loi que $X$. On pose 
     \[
     T_n = \frac{1}{n} \Sum{i=1}{n} X_i 
     \]
     \begin{enumerate} \item $(X_i)$ est un échantillon de la loi de $X$ dont $a$ est un paramètre et pour tout $n$, $T_n$ est une fonction de $X_1,\ \dots\ , X_n$ donc $(T_n)$ est un estimateur de $a$. \\
     \item On a $\E(T_n) = \E(X) = \frac{3}{4} a$ par linéarité de l'espérance donc en posant $S_n = \frac{4}{3} T_n$, la suite $(S_n)$ est un estimateur sans biais de $a$. \\
     \item $R_a (S_n) = \V(S_n)$ car $(S_n)$ est sans biais, donc $R_a (S_n) = \frac{16}{9} \V(T_n) = \frac{16}{9 n^2} V \left( \Sum{i=1}{n} X_i \right) = \frac{16}{9n^2} \Sum{i=1}{n} \V(X_i)$ par indépendance des $X_i$, en enfin : \\
       $R_a (S_n) = \frac{16}{9 n^2} \times n \V(X) = \frac{16}{9 n} \times \frac{7}{16} a^2 = \frac{7 a^2}{9n}$. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $A = \begin{smatrix} 1 & 2 & -2 \\ 2 & 1 & -2 \\ 2 & 2 & -3 \\ \end{smatrix}$. \begin{enumerate}
   \item $A^2 - I=0$.
   \item $X^2 - 1 = (X-1) (X+1)$ est annulateur de $A$ donc $\spc A \subset \{ -1 ; 1\}$. \\
     $(A - I) X = 0 \Leftrightarrow  X \in \Vect [ ( 1 , 1 , 1) ]$ et $(A+ I) X = 0 \Leftrightarrow X \in \Vect [ (1 , 0 , 1) , ( 1 , -1 , 0) ]$ donc la somme des dimensions des sous-espaces propres vaut 3, et $A$ est diagonalisable. \\
     De plus on a $A = P D P^{-1}$ avec $P = \begin{smatrix} 1 & 1 & 1 \\ 1 & 0 & -1 \\ 1 & 1 & 0 \\ \end{smatrix}$ et $D = \begin{smatrix} 1 & 0 & 0 \\0 & -1 & 0 \\ 0 & 0 & -1 \\ \end{smatrix}$.

   \end{noliste}
 \end{exercice}

 \newpage

 \section{\underline{Annales 2010}}

 %\setcounter{exercice}{0}

 \begin{exercice}  \indent \\
   \textbf{\underline{Exercice avec préparation}} \\
   \begin{enumerate}
   \item La loi géométrique est la loi d'attente du premier succès dans une succession illimitée d'épreuves de Bernouilli identiques et indépendantes, de paramètre $p$. \\
     Si $X \suit \mathcal{G} (p)$, on a $X(\Omega) = \N^*$ et pour tout $k \in \N^*$, $\Prob(\Ev{X = k}) = (1-p)^{k-1} p$. \\
     De plus $X$ admet une espérance et une variance, et $\E(X)=\frac{1}{p}$ et $\V(X) = \frac{1-p}{p^2}$. \\
   \item Pour tout $h \in \llb 1 ; n \rrb$, on définit la variable aléatoire $T_h$ égale au nombre d'années nécessaires pour que le $h$-ième bulbe fleurisse. \begin{enumerate} 
     \item $T_h \suit \mathcal{G} (p)$. \\
     \item $T =\max (T_1, T_2 ,\ \dots\ , T_n)$ donc $T(\Omega) = \N^*$ et pour tout $k \geq 1$ : \\
       $\Prob(\Ev{ T \leq k}) = P \left( \bigcap\limits_{h=1}^n [T_h \leq k] \right) = \prod\limits_{h=1}^n \Prob(\Ev{ T_h \leq k}) = \left( 1 - q^k \right)^n$, qui est valable aussi pour $k = 0$ (cela donne bien 0). \\
       On en déduit que $\Prob(\Ev{ T=k}) = \Prob(\Ev{ T \leq k}) - \Prob(\Ev{ T \leq k-1}) = \left( 1 - q^k \right)^n - \left( 1 - q^{k-1} \right)^n$. \\
     \end{enumerate}
   \item \begin{enumerate} 
     \item $\dlim{N \rightarrow +\infty} \Sum{k=1}{n} \binom{n}{k} (-1)^k N (q^k)^N = 0$ car c'est une somme finie de termes tendant vers 0 par croissances comparées de la suite $u_N = N$ et de la suite géométrique $v_N = (q^k)^N$. \\
     \item $ \Sum{k=1}{n} (-1)^k \binom{n}{k} \Sum{j=1}{N}   (q^k)^{j-1} = \Sum{k=1}{n} (-1)^k \binom{n}{k} \times \frac{1 - q^{k N} }{1 - q^k} \xrightarrow[ N \rightarrow + \infty]{} \Sum{k=1}{n} (-1)^k \binom{n}{k} \times \frac{1 }{1 - q^k}$.
       \\
     \item On étudie $ \Sum{j=1}{N} j \Prob(\Ev{ T = j}) =\Sum{j=1}{N} j \left[ \Sum{k=0}{n} \binom{n}{k} (-1)^k (q^{j})^k - \Sum{k=0}{n} \binom{n}{k} (-1)^k (q^{j-1})^k \right] \\ \\ = \Sum{k=0}{n} \binom{n}{k} (-1)^k \Sum{j=1}{N} j   (q^{k})^j - \Sum{k=0}{n} \binom{n}{k} (-1)^k \Sum{j=1}{N} j  (q^{k})^{j-1} \\ \\ = \binom{n}{0} (-1)^0 \Sum{j=1}{N} j   (1)^j + \Sum{k=1}{n} \binom{n}{k} (-1)^k \Sum{j=1}{N} j   (q^{k})^j - \binom{n}{0} (-1)^0 \Sum{j=1}{N} j  (1)^{j-1} - \Sum{k=1}{n} \binom{n}{k} (-1)^k \Sum{j=1}{N} j  (q^{k})^{j-1} \\ \\ =  \frac{N (N+1)}{2}  + \Sum{k=1}{n} \binom{n}{k} (-1)^k q^k \Sum{j=1}{N} j   (q^{k})^{j-1} - \frac{N (N+1)}{2}  - \Sum{k=1}{n} \binom{n}{k} (-1)^k \Sum{j=1}{N} j  (q^{k})^{j-1} \\ \\ \xrightarrow[ N \rightarrow +\infty]{}  \Sum{k=1}{n} \binom{n}{k} (-1)^k q^k \frac{1}{(1-q^k)^2} -   \Sum{k=1}{n} \binom{n}{k} (-1)^k \frac{1}{(1-q^k)^2} =  \Sum{k=1}{n} \binom{n}{k} (-1)^k \frac{q^k + 1   }{(1-q^k) (1 + q^k)} = \Sum{k=1}{n} \binom{n}{k} (-1)^k \frac{ 1 }{1-q^k} $  \\ \\ \\
       On retrouve le même résultat qu'à partir de la question b, mais je ne m'en suis pas du tout servi. On doit probablement pouvoir l'utiliser d'une manière ou d'une autre, peut-être en regroupant les deux sommes en $k$ tout de suite au lieu de le faire à la fin. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   $f$ est diagonalisable, on étudier la question dans une base $\mathcal{B}$ où la matrice de $f$ est diagonale, égale à $D = \begin{smatrix} a_1 & 0 &  \dots & 0 \\ 0 & a_2 &  & 0 \\ \vdots &  & \ddots & 0 \\ 0 & 0 & \dots & a_n \\ \end{smatrix}$. \\
   Pour avoir $\Im f \subset \ker f$ il faut avoir $f^2 = 0$, donc $D^2 = 0$, donc $a_i^2 = \Leftrightarrow a_i = 0$ pour tout $i$, et $D = 0$. \\
   La seule solution est donc l'endomorphisme nul, $f = 0_{\mathcal{L} (E)}$. \\ \\ \\
   Autre solution : $f^2 =0$ donc 0 est la seule valeur propre possible, et $f$ diagonalisable donc la matrice diagonale contenant 0 uniquement sur la diagonale, donc nulle, est une matrice de $f$, et $f=0$.
 \end{exercice}

 \newpage

 \begin{exercice}  \indent \\
   \textbf{\underline{Exercice avec préparation}} \\
   \begin{enumerate}
   \item $f = o(g)$ si et seulement si $ f = g \times \epsilon$ avec $\epsilon (x) \xrightarrow[x \rightarrow +\infty] 0$ puis on définit $f \sim g$ si et seulement si $f = g + o(g)$, ou $g = f + o(f)$, ou encore $f = g \times u$, avec $u(x) \xrightarrow[x \rightarrow +\infty]{} 1$. \\ \\
     Croissances comparées des fonctions usuelles : \\
     Pour tout $\alpha \in \R$, pour tout $\lambda > 0$, $x^{\alpha} = o (e^{\lambda x} ) $ donc $e^{-\lambda x} = o ( x^{\alpha})$ en $+\infty$. \\
     D'autre part pour tout $\alpha \in \R$, pour tout $\lambda > 0$, $x^{\alpha} = o (e^{-\lambda x} ) $ et $e^{\lambda x} = o ( x^{\alpha})$ en $-\infty$. \\
     Pour tous $\alpha >0$ et $\beta >0$, $( \ln x)^{\beta} = o ( x^{\alpha})$ en $+\infty$. \\
   \item Soit $g$ la fonction définie sur $\R_+^*$ à valeurs réelles, telle que :
     \[
     \forall x \in ] 0 ; +\infty[,\ \ \ g(x) = x \ln^2 (x). 
     \]
     \begin{enumerate} 
     \item $g$ est dérivable sur $]1 ; +\infty[$ et $g'(x) = \ln^2 (x) + x \times \frac{1}{x} \times 2 \ln (x) = \ln (x) ( \ln x + 2) >0$ sur $]  1 ; +\infty[$ donc $g$ est strictement croissante et continue donc réalise une bijection de $]1 ; +\infty[$ dans $]\dlim{1} g ; \dlim{+\infty} g [$. \\ \\
       Or par continuité de $g$, $\dlim{1} g = g(1) = 0$ et par produit de limites, $\dlim{+\infty} g = +\infty$, donc $g$ réalise une bijection de $] 1 ; +\infty [$ dans $] 0 ; +\infty[$. \\ \\
       Soit $h$ la bijection réciproque de la restriction de $g$ à l'intervalle $]1 ; +\infty[$. 
     \item \begin{enumerate}
       \item On écrit $g( h(x) ) = h(x) \ln^2 (h(x) ) = x$ et on compose par $\ln$ : \\
         On obtient $\ln (h(x) ) + 2 \ln ( \ln (h(x) ) ) = \ln (x)$. \\
       \item On a $\ln x = \ln (h(x) ) \left( 1 + 2 \frac{ \ln ( \ln (h (x) ) ) }{ \ln ( h(x) )} \right)$. \\
         On pose $X = \ln (h(x)) \xrightarrow[ x \rightarrow +\infty]{} +\infty$ par composée de limites, on a alors par croissances comparées, $ \frac{ \ln X }{X} \xrightarrow[X \rightarrow +\infty]{} 0$ donc par composition, $\dlim{x \rightarrow +\infty} 1 + 2 \frac{ \ln ( \ln (h (x) ) ) }{ \ln ( h(x) )} = 1$, et enfin $\ln x \sim \ln (h(x) )$ en $+\infty$. \\ \\
         ATTENTION : cela ne donne pas $h(x) \sim x$ !!!! \\ \\
         On écrit alors $[ \ln (h (x) ) ]^2 = \ln ( h(x) ) \times \ln ( h(x) ) \sim \ln (x) \times \ln (x) = \ln^2 (x)$ et on réutilise $h(x) \ln^2 (h(x)) = x \sim h(x) \ln^2 (x)$ donc $h(x) \sim \frac{x}{\ln^2 (x)}$. \\
       \end{enumerate}
     \end{enumerate}
   \item Soit $X$ une variable aléatoire de densité $f$ définie par :
     \[
     f(x) = \left\{ \begin{array}{ll} \frac{1}{2 g (\vert x \vert)} & \text{ si } \vert x \vert < \frac{1}{e} \text{ et } x \neq 0 \\ 0 & \text{sinon} \\ \end{array} \right. 
     \]
     \begin{enumerate}
     \item $f$ est continue sauf éventuellement en 0, $\frac{1}{e}$ et $\frac{-1}{e}$ et positive par positivité de $g$, il reste à prouver que $\int_{-\infty}^{+\infty} f $ est convergente et vaut 1. \\
       Par parité il suffit de prouver que $\int_0^{+\infty} f $ converge vers $\frac{1}{2}$. \\
       On a $\int_{\frac{1}{e} }^{+\infty} f$ converge et vaut 0 comme intégrale de la fonction nulle. \\
       Enfin on étudie $\int_x^{\frac{1}{e} } \frac{1}{2 t \ln^2 t}\ dt$ car l'intégrale n'est généralisée qu'en 0. \\
       $\int_x^{\frac{1}{e} } \frac{1}{2t \ln^2 t}\ dt = \left[ - \frac{1}{2\ln t} \right]_x^{\frac{1}{e} } = - \frac{1}{-2 \ln e } + \frac{1}{ 2 \ln x} \xrightarrow[ x \rightarrow 0]{} \frac{1}{2}$ donc l'intégrale converge et vaut $\frac{1}{2}$ et $f$ est bien une densité de probabilité. \\
     \item L'imparité puis la positivité permette de se restreindre à l'étude de la convergence de \\ $\int_0^{\frac{1}{e} } t \times \frac{1}{2 t \ln^2 t}\ dt = \int_0^{ \frac{1}{e} } \frac{dt}{2 \ln^2 t}$. \\
       Pour la convergence on peut écrire $ \frac{1}{2 \ln^2 t} = o \left( \frac{1}{t \ln^2 (t)} \right)$ et on a prouvé la convergence de l'intégrale de cette fonction : on utilise alors le théorème de comparaison des intégrales de fonctions positives. \\
       Ensuite l'imparité permet de donner $\E(X) = 0$ (sans réaliser le calcul d'intégrale, qui est impossible!) \\
     \item Sur le même principe on écrit $ \frac{t}{ 2 \ln^2 t} = o  \left( \frac{1}{t \ln^2 (t)} \right)$ pour prouver l'existence du moment d'ordre deux et donc de la variance. \\
       Par contre le calcul du moment d'ordre deux est celui de l'intégrale d'une fonction paire, et il faudrait réaliser le calcul de l'intégrale, qu'on ne sait pas faire. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   On note $\mathcal{B} = (e_1 , e_2 , e_3)$ la base canonique de $\R^3$. \\
   Soit $f$ l'endomorphisme de $\R^3$ dont la matrice dans la base $\mathcal{B}$ est $M = \begin{smatrix} 1 & -1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ \end{smatrix}$. \begin{enumerate}
   \item $f(e_1 + e_2 + e_3) = M \begin{smatrix} 1 \\ 1 \\ 1 \end{smatrix} = \begin{smatrix} 1 \\ 1 \\ 1 \end{smatrix}$, $f( e_2 ) = M \begin{smatrix} 0 \\ 1 \\ 0 \end{smatrix} = \begin{smatrix} 0 \\ 0 \\ 1 \end{smatrix}$ et $f(- e_1 + e_3) = M \begin{smatrix} -1 \\ 0 \\ 1 \end{smatrix} = \begin{smatrix} 0 \\ -1 \\ 0 \end{smatrix}$. \\
   \item On prouve que la famille au-dessus est une base, et on a $M' =Mat_{\mathcal{B'}} (f)$ est semblable à $M = Mat_{\mathcal{B} } (f)$. \\
   \item $M$ est diagonalisable si et seulement si $M'$ l'est. \\
     L'étude des valeurs propres de $M'$ donne $\spc M' = \{ 1 \}$; si $M'$ était diagonalisable, on aurait $M' = P I P^{-1} = I$ qui est absurde, donc $M'$ et $M$ ne sont pas diagonalisables.
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Soit $X$ une variable aléatoire définie sur un espace probabilisé $(\Omega , \mathcal{A} , P)$, qui suit la loi binomiale $\mathcal{B} (n , p)$, avec $n \geq 2$ et $0 < p < 1$. \\ \\
   On définit sur $(\Omega , \mathcal{A} , P)$ une variable aléatoire $Y$ de la façon suivante : \\
   \begin{itemize}
   \item pour tout $k \in \llb 1 ; n \rrb$, la réalisation de l'évènement $[X=k]$ entraîne celle de l'évènement $[Y=k]$;
   \item la loi conditionnelle de $Y$ sachant $[X=0]$ est la loi uniforme sur $\llb 1 ; n \rrb$.
   \end{itemize}
   \begin{enumerate}
   \item On réalise une succession de $n$ épreuves de Bernouilli indépendantes et identiques de paramètre $p$, et on compte le nombre de succès. \\
     La variable $X$ associée vérifie alors $X(\Omega) = \llb 0 ; b \rrb$. \\
     Pour calculer $\Prob(\Ev{X=k})$, on compte le nombre de possibilités amenant à ce résultat et la probabilité de chacune. \\
     Il faut obtenir $k$ succès et $n-k$ échecs : on place les $k$ succès parmi les $n$ épreuves pour obtenir toutes les possibilités : il y en a donc $\binom{n}{k}$. \\
     Dans chacun de ces cas, on obtient de manière indépendante $k$ succès et $n$ échecs avec une probabilité $p^k q^{n-k}$. \\
     On obtient alors $\Prob(\Ev{X=k}) = \binom{n}{k} p^k q^{n-k}$. \\
     L'espérance s'obtient en écrivant les $n$ $X_i$ variables de Bernouilli, avec la linéarité de l'espérance : $\E(X) = n p$. \\
     De même grâce à l'indépendance on calcule facilement la variance de la somme : $\V(X) = n p (1-p)$. \\
   \item On a $Y (\Omega ) = \llb 1 ; n \rrb$ et pour tout $k \in \N$, en utilisant le système complet $\Ev{X=i}_{0 \leq i \leq n}$ on a : \\
     $\Prob(\Ev{ Y = k}) = \Sum{i=0}{n} \Prob(\Ev{ X=i}) P_{\Ev{X=i} } \Ev{Y= k} = \Prob(\Ev{ X=k}) \times 1 + \Prob(\Ev{X= 0}) \times \frac{1}{n} = \binom{n}{k} p^k q^{n-k} + \frac{q^n}{n}$. \\
   \item $\E(Y) = \Sum{k=1}{n} k \binom{n}{k} p^k q^{n-k} + \Sum{k=1}{n} \frac{q^n}{n} =  \Sum{k=0}{n} k \binom{n}{k} p^k q^{n-k} - 0 + \frac{q^n}{n} \Sum{k=1}{n} 1 = \E(X) + \frac{q^n}{n} \times n = n p + q^n$. \\
   \item 
     \begin{enumerate} 
     \item Pour tout $k \in \llb 1 ; n \rrb$ on a $P_{X \neq 0} \Ev{ Y=k} = \frac{ \Prob( [Y=k] \cap [X \neq 0])}{\Prob( X \neq 0)} = \frac{P \Ev{X=k}}{1 - \Prob(\Ev{X=0}) } = \binom{n}{k} \frac{ p^k q^{n-k}}{1 - q^n}$. \\
     \item $\E(Y \slash X \neq 0) = \Sum{k=1}{n} k \binom{n}{k} \frac{ p^k q^{n-k}}{1 - q^n} = \frac{1}{1 - q^n} \Sum{k=1}{n} k \binom{n}{k} p^k q^{n-k} = \frac{1}{1 - q^n} \left(  \Sum{k=0}{n} k \binom{n}{k} p^k q^{n-k} - 0 \right) = \frac{\E(X)}{1 - q^n} = \frac{n p}{1 - q^n}$. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $A$ une matrice symétrique réelle d'ordre $n$ ($n \in \N^*$) et vérifiant $A^k = I_n$. \\
   Que peut-on dire dans les cas suivants : \begin{enumerate}
   \item $A$ est symétrique donc diagonalisable, et le polynôme $P(x) = x^k -1$ est annulateur de $A$. \\
     1 est une racine évidente de ce polynôme, étudions l'existence d'autres racines. \\
     $P'(x) = k x^{k-1} > 0$ sur $\R^*$ et nul en 0 car $k-1$ est pair, donc $P$ est strictement croissante et l'équation $P(x) = 1$ admet une unique solution, qui est donc $x=1$. \\
     On en déduit que $\spc A \subset \{ 1 \}$ et comme $A$ est diagonalisable elle admet au moins une valeur propre : d'où 1 est la seule valeur propre de $A$, il existe $P$ inversible tel que $A = P I P^{-1} = I$, donc $A$ est forcément la matrice identité. \\
   \item Ici l'étude de $P'(x)$ donne $P'(x) < 0$ sur $\R_-^*$, nul en 0 et strictement positif en 0. \\
     De plus $P(0) = -1 <0$ et $\dlim{+\infty} f =\dlim{-\infty} f = +\infty$ donc le théorème de la bijection donne exactement deux solutions, et celles-ci sont évidentes : 1 et $-1$. \\
     Il y a donc 3 possibilités : \\
     - $\spc A = \{ 1 \}$ qui donne $A = I$. \\
     - $\spc A = \{ -1 \}$ qui donne $A = -I$. \\
     - $\spc A = \{ -1 ; 1 \}$ qui donne $A = P D P^{-1}$ avec $D$ contenant 1 et $-1$ sur sa diagonale.
   \end{enumerate}
 \end{exercice}

 \newpage


 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Toutes les variables aléatoires de cet exercice sont définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$.
   \begin{enumerate}
   \item Soit $X$ une variable discrète finie telle que $ X(\Omega) = \{ x_i\ \big \vert\ 1 \leq i \leq n \}$. \\
     On pose alors $\E(X) = \Sum{i=1}{n} x_i \Prob(\Ev{X= x_i})$ qui est la valeur moyenne obtenue lorsqu'on réalise un grand nombre de fois l'expérience. \\
     $\V(X) = \Sum{i=1}{n} ( x_i - \E(X) )^2 \Prob(\Ev{ X= x_i})$ qui est la moyenne des écarts au carré, et qui permet de mesurer la dispersion des valeurs de $X$ (pondérées par leurs probabilités) autour de son espérance (qui est la valeur moyenne). \\
   \item Soient $a$ et $b$ deux réels tels que $a < b$. On considère une variable aléatoire $X$ (discrète ou possédant une densité) prenant toutes ses valeurs dans l'intervalle $[a ; b]$ et ayant un moment d'ordre 2.
     \begin{enumerate} 
     \item On a $\V(X) = E ( [X - \E(X)]^2 ) = E ( [ (X - \lambda ) + (\lambda - \E(X) )^2 ) \\ = E( [X - \lambda]^2 + 2 [\lambda - \E(X) ] [X - \lambda] + [ \lambda - \E(X) ]^2 ) \\ = E ( [X - \lambda]^2 ) + 2 (\lambda - \E(X) ) [ \E(X - \lambda) ] + [ \lambda - \E(X) ]^2   \\ = \E( [X - \lambda]^2) - [ \lambda - \E(X) ]^2 \leq \E( [X - \lambda]^2)$ car un carré est toujours positif. \\
     \item On se place en $\lambda = \frac{a + b}{2}$ la moyenne de $a$ et $b$ et on obtient : \\
       $ 0 \leq \left( X - \frac{a + b}{2} \right)^2 \leq \left(  \frac{ b-a}{2} \right)^2$ donc $0 \leq E \left[  \left( X - \frac{a + b}{2} \right)^2 \right]  \leq \left(  \frac{ b-a}{2} \right)^2$ et donc $\V(X) \leq E \left[  \left( X - \frac{a + b}{2} \right)^2 \right]  \leq  \frac{ (b-a)^2}{4} $. \\
     \end{enumerate}
   \item 
     \begin{enumerate} 
     \item On a alors $ \E(X) = \frac{1}{2} a + \frac{1}{2} b = \frac{a+b}{2}$ puis $\V(X) = \frac{1}{2} \left[ \left( a - \frac{a+b}{2} \right)^2 + \left( b - \frac{a+b}{2} \right)^2 \right] = \frac{1}{2} \times 2 \times \left( \frac{b-a}{2} \right)^2 = \frac{ (b-a)^2}{4}$. \\
     \item Etude d'une réciproque : on suppose que $\V(X) = \frac{(b-a)^2}{4}$. \\
       On a alors $\V(X)= E \left[  \left( X - \frac{a + b}{2} \right)^2 \right] =  \frac{ (b-a)^2}{4} $
       En reprenant la question 2.a., on voit que pour avoir $\V(X) = E \left[  \left( X - \frac{a + b}{2} \right)^2 \right] $, il faut avoir $\left( \frac{a+b}{2}  - \E(X) \right)^2 = 0$ donc $\E(X) = \frac{a+b}{2}$. \\
       Ensuite on reprend la question 2.b. : si il existe $c \in ] a ;b [$ tel que $\Prob(\Ev{ X = c}) \neq 0$ on a : \\
       $ E \left[  \left( X - \frac{a + b}{2} \right)^2 \right] = \left( c - \frac{a+b}{2} \right)^2 \Prob(\Ev{X= c}) + \sum\limits_{x \neq c} \left( x - \frac{a+b}{2} \right)^2 \Prob(\Ev{ X = x}) \leq  \left( c - \frac{a+b}{2} \right)^2 \Prob(\Ev{X= c})  + \frac{ (b-a)^2}{4}  \sum\limits_{x \neq c} \Prob(\Ev{ X = x}) = \left( c - \frac{a+b}{2} \right)^2 \Prob(\Ev{X= c})  + \frac{ (b-a)^2}{4} ( 1 - P \Ev{X = c} ) = \frac{ (b-a)^2}{4} + \Prob(\Ev{ X = c}) \left(  \left( c - \frac{a+b}{2} \right)^2 - \frac{ (b-a)^2}{4} \right) < \frac{ (b-a)^2}{4}$. \\ \\
       On en déduit que $X (\Omega) = \{ a ; b \}$, puis que $\E(X) = \frac{a+b}{2} = a \Prob(\Ev{ X = a}) + b ( 1 - \Prob(\Ev{ X= a}) )$ donc $\Prob(\Ev{X = a }) = \frac{ \frac{a + b}{2} - b }{a - b} = \frac{ \frac{ a-b}{2} }{a-b} = \frac{1}{2}$ et $\Prob(\Ev{ X = b}) = 1 - \Prob(\Ev{ X= a }) = \frac{1}{2}$. \\
     \end{enumerate}
   \item Cela signifie que la variance est maximale lorsque la dispersion est maximale et symétrique : il faut que les deux seules valeurs prises soient symétriques autour de l'espérance, et aient la même probabilité d'être atteintes. \\
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\  \begin{enumerate}
   \item $M$ est triangulaire donc elle est inversible si et seulement si $X $ et $Y$ sont non nuls. \\
     D'où $P ( M \text{ inversible } ) = P ( [ X \neq 0] \cap [ Y \neq 0] ) = 1$ car $ 0 \notin X(\Omega) = Y (\Omega)$. \\
   \item On a $\spc M = \{ X ; Y \}$ et comme $M$ n'est pas diagonale on a : si il y a une unique valeur propre, $M$ n'est pas diagonalisable et si il y a deux valeurs propres distinctes, elle l'est. \\
     D'où $\Prob( M \text{ diagonalisable } ) = P (X \neq Y) = 1 - P \Ev{ X = Y} = 1 - \Sum{k=1}{+\infty} p^2 (q^2)^{k-1} = 1 -  \frac{p^2}{1 - q^2} = 1 - \frac{p}{1+q} = \frac{1+q-p}{1+q} = \frac{2q}{1+q}$.

   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Soit $n$ un entier supérieur ou égal à 2, et $p$ et $q$ deux réels de $]0;1[$ tels que $p+q=1$. On considère deux variables aléatoires discrètes $X$ et $Y$ définies sur une espace probabilisé $(\Omega , \mathcal{A} , P)$. \\
   La loi du couple $(X , Y)$ est donnée par : \\
   pour tout $(j,k)$ tels que $0 \leq j \leq n$ et $1 \leq k \leq n$,
   \[
   \Prob( [X=j] \cap [Y=k] ) = \left\{ \begin{array}{cc} \binom{n}{k} p^k q^{n-k} & \text{ si } k=j,\ j \neq 0 \\ \\ \frac{q^n}{n} & \text{ si } j=0 \\ \\ 0 & \text{ si } k \neq j \text{ et } j \neq 0 \\ \end{array} \right.
   \]
   \begin{enumerate}
   \item La loi d'un couple $(X,Y)$ de variables aléatoires discrètes est la donnée de $(X,Y) (\Omega)$, ensemble des couples $(i,j)$ de valeurs telles que l'évènement $\Ev{X= i} \cap \Ev{Y = j}$ est possible. \\
     Les lois marginales sont les lois des variables $X$ et $Y$, obtenues à partir du couple et de la formule des probabilités totales. \\
     Les lois conditionnelles sont les lois d'une variable sachant que l'autre a donné un résultat précis, c'est-à-dire les lois du type $P_{\Ev{X=i_0}} \Ev{Y=j}$ ou $P_{\Ev{Y=j_0}} \Ev{X=i}$. \\
   \item 
     \begin{enumerate} 
     \item On a $X(\Omega) = \llb 0 ; n \rrb$ et pour tout $j \in \llb 1 ; n \rrb$, avec le système complet $\Ev{Y = k}_{1 \leq k \leq n}$ on a : \\
       $\Prob(\Ev{X = j}) = \binom{n}{j} p^j q^{n-j}$. \\
       Enfin pour $j=0$ on a $\Prob(\Ev{X = 0}) = \Sum{k=1}{n} \frac{q^n}{n} = q^n$. \\
       Finalement on voit que $X \suit \mathcal{B} (n , p)$. \\ \\
       D'autre part on a $Y (\Omega) = \llb 1 ; n \rrb$ et avec  le système complet $\Ev{X = j}_{0 \leq k \leq n}$ on a : \\
       $\Prob(\Ev{Y = k}) = \binom{n}{k} p^k q^{n-k} + \frac{q^n}{n}$. \\
     \item $\E(Y) = \Sum{k=1}{n} k \binom{n}{k} p^k q^{n-k} + \Sum{k=1}{n} \frac{q^n}{n} =  \Sum{k=0}{n} k \binom{n}{k} p^k q^{n-k} - 0 + \frac{q^n}{n} \Sum{k=1}{n} 1 = \E(X) + \frac{q^n}{n} \times n = n p + q^n$. \\
     \end{enumerate}
   \item Soit $j$ un entier tel que $0 \leq j \leq n$.
     \begin{enumerate} 
     \item Pour tout $ j \geq 1$, pour tout $1 \leq k \leq n$ on a $P_{\Ev{X=j}} \Ev{Y = k} = \frac{ P ( [ X=j] \cap [Y=k] )}{P \Ev{X=j}} = 0$ si $j \neq k$ et $1$ si $j = k$. \\
       Pour $j = 0$, on a pour tout $1 \leq k \leq n$, $P_{\Ev{X=0}} \Ev{Y= k} = \frac{ \frac{q^n}{n} }{ q^n} = \frac{1}{n}$. \\
     \item Si $j > 0$, on a $\E( Y \slash X=j) = j$ (c'est une variable certaine). \\
       Pour $j=0$, on a $\E( Y \slash X = 0) = \Sum{k=1}{n} k \times \frac{1}{n} = \frac{1}{n} \times \frac{n (n+1)}{2} = \frac{n+1}{2}$. \\
     \end{enumerate}
   \item
     \begin{enumerate} 
     \item $\Prob( [X = 1] \cap [Y=1] ) = n p q^{n-1}$ et $\Prob(\Ev{X=1}) \Prob(\Ev{Y=1}) = n p q^{n-1} \times \left( n p q^{n-1}  + \frac{q^n}{n} \right)$. \\
       Il faut donc prouver que $ \left( n p q^{n-1}  + \frac{q^n}{n} \right) = n q^{n-1} - n q^n + \frac{q^n}{n} \neq 1$. \\
       On étudie la fonction $f(q) = n q^{n-1} - n q^n + \frac{q^n}{n}$ qui est dérivable : \\
       $f'(q) = n (n-1) q^{n-2} - n^2 q^{n-1} + q^{n-1} = q^{n-2} \big[ n (n-1) - (n^2 - 1) q \big] = q^{n-2} \big[ n ( n-1) - (n-1) (n+1) q \big] = (n-1) q^{n-2} ( n - (n+1) q )$ qui s'annule en $q = \frac{n}{n+1}$ en étant positive avant et négative après; en rajoutant $f(0) = 0$ et $f(1) = \frac{1}{n}$ et $ f \left( \frac{n}{n+1} \right) = \frac{n^n}{(n+1)^n} + \frac{n^{n-1} }{(n+1)^n} = \frac{ n^n + n^{n-1} }{(n+1)^n} $ . \\ \\
       De plus on a $(n+1)^n = \Sum{k=0}{n} \binom{n}{k} n^k \geq n^n + n n^{n-1} > n^n + n^{n-1}$ et on obtient que $f(q) < 1$ sur $]0 ; 1[$. \\
       On ne déduit que les variables $X$ et $Y$ ne sont pas indépendantes. \\
     \item On utilise le théorème de transfert : \\
       $E (XY) = \Sum{j=0}{n} \Sum{k=1}{n} j k \Prob([ X=j] \cap [Y=k] ) = \Sum{j=1}{n} j^2 \Prob([ X=j] \cap [Y=j] ) =  \Sum{j=1}{n} j^2 \binom{n}{j} p^j q^{n-j} = \Sum{j=0}{n} j^2 \binom{n}{j} p^j q^{n-j} = \E(X^2) = \V(X) + [\E(X)]^2 = n p (1-p) + n^2 p^2 =n p ( 1 - p + np) = n p  [(n-1) p + 1 ]$. \\
       Enfin on a $\Cov (X,Y) = \E(XY) - \E(X) \E(Y) = n p  [(n-1) p + 1 ] - n p ( n p + q^n) = np [ (n-1) p + 1 - np - q^n] = n p [ 1 - p - q^n] = n p q ( 1 - q^{n-1} ) \neq 0$ sauf si $q^{n-1} = 1$ qui donne $q = 1$. \\
     \item On retrouve que $X$ et $Y$ ne sont pas indépendantes, quelle que soit la valeur de $q$. \\
     \end{enumerate} 
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Etudier la convergence de la suite $(u_n)_{n \in \N^* }$ définie par :
   \[
   \forall n \in \N^*,\ \ \ u_n = \frac{1}{n^{\alpha}} \Sum{k=1}{n} k \ln \left( 1 + \frac{k}{n} \right)
   \]
   où $\alpha$ est un nombre réel, \begin{enumerate}
   \item Si $\alpha = 2$ on a $u_n = \frac{1}{n} \Sum{k=1}{n} \frac{k}{n} \ln \left( 1 + \frac{k}{n} \right) \xrightarrow[ n \rightarrow +\infty]{} \int_0^1 x \ln ( 1 + x )\ dx = \int_1^2 (x-1) \ln x\ dx = \frac{ \ln 2 }{2} + \frac{1}{4} $ avec une intégration par parties (somme de Riemann). \\
   \item Si $\alpha < 2$ on a $u_n =  \frac{1}{n} \Sum{k=1}{n} \frac{k}{n} \ln \left( 1 + \frac{k}{n} \right) \times n^{2- \alpha} \xrightarrow[ n \rightarrow +\infty]{} +\infty$. \\
     Si $\alpha > 2$ on a $u_n =  \frac{1}{n} \Sum{k=1}{n} \frac{k}{n} \ln \left( 1 + \frac{k}{n} \right) \times n^{2- \alpha} \xrightarrow[ n \rightarrow +\infty]{} 0$.
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Soit $X$ une variable à densité, de densité $f$. On dit que
     $X$ admet un moment d'ordre $r$ si $X^r$ admet une espérance et
     on pose $\E(X^r)$ le moment d'ordre $r$ de $X$. \\ 
     Avec le théorème de transfert, $X$ admet un moment d'ordre $r$ si
     et seulement si l'intégrale $\int_{-\infty}^{+\infty} t^r f(t)\
     dt $ converge absolument (équivalent à la convergence grâce à la
     décomposition en une fonction de signe constant sur $[ 0 ;
     +\infty[$ et sur $] - \infty ; 0]$). \\ 
     Enfin si $X$ admet un moment d'ordre $r$, elle admet un moment
     d'ordre $k$ pour tout $1 \leq k \leq r$. \\ 
   \item On met au même dénominateur, on identifie et on obtient
     $\frac{1}{x (x+1)} = \frac{ 1}{x} - \frac{1}{x+1}$. 
   \item On pose :
     \[
     f(x) = \left\{ \begin{array}{cc} \frac{k}{x(x+1)} & \text{ si } x \geq 1 \\ 0 & \text{ sinon } \\ \end{array} \right.
     \]
     où $k$ est un paramètre réel. 
     \begin{enumerate}
     \item $f$ est positive et continue sauf en 1, vérifions $\int_{-\infty}^{+\infty} f = \int_1^{+\infty}\left( \frac{k}{x} - \frac{k}{x+1} \right)\ dx = 1$. \\
       On a $\int_1^t \left( \frac{k}{x} - \frac{k}{x+1} \right)\ dx = k \left[ \ln x - \ln (x+1) \right]_1^t = k \ln \left( \frac{t}{t+1} \right) + k \ln 2 \xrightarrow[ t \rightarrow +\infty]{} k \ln 2$ donc l'intégrale converge et vaut $k \ln 2$. \\
       D'où $f$ est une densité de probabilité si et seulement si $k = \frac{1}{\ln 2}$. \\
     \item L'intégrale $\int_1^{+\infty} x f(x)\ dx = \int_1^{+\infty} \frac{1}{(x+1) \ln 2}\ dx$ diverge comme intégrale de Riemann avec $\alpha =1$ donc $X$ n'admet pas d'espérance. \\
     \end{enumerate}
   \item \begin{enumerate} 
     \item On a $T (\Omega) = \N^*$ puis $\Prob(\Ev{T=k}) = \Prob( k \leq X < k+1) = \int_k^{k+1} f(t)\ dt = \left[ \ln \left( \frac{x}{x+1} \right) \right]_k^{k+1} = \ln \left( \frac{ (k+1)^2}{k (k+2)} \right) =  \ln \left( \frac{ k^2 + 2k + 1}{k (k+2)} \right) = \ln \left( \frac{ k( k+ 2) + 1}{k (k+2)} \right) =\ln \left( 1 + \frac{ 1}{k (k+2)} \right)  $. \\
     \item On en déduit que $\Sum{n=1}{+\infty} \ln \left( 1 + \frac{1}{n(n+2) } \right) = 1$ (c'est la somme des probabilités sur un système complet d'évènements). \\ 
     \end{enumerate}
   \item $Z(\Omega) = ] 0 ; 1]$ et on a pour tout $x \leq 0$, $F_Z(x) = 0$;\\
     Pour $0 < x \leq 1$, $F_Z(x) = \Prob( 0 <Z \leq x) = P \left( X \geq \frac{1}{x} \right) = 1 - F_X \left( \frac{1}{x} \right) = \frac{1}{\ln 2} \ln \left( \frac{ \frac{1}{x} + 1 }{ \frac{1}{x} } \right) = \frac{1}{\ln 2} \ln ( 1+x  )$; \\
     Enfin pour $x \geq 1$, $F_Z(x) = 1$. \\
     Enfin on obtient $f_Z(x) = 0$ si $x \leq 0$ ou $x \geq 1$ et $f_Z(x) = \frac{1}{(1+x) \ln 2}$ si $x \in ]0 ; 1[$. \\
   \item \begin{enumerate}
     \item On a $Y (\Omega) = [ 0 ; 1[$ donc $F_Y(x) = 0$ si $x \leq 0$ et 1 si $x \geq 1$. \\
       Enfin si $0 < x < 1$ on a $F_Y(x) = \Sum{k=1}{+\infty} \Prob( k \leq X \leq k + x) = \frac{1}{\ln 2}  \Sum{k=1}{+\infty} \ln \left( \frac{ (k+x) (k+1)}{k (k+x + 1)} \right) = \frac{1}{\ln 2}  \Sum{k=1}{+\infty} \ln \left( \frac{ (k+x)}{k } - \frac{k+x+1}{k+1} \right) = \dlim{n \rightarrow +\infty} \frac{1}{\ln 2}  \ln( 1+x) - \frac{1}{\ln 2}  \ln \left( \frac{ n+x+1}{n+1} \right)  = \frac{1}{\ln 2}  \ln (1+x) $ (télescopage). \\
       On reconnaît la même loi que $Z$. \\
     \item Les deux autres intégrales étant trivialement convergentes vers 0, il faut prouver que $\int_0^1  \frac{ x^r }{(1+x)\ln 2}$ converge, ce qui ne pose aucun problème puisque la fonction est continue sur $[ 0 ; 1]$ et l'intégrale n'estg pas généralisée. \\
     \item $\E(Y) = \frac{1}{\ln 2} \int_0^1 \frac{x}{(1+x)}\ dx = \frac{1}{\ln 2} \int_1^2 \frac{x-1}{(x)}\ dx = \frac{1}{\ln 2} \int_1^2 dx -  \frac{1}{\ln 2} \int_1^2 \frac{1}{x}\ dx =  \frac{1}{\ln 2} -  \frac{1}{\ln 2} \ln 2 =  \frac{1}{\ln 2} - 1$. \\
     \end{enumerate} 
   \end{noliste}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $n \geq 2$ et $A = \begin{smatrix} 0 & 1 & \dots & 1 \\ 1 & 0 & \dots & 1 \\ \vdots & & \ddots & \vdots \\ 1 & 1 & \dots & 0 \\ \end{smatrix} \in \mathcal{M}_n (\R)$. \\ \\
   Avec des pivots habiles et des calculs très rigoureux, on trouve $A^{-1} = \frac{1}{n-1} ( A + (2-n) I)$. \\
   Méthode beaucoup plus simple : chercher un polynôme annulateur en calculant $A^2$ : \\
   On trouve $A^2 = (n-2) A + (n-1) I$ donc $A ( A + (2-n) I ) = (n-1) I$ et enfin $A^{-1} = \frac{1}{n-1} ( A + (2-n) I )$.
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Un estimateur d'un paramètre $\theta$ de la loi $P_X$ d'une
     variable aléatoire $X$ dont on dispose d'un échantillon $(X_n)$
     est une suite de variables aléatoires $(T_n)$ où pour tout $n$,
     $T_n$ est une fonction des variables $X_1 ,\ \dots\ , X_n)$. On
     dit qu'il est sans biais si pour tout $n$, $\E(T_n) = \theta$. \\
     \\ 
     Soit $Z$ une variable aléatoire discrète d'espérance $\E(Z) =
     \theta$ ($\theta \in \R^*$) et de variance $\V(Z) = 1$. \\ \\ 
     Pour $n$ entier de $\N^*$, on dispose d'un $n$-échantillon $(Z_1
     , Z_2 ,\ \dots\ , Z_n)$ de variables aléatoires indépendantes et
     de même loi que $Z$, définies sur un espace probabilisé $(\Omega
     , \mathcal{A} , P)$. \\ \\ 
     On pose $\overline{Z_n} = \frac{1}{n} \Sum{j=1}{n} Z_j$. On
     suppose que $\theta$ est inconnu.
   \item 
     \begin{enumerate} 
     \item Oui. Le calcul est évident. \\
     \item $R_{\theta} = \V(\overline{Z_n}) =\frac{1}{n^2} \times n
       \V(Z) = \frac{1}{n}$. \\ 
     \end{enumerate}

   \item Soient $\beta_1, \beta_2,\ \dots\ , \beta_n$ des réels non
     nuls et $Y_n = \Sum{j=1}{n} \beta_j Z_j$. 
     \begin{enumerate}
     \item Il faut $\Sum{j=1}{n} \beta_j = 1$. \\ \\
       On suppose que cette condition est vérifiée. \\
     \item On a vu $\V( \overline{Z_n} ) = \frac{1}{n}$. \\
       On a $\V( \overline{Z_n} + Y_n ) = V \left( \Sum{j=1}{n} \left(
           \beta_j - \frac{1}{n} \right) Z_j \right) = \Sum{j=1}{n}
       \left( \beta_j - \frac{1}{n} \right)^2 \V(Z_j) =\Sum{j=1}{n}
       \left( \beta_j - \frac{1}{n} \right)^2$. \\  
       On en déduit que $\V(\overline{Z_n} ) + \V(Y_n) + 2 \Cov (
       \overline{Z_n} , Y_n) = \Sum{j=1}{n}  \left( \beta_j -
         \frac{1}{n} \right)^2$. \\ 
       D'où $\Cov ( \overline{Z_n} , Y_n) = \frac{1}{2} \left(
         \Sum{j=1}{n}  \left( \beta_j - \frac{1}{n} \right)^2 -
         \beta_j^2 - \frac{1}{n^2} \right) =   \Sum{j=1}{n}
       \frac{\beta_j}{n} = \frac{ \Sum{j=1}{n} \beta_j }{n} =
       \frac{1}{n}$. \\ 
       On a alors $0 \leq \V(Y_n - \overline{Z_n} ) = \V(Y_n) +
       V(\overline{Z_n} - 2 \Cov ( U_n , \overline{Z_n} ) = \V(Y_n) +
       \frac{1}{n} - \frac{2}{n} = \V(Y_n) - \frac{1}{n} = \V(Y_n) - \V(
       \overline{Z_n} )$ donc $\V(Y_n) \geq \V( \overline{Z_n})$. \\ 
       Interprétation :  
     \end{enumerate}
   \item Soient $\alpha_1, \alpha_2,\ \dots\ , \alpha_n$ des réels non
     nuls. \\ 
     On définit la variable aléatoire $U_n$ par : $U_n = \Sum{j=1}{n}
     \alpha_j Z_j$, \\ 
     et on suppose que $\E(U_n) = \theta$ et $\V(U_n) = \frac{1}{n}$. \\
     On a alors $\V( U_n - \overline{Z_n} ) = \V( U_n) + \V(
     \overline{Z_n}) - 2 \Cov (U_n , \overline{Z_n}) = \frac{2}{n} -
     \frac{2}{n} = 0$. \\ 
     On en déduit que la variable $U_n - \overline{Z_n}$ est
     quasi-certaine : si elle pouvait prendre deux valeurs distinctes
     avec une probabilité non nulle, l'une des deux au moins serait
     différente de son espérance et la variance serait donc
     strictement positive. \\ 
     On en déduit qu'il existe $a$ tel que $\Prob( U_n - \overline{Z_n} =
     a ) = 1$. \\ 
     On a alors $E ( U_n - \overline{Z_n} ) = a$; or on sait que
     $\E(U_n - \overline{Z_n} ) = \theta - \theta = 0$, donc $a = 0$ et
     $U_n = \overline{Z_n}$ avec une probabilité égale à 1 (presque
     sûrement). 
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ 
   Soit $f$ la fonction définie sur $\R_+^* \times \R_+^*$, à valeurs
   réelles, par :  
   \[
   f(x,y) = \frac{x^2 + xy + \sqrt{y} }{x \sqrt{y} }. 
   \]
   \begin{enumerate}
   \item On a des produits, sommes et quotient dont le dénominateur ne
     s'annule pas des fonctions $(x,y) \rightarrow x$, $(,y)
     \rightarrow y$ et $(x,y) \rightarrow \sqrt{y}$ qui sont de classe
     $C^2$ sur $\R_+^* \times \R_+^*$. 
   \item $f_x'(x,y) = \frac{(2x + y ) x \sqrt{y} - \sqrt{y} (x^2 + xy
       + \sqrt{y} ) }{x^2 y} = \frac{x^2  -  \sqrt{y}  }{x^2
       \sqrt{y}}$ après simplifications. \\ 
     $f_y'(x,y) = \frac{\left(x + \frac{1}{2 \sqrt{y} } \right) x
       \sqrt{y} - \frac{x}{2 \sqrt{y} } ( x^2 + xy + \sqrt{y} ) }{x^2
       y} =  \frac{ y - x }{2 y \sqrt{y}}$ après simplifications. \\ 
     On en déduit que $f_x'(x,y) = f_y'(x,y)$ si et seulement si $x =
     y$ et $x^2 = \sqrt{y}$, qui donne $x^2 = \sqrt{x}$, $x^4 = x$,
     $x^3 =1$ car $x \neq 0$, et enfin $x=y=1$. \\  
   \item On a $f_{x,x}'' (x,y) = \frac{ 2x^3 \sqrt{y} - 2 x \sqrt{y}
       (x^2 - \sqrt{y}) }{ x^4 y} = \frac{ 2 }{ x^3}$ donc $r =
     2$. \\
     $f_{x,y}'' (x,y) = -\frac{ 1}{2 y \sqrt{y} }$ donc $s = - \frac{1}{2}$. \\
     $f_{y,y}'' = \frac{ 2 y \sqrt{y} - (y-x) 3 \sqrt{y} }{4 y^3} =
     \frac{3x - y}{4 y^2 \sqrt{y}} $ donc $t = \frac{2}{4} =
     \frac{1}{2}$. \\
     On a alors $rt - s^2 = 1 - \frac{1}{4} = \frac{3}{4} > 0$ donc
     $f$ admet un extremum local en $(1,1)$, et $r = 2 > 0$ donc c'est
     un minimum local.
   \end{noliste}
 \end{exercice}

 \newpage

 \section{\underline{Annales 2009}}

 %\setcounter{exercice}{0}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Soit $(A_i)_{i \in I}$ un système complet d'évènements, c'est-à-dire vérifiant que $A_i \cap A_j = \emptyset$ si $i \neq j$, $ \bigcup\limits_{i \in I} A_i = \Omega$ et $\Prob(A_i) \neq 0$ pour tout $i$. \\
     Alors pour tout évènement $E$, on a $\Prob(E) = \sum\limits_{i \in I} \Prob( E \cap A_i) = \sum\limits_{i \in I} \Prob(A_i) P_{A_i} (E)$. \\
   \item $p_1 = q_1 = r_1 = \frac{1}{3}$ (on choisit au hasard). \\
   \item $p_n + q_n + r_n = 1$ car les évènements associés forment un système complet d'évènements. \\
   \item On a $p_{n+1} = \frac{1}{3} p_n + \frac{1}{12} r_n$. \\
     $q_{n+1} =\frac{1}{3} p_n + q_n + \frac{7}{12} r_n$. \\
     $r_{n+1} = \frac{1}{3} p_n + \frac{1}{3} r_n$. \\
   \item $p_n = \frac{1}{3} p_{n-1} + \frac{ 1}{12} r_{n-1} = \left( \frac{1}{3} p_{n-1} + \frac{1}{3} r_{n-1} \right) - \frac{1}{3} r_{n-1} + \frac{1}{12} r_{n-1} = r_n - \frac{1}{4} r_{n-1}$. \\
   \item $r_{n+1} = \frac{1}{3} p_n + \frac{1}{3} r_n = \frac{1}{3} r_n - \frac{1}{12} r_{n-1} + \frac{1}{3} r_n = \frac{2}{3} r_n - \frac{1}{12} r_{n-1}$ et en multipliant par 12 on a : $12 r_{n+1} - 8 r_n + r_{n-1} = 0$, qui est bien une relation de récurrence linéaire double. \\ \\
     On étudie l'équation caractéristique : $12 r^2 - 8 r + 1 = 0$ a pour discriminant $\Delta = 64 - 48 = 16$ donc les racines sont $r_1 = \frac{8 -4}{24} =  \frac{1}{6}$ et $r_2 = \frac{8+4}{24} = \frac{1}{2}$. \\ \\
     On obtient $r_n = a \left( \frac{1}{6} \right)^n + b \left( \frac{1}{2} \right)^n$, puis on étudie les premières valeurs : \\
     $r_1 = \frac{1}{3}$ donc $\frac{1}{6} a + \frac{1}{2} b = \frac{1}{3}$, ou encore $\frac{a + 3b - 2}{6} = 0$. \\
     $r_2 = \frac{1}{3} r_n + \frac{1}{3} p_1 = \frac{2}{9}$ donc $ \frac{1}{36} a + \frac{1}{4} b = \frac{2}{9}$, ou encore $\frac{ a + 9 b - 8}{36} = 0$. \\
     D'où $a = 2 - 3b$, puis $2-3b + 9 b - 8 = 0$, $b = 1$ et $a = -1$. \\
     Enfin $r_n = - \left( \frac{1}{6} \right)^n +  \left( \frac{1}{2} \right)^n$. \\ \\
     Ensuite on a $p_n = r_n - \frac{1}{4} r_{n-1} = - \left( \frac{1}{6} \right)^n +  \left( \frac{1}{2} \right)^n +\frac{1}{4} \left( \frac{1}{6} \right)^{n-1} - \frac{1}{4}  \left( \frac{1}{2} \right)^{n-1} = \left( \frac{1}{6} \right)^n \left[ - 1 + \frac{6}{4} \right] +  \left( \frac{1}{2} \right)^n \left[ 1 - \frac{2}{4} \right] = \frac{1}{2} \left[  \left( \frac{1}{6} \right)^n +  \left( \frac{1}{2} \right)^n \right]$. \\ \\
     Enfin on a $q_n = 1 - r_n - p_n = 1 + \frac{1}{2}  \left( \frac{1}{6} \right)^n  - \frac{3}{2}  \left( \frac{1}{2} \right)^n$. \\
   \item On a $p_n \rightarrow 0$, $r_n \rightarrow 0$ et $q_n \rightarrow 1$. \\
   \end{noliste}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Une matrice symétrique vérifie $M = {}^t M$ donc $(I , M , {}^t M)$ sont bien liées. \\
   Si une telle matrice est diagonalisable : \\
   1er cas : la relation de dépendance ne concerne pas la transposée, alors $M = \lambda I = {}^t M$ est bien diagonalisable. \\
   2e cas : on a ${}^t M  = \lambda I$, alors ${}^t M = M = \lambda I$ est bien diagonalisable. \\
   3e cas : On a ${}^t M = \alpha M$, alors on a $m_{i,j} = \alpha m_{j,i} = \alpha^2 m_{i,j}$ donc $\alpha^2 = 1$, et $M$ est symétrique (donc diagonalisable) ou antisymétrique (et on ne sait rien dire). \\
   4e cas : on a $M = \alpha I + \beta {}^t M$. \\
   $M$ est diagonalisable et s'écrit $P D P^{-1}$, et $I = P I P^{-1}$ donc on obtient $\beta {}^t M = P ( D - \alpha I) P^{-1}$ et enfin ${}^t M = P \left( \frac{1}{\beta} ( D - \alpha I) \right) P^{-1} = P D' P^{-1}$ est diagonalisable dans la même base que $M$ car $D'$ est diagonale. \\
   On pourrait encore dire bien des choses avec la théorie des matrices orthogonales, mais comme elle est hors programme...
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item La loi géométrique est la loi d'attente du premier succès dans une succession illimitée d'épreuves de Bernouilli identiques et indépendantes, de paramètre $p$. \\
     Si $X \suit \mathcal{G} (p)$, on a $X(\Omega) = \N^*$ et pour tout $k \in \N^*$, $\Prob(\Ev{X = k}) = (1-p)^{k-1} p$. \\
     De plus $X$ admet une espérance et une variance, et $\E(X)=\frac{1}{p}$ et $\V(X) = \frac{1-p}{p^2}$. \\
   \item Soit $x$ un réel de $]0 ; 1[$.
     \begin{enumerate} 
     \item La somme des termes d'une suite géométrique donne $\Sum{k=1}{n} x^{k-1} = \frac{1 - t^n}{1-t}$. \\
       On intègre sur $[ 0 ; x]$ ces fonctions continues : \\
       $\int_0^x \frac{1-t^n}{1-t}\ dt = \Sum{k=1}{n} \int_0^x t^{k-1}\ dt = \Sum{k=1}{n}\left[ \frac{t^{k}}{k} \right]_0^x = \Sum{k=1}{n}\frac{x^{k}}{k}$. \\
     \item On a $ 0  \leq 1-x < 1-t \leq 1$ sur $[ 0 ; 1[$, donc $0 \leq \frac{1}{1-t} \leq \frac{1}{1-x}$ et $0 \leq \frac{t^x}{1-t} \leq \frac{t^n}{1-x}$. \\
       On intègre sur $[ 0 ; x]$ : $ 0 \leq  \int_0^x \frac{t^n}{1-t}\ dt \leq \frac{1}{1-x} \int_0^x t^n\ dt = \frac{x^{n+1} }{(n+1) (1-x)} \xrightarrow[ n \rightarrow +\infty]{} 0$ donc par théorème de comparaison, $\dlim{n \rightarrow +\infty} \int_0^x \frac{t^n}{1-t}\ dt = 0$. \\
     \item On en déduit que pour tout $n \in \N$, $S_n = \Sum{k=1}{n} \frac{x^k}{k} = \int_0^x \frac{1}{1-t}\ dt - \int_0^x \frac{t^n}{1-t}\ dt \xrightarrow[ n \rightarrow +\infty]{} \int_0^x \frac{1}{1-t}\ dt = \left[ - \ln (\vert 1-t \vert ) \right]_0^x = - \ln (1-x) $. \\
       La suite des sommes partielles converge vers $- \ln (1-x)$ donc la série converge et $\Sum{k=1}{+\infty} \frac{x^k}{k} = - \ln (1-x)$. \\
     \end{enumerate}
   \item \begin{enumerate}
     \item $X(\Omega) = \N^*$ donc $Y(\Omega) = \left\{ \frac{1}{n}\ \big\vert\ n \in \N^* \right\}$. \\
       De plus pour tout $n \in \N^*$, $P \left( Y = \frac{1}{n} \right) = \Prob(\Ev{ X = n}) = q^{n-1} p$, où $q = 1-p$. \\
     \item Le moment d'ordre $r$ de $Y$ existe si et seulement si la série $\Sum{n = 1}{+\infty} \left( \frac{1}{n} \right)^r q^{n-1} p$ converge absolument, ce qui est équivalent à la convergence si la série est à termes positifs. \\
       Or on a pour tout $n \geq 1$, pour tout $r \geq 1$, $n^r \geq 1$ donc $0 \leq \frac{1}{n^r} \leq 1$ et enfin $0 \leq \frac{ p q^{n-1} }{n^r} \leq q^{n-1} p$. \\
       Or la série de terme général $q^{n-1} p$ converge (série géométrique avec $ \vert q \vert 1$, donc par théorème de comparaison des séries à termes positifs, la série converge et converge absolument car elle est à termes positifs. \\
     \item $\E(Y) = \Sum{n=1}{+\infty} \frac{1}{n} p q^{n-1}= \frac{p}{q} \Sum{n = 1}{+\infty} \frac{q^n}{n} = \frac{ - p \ln (1-q) }{q} = \frac{ - p \ln p}{1-p}$. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit la matrice $A = \begin{smatrix} 1 & 2 \\ 3 & 4 \\ -1 & 4 \\ \end{smatrix}$.\begin{enumerate}
   \item On prend $B$ quelconque, on a $A B =  \begin{smatrix} 1 & 2 \\ 3 & 4 \\ -1 & 4 \\ \end{smatrix}  \begin{smatrix} a & b &  c \\ d & e & f \\ \end{smatrix} =  \begin{smatrix} a+2d & b + 2e & c + 2f \\ 3 a + 4d & 3b + 4e & 3c + 4f \\ -a + 4d & -b + 4e & -c + 4f \\ \end{smatrix}$. \\
     En identifiant les coefficients, on obtient trois systèmes sur $(a,d)$, $(b , e)$ et $(c,f)$, et la résolution du premier donne $a=d=0$ et $a+2d=1$ (absurde). \\
     Il n'y a donc pas de solution. \\
   \item On prend $c$ quelconque, on a $C A  = \begin{smatrix} a & b &  c \\ d & e & f \\ \end{smatrix}  \begin{smatrix} 1 & 2 \\ 3 & 4 \\ -1 & 4 \\ \end{smatrix}   =  \begin{smatrix} a+3b-c & 2a+4b+4c  \\ d+3e-f & 2d+4e+4f \\\end{smatrix}$. \\
     En identifiant les coefficients, on obtient deux systèmes sur $(a,b,c)$ et $(d,e,f)$ dont la résolution donne $a=-2b-2c$ et $b=c+1$ donc $a = -4c - 2$ qui a une infinité de solutions (une pour chaque valeur réelle de $c$). \\
     De même le deuxième donne une infinité de solutions, et il y a donc une infinité de solutions à l'équation matricielle $C A = I_2$.

   \end{noliste}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   On considère la suite réelle $(u_n)_{n \in \N}$ définie par : 
   \[
   u_0 = 3,\ u_1 = \frac{29}{9} \text{ et } \forall n \in \N,\ u_{n+2} = 9 - \frac{26}{u_{n+1}} + \frac{24}{u_n u_{n+1}}.
   \]
   \begin{enumerate}
   \item L'ensemble des suites réelles vérifiant $a u_{n+2} + b u_{n+1} + c u_n$ est obtenu à l'aide de l'étude de l'équation caractéristique $a r^2 + b r + c$ et de son discriminant $\Delta$. \\
     Si $\Delta = 0$, il y a une unique solution $r_0$ à l'équation, et les suites solutions sont les $(u_n)$ telles que pour tout $n$, $u_n = a r_0^n + b n r_0^n$. \\
     Si $\Delta > 0$, il y a deux solutions distinctes $r_1$ et $r_2$ à l'équation, et les suites solutions sont les $(u_n)$ telles que pour tout $n$, $u_n = a r_1^n + b  r_2^n$. \\
     Enfin si $\Delta < 0$, on ne connaît pas les solutions (elles s'écrivent avec des nombres complexes ou des sinus et cosinus). \\
   \item Il faut garder les valeurs de $u_n$ et $u_{n-1}$ pour obtenir celle de $u_{n+1}$, donc il faut déjà deux variables $u$ et $v$. \\
     De plus quand on donne la valeur de $u_{n+1}$, on efface la valeur de $u_n$, qui était dans $u$, et qu'on doit pourtant redonner à $v$ pour l'étape suivante (vous me suivez???) donc il faut une variable auxiliaire pour ne pas la perdre. \\
     Cela donne : \\
     var u, v , aux  : real ; k, n : integer; \\
     readln (n) \\
     u : = 29/9 ; v:=3 ; \\
     for k:= 2 to n do \\
     begin \\
     aux := u; u : = 9 - 26 / u + 24 / ( u $\ast$ v) ; v : = aux ; \\
     end; \\
     writeln (u ) ; \\ \\
     On peut rajouter quelques fioritures pour rendre le programme plus facile à l'utilisateur ( writeln ('n?') au début , writeln( ' la valeur du terme d'ordre n de la suite est ') avant le résultat). \\
   \item Il existe une unique suite vérifiant $a_0 = 3$ et $a_{n+1} = a_n \times u_n$ (définition par récurrence des suites). \\
     Il faut alors vérifier que $a_{n+3} = 9 a_{n+2} - 26 a_{n+1} + 24 a_n$, et que $(a_n)$ est à valeurs dans $\N^*$. \\
     Pour la relation de récurrence linéaire, on écrit : \\
     $a_{n+3} = u_{n+2} a_{n+2} = \left(  9 - \frac{26}{u_{n+1}} + \frac{24}{u_n u_{n+1}} \right) a_{n+2} = 9 a_{n+2} - 26 \frac{ a_{n+2}}{u_{n+1}} + 24 \frac{ a_{n+2}}{u_n u_{n+1}}$. \\ \\
     Or $a_{n+2}{u_{n+1}} =\frac{ a_{n+1} u_{n+1} }{ u_{n+1} } = a_{n+1}$ et $ \frac{a_{n+2} }{u_n u_{n+1} } = \frac{a_{n+1} }{u_n} = \frac{ a_n u_n }{u_n} = a_n$ donc on a bien : \\
     $a_{n+3} = 9 a_{n+2} - 26 a_{n+1} + 24 a_n$. \\ \\
     On montre par récurrence que pour tout $n$, $a_n \in \Z$. \\
     Initialisation : $a_0 = 3 \in \N^*$, $a_1 =u_0 a_0 = 9$ et $a_2 = u_1 a_1 = 9 \frac{29}{9} = 29$ donc c'est bon. \\
     Hérédité : on suppose qu'il existe $n \geq 2$ tel que $a_n$, $a_{n-1}$ et $a_{n-2}$ soient des entiers naturels non nuls. \\
     Alors $9 a_{n+2}$, $26 a_{n+1}$ et $24 a_n$ sont des entiers, et par somme $a_{n+3}$ est un entier. \\ \\
     Je ne vois pas du tout comment montrer que la suite $(a_n)$ est à termes positifs, ce qui est pourtant essentiel pour prouver que la suite $(u_n)$ est bien définie. \\
     La question suivante permet de le prouver, mais a priori ici il faudrait le montrer sans s'en servir.... \\
   \item Récurrence pas trop difficile : \\
     Initialisation : $a_0 = 3$, et $2^0 + 3^0+4^0 = 1+1+1=3$. \\
     $2^1+3^1+4^1 = 9 = a_1$, et $2^2 + 3^2+4^2 = 29 = a_2$. \\
     Hérédité :  on suppose qu'il existe $n \geq 0$ tel que $a_k = 2^k + 3^k + 4^k$ pour $ 0 \leq k \leq n + 2$. \\
     On a alors $a_{n+3} = 9 ( 2^{n+2} + 3^{n+2} + 4^{n+2} ) - 26 ( 2^{n+1} + 3^{n+1} + 4^{n+1} ) + 24 ( 2^n + 3^n + 4^n) = 2^{n+2} ( 9 - 13 + 6) + 3^{n+1} ( 27 - 26 + 8) + 4^{n+1} ( 36 - 26 + 6) = 2 \times 2^{n+2} +  3^2 \times 3^{n+1} + 4^2 \times 4^{n+1} = 2^{n+3} + 3^{n+3} + 4^{n+3}$. \\ \\
     Cela prouve que $a_n > 0$, puis que $u_n = \frac{a_{n+1}}{a_n}$ est bien définie. \\
   \item On a alors $u_n = \frac{ a_{n+1}}{a_n} = \frac{2^{n+1} + 3^{n+1} + 4^{n+1} }{2^n + 3^n + 4^n}$. \\
     Le numérateur s'écrit $4^{n+1} \left( \frac{1}{2^{n+1} } + \left( \frac{3}{4} \right)^{n+1} + 1 \right) \sim 4^{n+1}$. \\
     De même $2^n + 3^n +4^n \sim 4^n$, donc $u_n \sim \frac{4^{n+1} }{4^n} = 4$ donc $\dlim{n \rightarrow +\infty} u_n = 4$. \\
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soient $X_1$ et $X_2$ deux variables aléatoires définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$, indépendantes et de lois géométriques de paramètres $p_1$ et $p_2$ respectivement ($p_i \in ]0 ; 1[$ pour $i = 1,2$). \\
   On pose $U = X_1 + X_2$ et $T = X_1 - X_2$. \begin{enumerate}
   \item Commençons avec le calcul de la covariance. \\
     $\Cov ( U , T) = \Cov( X_1 + X_2 , X_1 - X_2) = \V(X_1) - \V(X_2) = \frac{1-p_1}{p_1^2} - \frac{1- p_2}{p_2^2} = \frac{ p_2^2 - p_1 p_2^2 - p_1^2 + p_1^2 p_2  }{p_1^2 p_2^2} = \frac{ (p_2 - p_1) (p_1 + p_2) + p_1 p_2 ( p_1 - p_2)}{(p_1 p_2 )^2} = \frac{p_1 - p_2}{(p_1 p_2 )^2} ( p_1 p_2 - p_1 - p_2)$. \\
     Ce dernier facteur est non nul car $0 < p_1 p_2 < p_2$ donc $-p_2 - p_1 < p_1 p_2 - p_2 - p_1 < -p_1 < 0$. \\
     Enfin on a $p_1 \neq p_2$, donc $p_1 - p_2 \neq 0$, et donc $\Cov ( U ,T) \neq 0$, donc $U$ et $T$ ne sont pas indépendantes. \\
   \item Cette fois on a $\Cov (U ,V) = \V(X_1) - \V(X_2) = 0$ ce qui ne permet pas de conclure. \\
     On a $U (\Omega) = \llb 2 ; +\infty \llb$ et $T(\Omega) = \Z$. \\
     Pour tout $n \geq 2$, pour tout $k \in \Z$, $\Prob( [U =n] \cap [ T = k] ) = P \left( \left[X_1 =  \frac{n+k}{2}\right] \cap \left[ X_2 = \frac{n-k}{2} \right] \right)$. \\
     Prenons une valeur nulle, par exemple avec $n+k$ impair, donc $n = 2$ et $k=1$. \\
     $\Prob( [U =2] \cap [T = 1] ) = 0$, et $\Prob(\Ev{ U = 2}) = \Prob( [ X_1 = 1] \cap [X_2 = 1] ) = \Prob(\Ev{ X_1=1}) \Prob(\Ev{X_2 = 1}) \neq 0$. \\
     De même $ [\Ev{ X_1 = 2} \cap \Ev{X_2 = 1}] \subset \Ev{T=1}$ donc $\Prob(\Ev{ T=1}) \geq P [\Ev{ X_1 = 2} \cap \Ev{X_2 = 1}] = \Prob(\Ev{ X_1 =2}) \Prob(\Ev{ X_2=1}) \neq 0$. \\ \\
     Les variables $U$ et $T$ ne sont donc pas indépendantes.
   \end{enumerate}
 \end{exercice}

 \newpage


 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Soient $(a,b,c) \in \R^3$ et $A = \begin{smatrix} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \\ \end{smatrix}$. \\
   On pose $N = A- I$ et $M = N^2 - N$ (où $I$ désigne la matrice identité de $\mathcal{M}_3(\R)$. \\ \\
   Soient $u$ et $v$ les endomorphismes de $\R^3$ canoniquement associés aux matrices $M$ et $N$.
   \begin{enumerate}
   \item Deux matrices $M$ et $N$ sont semblables s'il existe $P$ inversible telle que $M = P N P^{-1}$. \\
     On prouve que deux matrices sont semblables si et seulement si ce sont deux matrices d'un même endomorphisme dans des bases distinctes, et on en déduit que les dimensions de leurs noyaux et leurs images sont égales, que leurs valeurs propres sont égales, et que les sous-espaces propres associés sont de même dimension. \\
     Enfin on obtient que l'une est inversible si et seulement si l'autre l'est, et que l'une est diagonalisable siet seulement si l'autre l'est. \\
     Enfin on sait que si $M$ et $N$ sont semblables, pour tout $n \in \N$ (et même $n \in \Z$ si elles sont inversibles), $M^n$ et $N^n$ sont semblables et plus précisément : $M^n = P N^n P^{-1}$. \\
   \item $A$ est triangulaire donc son unique valeur propre est 1. On en déduit par l'absurde que $A$ est diagonalisable si et seulement si $A = I$, donc si et seulement si $a=b=c=0$. \\
   \item $0 \notin \spc A$ donc $A$ est inversible. \\
     $A$ étant triangulaire, on peut déterminer son inverse avec un pivot qui sera très rapide : on obtient $A^{-1} = \begin{smatrix} 1 & -a & -b+ac \\ 0 & 1 & -c \\ 0 & 0 & 1 \\ \end{smatrix} = I + M$ en calculant $M = \begin{smatrix} 0 & -a & -b+ac \\ 0 & 0 & -c \\ 0 & 0 & 0 \\ \end{smatrix}$. \\
   \item On suppose dans cette question que le rang de $u$ est égal à 2. 
     \begin{enumerate}
     \item $N$ est nilpotente, de la forme $\begin{smatrix} 0 & a & b & \\ 0 & 0 & c \\ 0 & 0 & 0 \\ \end{smatrix}$, avec $a \neq 0$ et $c \neq 0$, sinon les trois colonnes seraient colinéaires et $u$ serait de rang 1. \\
       On a alors $N^2 = \begin{smatrix} 0 & 0 & a c & \\ 0 & 0 & 0 \\ 0 & 0 & 0 \\ \end{smatrix} \neq 0$, et $N^3 = 0$. \\
       D'où $u^2 \neq 0$, on prend $x$ tel que $u^{2} (x) \neq 0$. \\
       Soient alors $a,\ b$ et $c$ tels que $a x + b u(x) + c u^2 (x) = 0$. \\
       On compose par $u^2$, on a $a u^2 (x) + b u^3 (x) + c u^4 (x) = a u^2 (x) = u^2 (0) = 0$, et $u^2 (x) \neq 0$ donc $a =0$. \\
       Ensuite on a $b u(x) + cu^2 (x) = 0$, on compose par $u$ et on a $b u^2 (x) + c u^3 (x) = b u^2 (x) = u ( 0) = 0$ donc $b=0$, et enfin $c u^2 (x) = 0$ donc $c = 0$. \\
       La famille $(u^2 (x), u(x) , x)$ est donc libre et de cardinal 3 donc c'est une base.
       \\ \\
       Dans cette base on a $ u( u^2 (x)) = u ( u^2 (x))  = u^3 (x) = 0$. \\
       Ensuite $u( u(x) ) = u^2 (x)$  et enfin on a $u(x) = u(x)$ donc la matrice de $u$ est $\begin{smatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \\ \end{smatrix}$, et $N$ est bien semblable à cette matrice, car elles sont deux matrices associées au même endomorphisme. \\
     \item $Mat_{ \mathcal{B} } (v) = \begin{smatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \\ \end{smatrix}^2 - \begin{smatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \\ \end{smatrix} = \begin{smatrix} 0 & -1 & 1 \\ 0 & 0 & -1 \\ 0 & 0 & 0 \\ \end{smatrix}$ vérifie les conditions de la question précédente : nilpotente d'ordre 3, triangulaire et de rang 2 : en appliquant cette question, on montre que cette matrice, et donc $M$, sont semblables à la matrice $\begin{smatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \\ \end{smatrix}$, elle-même semblable à $N$. \\
       Finalement $M$ et $N$ sont bien semblables. \\
     \item Il existe alors $P$ inversible telle que $M = P N P^{-1}$. \\
       On en déduit que $A^{-1} = I + P N P^{-1} = P I P^{-1} + P N P^{-1} = P ( I + N ) P^{-1} = P A P^{-1}$ et $A$ et $A^{-1}$ sont donc semblables. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $X$ une variable aléatoire que suit la loi de Poisson de paramètre $\lambda > 0$. \\
   On désigne l'espérance par $E$. \begin{enumerate}
   \item Le théorème de transfert assure que c'est équivalent à la convergence absolue, et par positivité à la convergence, de la série $\Sum{n=0}{+\infty} \frac{1}{1 + n} \times \frac{\lambda^n e^{-\lambda} }{n!}$. \\
     Enfin on a $ 0 \leq \frac{1}{1 + n} \times \frac{\lambda^n e^{-\lambda} }{n!} \leq \frac{\lambda^n e^{-\lambda} }{n!}$ donc par comparaison des séries à termes positifs (cette dernière suite est le terme général d'une série exponentielle convergente), la série converge et l'espérance de $\frac{1}{1+X}$ existe bien. \\
   \item La majoration précédente montre que $E \left( \frac{1}{1+X} \right) \leq \Sum{n=0}{+\infty} \frac{\lambda^n e^{-\lambda} }{n!} = 1$ (somme des probabilités de la loi de Poisson de paramètre $\lambda$. \\
     Ensuite on écrit que $ 1 +n \geq n$ donc $\frac{1}{1+n} \leq \frac{1}{n}$, et $\frac{1}{1+n} \frac{\lambda^n e^{-\lambda}}{n!} \leq \frac{1}{\lambda} \frac{ \lambda^{n+1} e^{- \lambda} }{(n+1)!}$. \\
     On obtient alors $E \left( \frac{1}{1+X} \right) \leq  \frac{1}{\lambda} \Sum{n=0}{+\infty} \frac{ \lambda^{n+1} e^{- \lambda} }{(n+1)!} \leq  \frac{1}{\lambda} \Sum{n=1}{+\infty} \frac{ \lambda^{n} e^{- \lambda} }{n!} \leq  \frac{1}{\lambda} \Sum{n=0}{+\infty} \frac{ \lambda^{n} e^{- \lambda} }{n!} \leq \frac{1}{\lambda}$. \\ \\
     On obtient bien que $E \left( \frac{1}{1+X} \right) \leq \min \left( 1 , \frac{1}{\lambda} \right)$.
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Une urne contient des boules blanches, noires et rouges. Les proportions respectives de ces boules sont $b$ pour les blanches, $n$ pour les noires et $r$ pour les rouges ($b + n + r = 1$). \\
   On effectue dans cette urne des tirages successifs indépendants avec remise. Les proportions de boules restent ainsi les mêmes au cours de l'expérience. \\
   On modélise l'expérience par un espace probabilisé $(\Omega , \mathcal{A} , P)$.
   \begin{enumerate}
   \item La loi d'un couple $(X,Y)$ de variables aléatoires discrètes est la donnée de $(X,Y) (\Omega)$, ensemble des couples $(i,j)$ de valeurs telles que l'évènement $\Ev{X= i} \cap \Ev{Y = j}$ est possible. \\
     Les lois marginales sont les lois des variables $X$ et $Y$, obtenues à partir du couple et de la formule des probabilités totales. \\
   \item Pour $k \in \N^*$, on note $Z_k$ la variable aléatoire qui prend la valeur $+1$ si une boule blanche est tirée au $k$-ième tirage. On note $S_k = Z_1 + \dots + Z_k$.
     \begin{enumerate} 
     \item $S_1 = Z_1$ vérifie $Z_1 (\Omega) = \{ -1 ; 0 ; 1 \}$, avec $\Prob(\Ev{ Z_1 = -1}) = n$, $\Prob(\Ev{ Z_1 = 0}) = r$ et $\Prob(\Ev{ Z_1 = 1}) = b$, d'espérance $\E(Z_1) = b - n$ et de variance : \\
       $\V(Z_1) = b ( b-n-1)^2 + r (b-n)^2 + n (b-n+1)^2 = (b-n)^2 ( b + r + n) + b + n + 2 ( n (b-n) - b (b-n) ) \\ \V(Z_1) = (b-n)^2 + b + n - 2 n^2 - 2 b^2 = -n^2 - b^2  - 2 b n + b + n  \\ \V(Z_1)= - (n+b)^2 + n + b = - (1-r)^2 + 1-r = (1-r) ( 1 - 1 + r) = r (1-r)$. \\ \\
       D'où par linéarité de l'espérance et indépendance des tirages pour la variance $S_k$ a pour espérance $k (b-n)$ et pour variance $k r (1-r)$. \\
     \item $g_k(t) = E \left(t^{S_k} \right) = E \left( t^{ \Sum{i=1}{k} Z_i} \right) = E \left( \prod\limits_{i=1}^k t^{  Z_i} \right) = \prod\limits_{i=1}^k E \left(  t^{  Z_i} \right)$ par indépendance des variables $Z_i$, qui donnent l'indépendance des variables $t^{Z_i}$. \\ 
       De plus comme elles suivent toutes la même loi, on a $g_k(t) = \left[ E \left(  t^{  Z_1} \right) \right]^k$. \\ \\
       Enfin on a par théorème de transfert, $E \left( t^{Z_1} \right) = t^{-1} \times n + t^0 \times r + t^1 \times b = \frac{n}{t} + r + b t = \frac{ n + r t + b t^2}{t}$. \\ \\
       Enfin on a $g_k(t) = \left( \frac{ n + r t + b t^2}{t} \right)^k$.   \\
     \item $g_k(t) = \sum\limits_{a \in S_k (\Omega) } t^a \Prob(\Ev{S_k = a})$ donc $g_k'(t) =  \sum\limits_{a \in S_k (\Omega) } a t^{k-1} \Prob(\Ev{ S_k = a})$ et enfin $g_k'(1) =  \sum\limits_{a \in S_k (\Omega) } a 1^{k-1} \Prob(\Ev{ S_k = a}) =  \sum\limits_{a \in S_k (\Omega) } a  \Prob(\Ev{ S_k = a}) = \E( S_k)$. \\ \\
       On calcule $g_k'(t) = k \times \frac{ (r + 2bt) t - n -rt - b t^2}{t^2} \times  \left( \frac{ n + r t + b t^2}{t} \right)^{k-1} = k \times \frac{b t^2 - n}{t^2} \times   \left( \frac{ n + r t + b t^2}{t} \right)^{k-1}$ et enfin $E ( S_k) = g_k'(1) =k( b -n) (n+r+b)^{k-1} = k(b-n) $ car $n+r+b=1$. \\ 
     \end{enumerate}
   \item 
     \begin{enumerate} 
     \item $X_1 \suit \mathcal{G} (b)$, $\E(X_1) = \frac{1}{b}$ et $\V(X_1) = \frac{1-b}{b^2}$. \\
     \item Si $X_1=k$, on sait qu'on a pas tiré de boule blanche; on est donc dans la situation de deux couleurs, noir de proportion $\frac{n}{n+r}$ et rouge de proportion $\frac{r}{n+r}$. \\
       On a alors $P_{\Ev{X_1 = k}} ( R_1 \cap \dots \cap R_{k-1} ) = \left( \frac{r}{n+r} \right)^{k-1}$. \\
     \item C'est la loi binomiale de paramètres $k-1$ et $\frac{r}{n+r}$. \\
     \item Avec le système complet d'évènements $\Ev{X_1 = k}_{k \in \N^*}$ on a pour tout $i \in \N$ :\\
       $\Prob(\Ev{W = i}) = \Sum{k=1}{+\infty} \Prob(\Ev{X_1 = k}) P_{\Ev{X_1=k}} \Ev{W=i} = \Sum{k=i+1}{+\infty} b (r+n)^{k-1} \binom{k}{i} \left( \frac{r}{r+n} \right)^i \left( \frac{n}{r+n} \right)^{k-1-i} = \Sum{k=i+1}{+\infty} b \binom{k}{i} r^i n^{k-1-i}$. \\
     \end{enumerate}
   \item On note $Y_1$ la variable représentant le numéro du tirage auquel une boule noire sort pour la première fois.
     \begin{enumerate} 
     \item Si $k=l$, la probabilité est nulle (on ne peut pas tirer en même temps une boule noire et une blanche. \\
       Si $k < l$ on a des rouges en position 1 à $k-1$, une blanche en position $k$, une blanche ou une rouge en positions $k+1$ à $l-1$ et une noire en position $l$. \\
       D'où $P ( X_1 =k , Y_1 =l) = r^{k-1} b (r+b)^{l-k-1} n$. \\
       Par symétrie si $k >l$, $\Prob(X_1 = k , Y_1 = l) = r^{l-1} n (r+n)^{k-l-1} b$.  \\ \\
       $\Prob(\Ev{X_1 = 1}) \Prob(\Ev{Y_1 = 1}) \neq 0 = \Prob( X_1 =1 , Y_1 =l)$ donc $X_1$ et $Y_1$ ne sont pas indépendantes. \\
     \item L'une des deux variables vaut toujours 1, et l'autre vaut une valeur $k \geq 2$. \\
       D'où $X_1 Y_1 (\Omega) = \llb 2 ; +\infty \llb$ et $\Prob( X_1 Y_1 = k) = \Prob( X_1=1 , Y_1=k) + \Prob( X_1=k , Y_1=1) = b^{k-1} n + n^{k-1} b $. \\
       On obtient en sommant $\E(X_1 Y_1) = n \left( \frac{1}{(1-b)^2}  - 1 \right) + n \left( \frac{1}{(1-b)^2}  - 1 \right) = \frac{1}{n} + \frac{1}{b} - n-b = \frac{1}{n} + \frac{1}{b} - 1$. \\ \\
       On a ensuite $\Cov (X_1 , Y_1) = E ( X_1 Y_1) - \E(X_1) \E(Y_1) =\frac{1}{n} + \frac{1}{b} - 1 - \frac{1}{n} - \frac{1}{b} = -1$. \\
     \end{enumerate} 
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soient $n \geq 2$ et $(x_1 , x_2,\ \dots\ , x_n) \in \R^n - \{(0,\ \dots\ ,0) \}$. \\
   On pose $X = \begin{smatrix} x_1 \\ x_2 \\ \vdots \\ x_n \\ \end{smatrix}$, puis $B = {}^tX X$ et $A = X {}^t X$. \begin{enumerate}
   \item $B = \Sum{k=1}{n} x_k^2$ est un réel. \\
   \item $A$ est symétrique donc diagonalisable, on doit trouver la somme des dimensions égale à $n$. \\
     On étudie l'équation $A Y = \lambda Y \Leftrightarrow (X {}^t X) Y = \lambda Y \Leftrightarrow X ({}^t X Y) = \lambda Y \Leftrightarrow ({}^t X Y)  X = \lambda Y \Leftrightarrow \left( \Sum{k=1}{n} x_k y_k \right) X = \lambda Y$. \\
     Deux possibilités : soit $\lambda = 0$, et on obtient $\Sum{k=1}{n} x_k y_k = 0$. \\
     Soit $\lambda \neq 0$; alors $Y = \frac{ \left( \Sum{k=1}{n} x_k y_k \right)}{\lambda} X$ est colinéaire à $X$. \\
     Calculons alors $ X {}^t X X = \left( \Sum{k=1}{n} x_k^2 \right) X$ donc $\lambda_0 = \left( \Sum{k=1}{n} x_k^2 \right) \neq 0$ est valeur propre et $X$ un vecteur propre associé, et ce qui précède donne $E_{\lambda_0} (A) = \Vect{ X}$. \\
     On a obtenu de plus $\spc ( A) = \{ \lambda_0 , 0 \}$. \\ \\
     Il reste à déterminer $\ker A$, qui doit être de dimension $n-1$. \\
     Les colonnes de $A$ sont toutes égales à $x_k X$, et il existe $i$ tel que $x_i \neq 0$. \\
     On a alors $C_k = x_k X = \frac{x_k}{x_i} \times x_i X = \frac{x_k}{x_i} C_i$ et $C_k - \frac{x_k}{x_i} C_1 = 0$. \\
     On trouve que pour tout $k \neq i$, $e_k - \frac{x_k}{x_i} e_i \in \ker A$. \\
     La famille obtenue est libre car échelonnée et admet $n-1$ vecteurs donc c'est une base de $\ker A$. \\ \\
     Cet exercice ne peut être résolu qu'en écrivant la matrice $A$ explicitement, afin de comprendre à la vue de la matrice comment trouver cette base de $\ker A$ (qui semble sortir de nulle part dans cette correction!!).
   \end{enumerate}
 \end{exercice}

 \newpage

 \section{\underline{Annales 2008}}

 %\setcounter{exercice}{0}

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Toutes les variables admettent alors une variance et on a : \\
     $\V( \Sum{k=1}{n} X_k ) = \Sum{k=1}{n} \V(X_k) + 2 \sum\limits_{1 \leq i < j \leq n} \Cov (X_i , X_j)$. \\ \\
     Soit $(X_n)_{n \geq 1}$ une suite de variables aléatoires indépendantes de même loi, à valeurs dans $\{-1 ; 1\}$, définies sur une même espace probabilisé $(\Omega , \mathcal{A} , P)$. On pose, pour tout $n \in \N^*$, $p = \Prob(\Ev{[X_n = 1]})$, et on suppose que $p \in ] 0 ; 1[$. \\
   \item Pour tout $n \in \N^*$, on pose $Y_n = \prod\limits_{i=1}^n X_i$.
     \begin{enumerate} 
     \item $Y_2 (\Omega) = \{ -1 ; 1\}$ et $\Prob(\Ev{Y_2 = 1}) = \Prob(\Ev{ X_1 = Y_1}) = p^2 + (1-p)^2$, et $\Prob(\Ev{Y_2 = -1}) = \Prob( X_1 \neq Y_1) = 2 p (1-p)$. \\ \\
       $Y_3 (\Omega) = \{ -1 ; 1\}$, $\Prob(\Ev{Y_3=1}) = \Prob(\Ev{Y_2 = X_3}) = p (p^2 + (1-p)^2 ) + 2 p (1-p)^2$ et $\Prob(\Ev{Y_3 = -1}) = \Prob( Y_2 \neq X_3) = p^2 (1-p) + (1-p)^3 + 2 p^2 (1-p)$. \\ 
       On peut s'amuser à simplifier ces résultats mais cela n'a pas grand intérêt. \\
     \item De même $Y_n ( \Omega) = \{-1 ; 1\}$ pour tout $n$ et on a : \\
       $\Prob(Y_{n+1} = 1) = P (Y_n X_{n+1} = 1) = \Prob(Y_n = X_{n+1}) = p_n \times p + (1-p_n) (1-p) = p_n (2p -1) + 1 - p$. \\ \\
       C'est une suite arithmético-géométrique, on résout l'équation $k = k (2p-1) + 1-p \Leftrightarrow 2 k (1-p) = 1-p \Leftrightarrow k = \frac{1}{2}$. \\ \\
       Puis on considère $u_n = p_n - \frac{1}{2}$, on montre qu'elle est géométrique de raison $2p-1$ et on obtient $u_n = (p-1)^{n-1} u_1$, donc $p_n = (2p-1)^{n-1} \left( p_1 + \frac{1}{2} \right) + \frac{1}{2} = (2p-1)^{n-1} \left( p + \frac{1}{2} \right) + \frac{1}{2}$. \\
     \item Il faut pour cela $\Prob(Y_n = i , Y_{n+1} = j) = \Prob(\Ev{Y_n = i}) \Prob( Y_{n+1} = j)$ pour $i$ et $j$ dans $\{ 1 ; 2\}$. \\
       Or $\Prob(Y_n=1 , Y_{n+1} = 1) = \Prob( Y_n =1 , X_{n+1} = 1 ) = p_n p$ donc il faut que $\Prob( Y_{n+1} = 1) = p_{n+1} = p$ ou que $p_n = 0$. \\ \\
       De même $\Prob(Y_n = -1 , Y_{n+1} = -1) = \Prob(Y_n = -1 , X_{n+1} = 1) = (1-p_n) p$ donc il faut $\Prob(Y_{n+1} = -1) = 1 - p_{n+1} = p$ ou $p_n = 1$.
       $\Prob(Y_n =1 , Y_{n+1} = -1) = \Prob(Y_n = 1 , X_{n+1} = -1) = p_n (1-p)$ donc il faut $p_n = 0$ ou $1 - p_{n+1} = 1-p$; cela ne donne rien de plus. De même pour la dernière. \\ \\
       Comme on ne peut avoir $p_n = 0 = 1$, il y a deux possibilités : soit $p_n= 0$ et $p_{n+1} = 1-p$, soit $p_n = 1$ et $p_{n+1} = p$. \\ \\
       $p_n = 0$ donne $(2p-1)^{n-1}  = - \frac{1}{2 \left( p + \frac{1}{2} \right)} $ qui impose $2p - 1 < 0$ et $n-1$ impair, et $(n-1) \ln (1-2p) = - \ln 2 - \ln \left( p + \frac{1}{2} \right)$ donc $n = 1 - \frac{ \ln 2 + \ln \left( p + \frac{1}{2} \right)}{ \ln (1-2p)}$, $n$ impair et $p < \frac{1}{2}$. \\ \\
       On a alors $p_{n+1} = p_n ( 2p-1) + 1 - p = 0 (2p-1) + 1-p= 1-p$ et la deuxième condition est bien vérifiée. \\ \\
       $p_n = 1$ donne $(2p-1)^{n-1}  =  \frac{1}{2 \left( p + \frac{1}{2} \right)} $ qui impose $2p - 1 > 0$ ou $n-1$ pair; \\
       si $2p-1 <0$ on a $(n-1) \ln (1-2p) =  \ln 2 + \ln \left( p + \frac{1}{2} \right)$ donc $n = 1 + \frac{ \ln 2 + \ln \left( p + \frac{1}{2} \right)}{ \ln (1-2p)}$, $n$ pair et $p < \frac{1}{2}$. \\ 
       si $2p-1 >0$ on a $(n-1) \ln (2p-1) =  \ln 2 + \ln \left( p + \frac{1}{2} \right)$ donc $n = 1 + \frac{ \ln 2 + \ln \left( p + \frac{1}{2} \right)}{ \ln (2p-1)}$, $n$ pair ou impair et $p > \frac{1}{2}$. \\ \\ 
       On a alors $p_{n+1} = p_n ( 2p-1) + 1 - p =  (2p-1) + 1-p= p$ et la deuxième condition est bien vérifiée. \\ 
     \end{enumerate}
   \item On détermine les valeurs de $S_n$ en considérant que si $k$ variables $X_i$ valent 1, alors $n-k$ valent $-1$ et $S_n = k - (n-k) = 2k-n$. \\
     Comme la fonction de $k$ obtenue est bijective, la probabilité que $S_n = 2k - n$ est celle que $k$ variables $X_i$ valent 1, et en posant $X_i' =1$ si $X_i = 1$ et 0 si $X_i = -1$, $\Prob(S_n=2k - n) = \Prob( \Sum{i=1}{n} X_i' = k)$ qui est une loi binomiale. \\
     D'où $\Prob(\Ev{S_n = 2k-n}) = \binom{n}{k} p^k (1-p)^{n-k}$, $\E(S_n) = 2  \E( \Sum{k=1}{n} X_i' ) -n = 2 n p - n = n (2p - 1)$ puis $\V(S_n) =4 \V(\Sum{k=1}{n} X_i') = 4 n p(1-p)$.  \\ \\
     Deuxième solution : on exprime tout de suite les $X_i'$ sous la forme $X_i' = \frac{X_i + 1}{2}$ donc $X_i = 2 X_i' - 1$ et on a $S_n = 2 S_n' -n$, avec $S_n'$ qui suit la loi binomiale de paramètres $n$ et $p$. On retrouve le même résultat. \\
   \item La deuxième modélisation ci-dessus permet de le faire. \\
     var S , x , k , n : integer; p : real; \\
     begin ; \\
     readln (p); readln (n); S:=0; \\ randomize ;\\
     for k:=1 to n do \\
     begin \\
     x : = random (1); x : = 2x - 1 ; S:=S+x; \\
     end ; end. \\ \\
     Si on utilise la première modélisation, on remplace (x := 2x-1 ;) par (if x:=0 then x:= -1;) et on obtient le même résultat.
     \\
   \end{noliste}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   La question suivante permet de conjecturer que la limite est 1, et $u_n$ est trivialement supérieure à 1, on va donc chercher à majorer $u_n -1 = \frac{u_{n-1}}{n+1}$ par une suite qui tend vers 0; cela conduit à essayer de majorer $u_n$ par une constante. \\
   Le calcul des premiers termes ($u_0 < 1$, $u_1 = 1 + u_0 < 2$, $u_2 = 1 + \frac{1+u_0}{2} < 2$, etc...) permet de conjecturer $u_n \leq 2$, qu'on prouve par récurrence : \\
   $u_0 \leq 2$, $u_1 \leq 2$ et $u_2 \leq 2$ viennent d'être prouvés. \\
   Si il existe $n \geq 2$ tel que $u_n \leq 2$, alors $u_{n+1} = 1 + \frac{u_n}{n+1} \leq 1 + \frac{u_n}{3} \leq 1 + \frac{2}{3} \leq 2$ et le résultat est prouvé pour tout $n$. \\ \\
   Attention l'hérédité doit être faite à partir de $n =1$ au plus bas; en effet si on suppose $u_0 \leq 2$ seulement on a $u_1 \leq 1 + 2 \leq 3$ qui n'est pas suffisant. Mais comme l'énoncé donnait $u_0 \leq 1$ on a résolu le problème en initialisant aux valeurs $0$ et $1$. \\ \\
   Enfin on obtient pour tout $n \geq 1$, $1 \leq u_n \leq 1 + \frac{u_{n-1} }{n} \leq 1 + \frac{2}{n}$ et par théorème de comparaison, $\lim u_n = 1$.
   \\ \\
   Pour trouver la valeur de $a$ on regarde $ (u_n -1) \times n$ qui doit converger vers $a$. \\
   On a $n (u_n - 1) = \frac{n u_{n-1} }{n+1} \xrightarrow[ n \rightarrow +\infty]{} 1$ donc $a = 1$. \\
   Ensuite doit avoir $n[n(u_n -1) -1 ] \rightarrow b$ donc on étudie : \\
   $n[ n (u_n - 1) - 1] = n \left( \frac{n u_{n-1} }{n+1}  -1 \right) = n \frac{ n u_{n-1} - n -1}{n+1} \sim n u_{n-1} - n - 1 = n ( u_{n-1} - 1) -1 = (n-1 + 1) ( u_{n-1} -1 ) - 1 = (n-1) (u_{n-1} - 1) + u_{n-1} - 2 \xrightarrow[ n \rightarrow +\infty]{} 1 + 1 - 2 = 0$ donc $b = 0$. \\ \\
   On obtient $n[ n (u_n - 1) - 1] = o(1)$ donc $n (u_n -1) -1 = o \left( \frac{1}{n} \right)$, $n (u_n -1) = 1 + o \left( \frac{1}{n} \right)$, $u_n - 1 = \frac{1}{n} +o \left( \frac{1}{n^2} \right)$, et enfin $u_n = 1 + \frac{1}{n} + o \left( \frac{1}{n^2} \right)$.

 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Une variable suit la loi de Bernouilli si elle a deux issues possibles : le succès pour lequel elle vaut 1, et l'échec pour lequel elle vaut 0. \\
     On a alors $\Prob(\Ev{X=1}) = p= \E(X)$ et $\V(X) = p (1-p)$. \\
     On réalise une succession de $n$ épreuves de Bernouilli indépendantes et identiques de paramètre $p$, et on compte le nombre de succès : on obtient alors une loi binomiale de paramètres $n$ et $p$. \\
     La variable $X$ associée vérifie alors $X(\Omega) = \llb 0 ; b \rrb$. \\
     Pour calculer $\Prob(\Ev{X=k})$, on compte le nombre de possibilités amenant à ce résultat et la probabilité de chacune. \\
     Il faut obtenir $k$ succès et $n-k$ échecs : on place les $k$ succès parmi les $n$ épreuves pour obtenir toutes les possibilités : il y en a donc $\binom{n}{k}$. \\
     Dans chacun de ces cas, on obtient de manière indépendante $k$ succès et $n$ échecs avec une probabilité $p^k q^{n-k}$. \\
     On obtient alors $\Prob(\Ev{X=k}) = \binom{n}{k} p^k q^{n-k}$. \\
     L'espérance s'obtient en écrivant les $n$ $X_i$ variables de Bernouilli, avec la linéarité de l'espérance : $\E(X) = n p$. \\
     De même grâce à l'indépendance on calcule facilement la variance de la somme : $\V(X) = n p (1-p)$. \\ 
   \item $X_i$ suit une loi de Bernouilli, et $\Prob(\Ev{X_i = 1}) = \frac{ \binom{1}{1} \times \binom{2n-1}{n-1} }{ \binom{2n}{n} } = \frac{ (2n-1)! \times (n!)^2 }{(2n)! (n-1)! (n!)} = \frac{n}{ 2n } = \frac{1}{2 }$. \\
   \item $X_ i X_j$ est la variable de Bernouilli qui vaut 1 si la boule $i$ et la boule $j$ sont dans la poignée. \\
     D'où $\E( X_i X_j) = \Prob( X_i X_j = 1) = \frac{ \binom{2}{2} \times \binom{2n-2}{n-2} }{ \binom{2n}{n} } = \frac{ (2n-2)! \times (n!)^2 }{(2n)! (n-2)! (n!)} = \frac{n(n-1)}{ 2n (2n-1) } = \frac{n-1}{4n-2 }$. \\ \\
     On en déduit que $\Cov (X_i , X_j) = \frac{n-1}{4n-2} - \frac{1}{4} = \frac{ 2 n - 2 - ( 2n - 1) }{8n - 4} = -\frac{1}{4 (2n -1)}$. \\
   \item On note $S$ la variable aléatoire réelle prenant pour valeur la somme des numéros portés par les boules figurant dans la poignée. 
     \begin{enumerate}
     \item $S= \Sum{i=1}{n} i X_i$. \\
     \item Par linéarité de l'espérance, $\E(S) = \Sum{i=1}{n} \frac{i}{2} = \frac{1}{2} \times \frac{n (n+1)}{2} = \frac{n (n+1)}{4}$. \\
       Par indépendance des $X_i$ on a $\V( S) = \Sum{i=1}{n} i^2 \V(X_i) = \frac{1}{4} \times \frac{n (n+1) (2n-1)}{6} = \frac{ n (n+1) (2n-1)}{24}$. \\
     \end{enumerate}
   \item On note $Y = \Sum{i=1}{n} X_i$ qui compte le nombre de boules non numérotées $0$ dans la poignée, et qui suit une loi binomiale de paramètres $n$ et $\frac{1}{2}$. \\
     On a alors $Z = n - Y$ donc $Z(\Omega) = \llb 0 ; n \rrb$ et $\Prob(\Ev{Z=k}) = \Prob(\Ev{ Y= n-k}) = \binom{n}{k} \frac{1}{2^n}$. \\
     On voit que $Z$ suit une loi binomiale, et on a $\E(Z) = \frac{n}{2}$. \\
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $f$ la fonction définie par : 
   \[
   \forall (x ,y) \in \R^2,\ f(x,y) = x^3 + y^3 - 3 xy + 1.
   \] \begin{enumerate}
   \item Aucune difficulté ici : \\
     $f_x'(x,y) = 3 x^2 - 3 y$, $f_y'(x,y) = 3 y^2 - 3 x$, $f_{x,x}''(x,y) = 6 x$, $f_{x,y}''(x,y) = -3$ et $f_{y,y}''(x,y) = 6 y^2$. \\
   \item $f_x'(x,y) = f_y'(x,y) = 0$ donne $x^2 = y$ (donc $y$ positif) et $y^2 = x$ (donc $x$ positif), puis $y^2 = x^4 = x$ donc $x = 0$ ou $x=1$, puis $x=0$ donne $y=0$ et $x=1$ donne $y=1$. \\
     Les deux points critiques sont $(0,0)$ et $(1,1)$. \\
   \item En $(0,0)$ on a $r=t=0$ et $s=-3$ donc $rt - s^2 = -9 < 0$, c'est un point selle. \\
     En $(1,1)$ on a $r=t=6$ et $s = -3$ donc $rt-s^2 = 36-9=27>0$, avec $r>0$ donc c'est un minimum local.
   \end{noliste}
 \end{exercice}


 \newpage


 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item Un estimateur d'un paramètre $\theta$ de la loi $P_X$ d'une variable aléatoire $X$ dont on dispose d'un échantillon $(X_n)$ est une suite de variables aléatoires $(T_n)$ où pour tout $n$, $T_n$ est une fonction des variables $X_1 ,\ \dots\ , X_n)$. \\
     On définit alors son risque quadratique comme l'espérance des écarts à $\theta$ mis au carré : \\ $R = E ( [X - \theta]^2)$. \\ \\
   \item On pose, pour tout $n \in \N^*$, $M_n = \dfrac{1}{n} \Sum{k=1}{n} Z_k$. \\
     On a $Z_k (\Omega) = \llb 1 ; N \rrb$ et pour tout $i \in \llb 1 ; N \rrb$, $\Prob(\Ev{Z_k = i}) = \frac{1}{N}$. \\
     D'où $\E(Z_k) = \frac{1}{N} \Sum{i=1}{N} i = \frac{N+1}{2}$. \\
     On a donc par linéarité de l'espérance, $\E(M_n) = \frac{N+1}{2}$ et en posant $T_n = 2 M_n -1$, on obtient un estimateur $(T_n)$ sans biais de $N$, car $\E(T_n) = 2 \E(M_n) - 1 = N + 1 - 1 =N$. \\
     De plus on a $R(T_n) = b^2 + \V(T_n) = \V(T_n) = 4 \V(M_n) = \frac{4}{n^2} \times n \V(Z_1) = \frac{4}{n} \V(Z_k) \xrightarrow[n \rightarrow +\infty]{} 0$. \\
   \item On note $S_n = \max ( Z_1 , Z_2 ,\ \dots\ , Z_n)$.
     \begin{enumerate} 
     \item Pour tout $x \in \R$, on a $F_{S_n} (x) = \Prob(\Ev{ S_n \leq x}) = P( \max (Z_1 ,\ \dots\ , Z_n ) \leq x ) = P \left( \bigcap\limits_{k=1}^n \Ev{ Z_k \leq x} \right) = \prod\limits_{k=1}^n F_{Z_k} (x) = \left( F_{Z_1} (x) \right)^n$. \\
       Or pour tout $k \in \llb 1 ; N \rrb$ on a $F_{Z_1} (k) = \Prob(\Ev{ Z_1 \leq k}) = \frac{k}{N}$ donc pour tout $k \in \llb 1 ; N \rrb$, $F_{S_n} (k) = \left( \frac{k}{N} \right)^n$. \\
     \item $ \Sum{k=1}{N} \Prob(\Ev{ Y \geq k}) = \Sum{k=1}{N} \Sum{i=k}{N} \Prob(\Ev{ Y=i}) = \sum\limits_{ i \geq k , 1 \leq k \leq N, 1 \leq i \leq N} \Prob(\Ev{ Y=i}) = \sum\limits_{ k \leq i , 1 \leq k \leq N, 1 \leq i \leq N} \Prob(\Ev{ Y=i})  = \Sum{i=1}{N} \Sum{k=1}{i} \Prob(\Ev{ Y=i}) = \Sum{i=1}{N} i \Prob(\Ev{ Y=i}) = \E(Y) $. \\
     \item On a donc $\E(S_n ) = \Sum{k=1}{N} \Prob(\Ev{ S_n \geq k}) = \Sum{k=1}{N} [ 1 - \Prob(\Ev{ S_n < k}) ] = N - \Sum{k=1}{N} \Prob(\Ev{ S_n \leq k -1})  = N  - \Sum{k=1}{N} \left( \frac{k-1}{N} \right)^{n}$. \\ \\
       $\E(S_n) \geq N - \frac{N}{n+1}$ ???? J'obtiens la majoration $\E(S_n) \geq N - N  \left( 1 - \frac{1}{N} \right)^n$ qui donne le résultat pour la question suivante, mais je ne vois absolument pas comment obtenir la majoration demandée. \\  
     \item $S_n$ est n estimateur de $N$ par définition, et on a $N \geq \E(S_n) \geq N - \frac{N}{n+1} \xrightarrow[n \rightarrow +\infty]{} N$ donc par théorème d'encadrement, $\E(S_n) \rightarrow N$. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $A$ la matrice de $\mathcal{M}_3 (\R)$ telle que :
   \[
   A = \begin{smatrix} 0 & 1 & -1 \\ -1 & 2 & -1 \\ 1 & -1 & 2 \\ \end{smatrix}. 
   \] 
   \begin{enumerate}
   \item \begin{enumerate}
     \item On calcule $A^2$ et on obtient $A^2 = 3 A - 2I$. \\
     \item On en déduit que $A \left[ \frac{1}{2} ( 3I - A) \right] = I$ donc $A$ est inversible et $A^{-1} = \frac{1}{2} ( 3I - A)$. \\
     \end{enumerate}
   \item Le polynôme $P(x) = x^2 - 3 x + 2$ est annulateur de $A$ et a pour racines $1$ et $2$ donc $\spc A \subset \{ 1 ; 2\}$.
   \item On a $A - I = \begin{smatrix} -1 & 1 & -1 \\ -1 & 1 & -1 \\ 1 & -1 & 1 \\ \end{smatrix}$ est de rang 1 (toutes les colonnes sont colinéaires) donc $1$ est valeur propre et $\dim E_1 (A) = \dim \ker (A-I) = 2$ par théorème du rang. \\
     On a $A - 2I = \begin{smatrix} -2 & 1 & -1 \\ -1 & 0 & -1 \\ 1 & -1 & 0 \\ \end{smatrix}$ n'est pas inversible car $C_1 + C_2 - C_3 = 0$ donc $2$ est valeur propre et $A$ est de rang 2 car les deux premières colonnes ne sont pas colinéaires, donc $\dim E_2 (A) = \dim \ker (A-2I) = 1$ par théorème du rang. \\ \\
     Enfin la somme des dimensions des sous-espaces propres vaut 3 donc $A$ est diagonalisable.
   \end{noliste}
 \end{exercice}

 \newpage


 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Dans cet exercice, on note $C^0$ l'espace vectoriel des fonctions continues de $\R$ dans $\R$.
   \begin{enumerate}
   \item Soit $u$ un endomorphisme de $E$, un réel $\lambda \in \R$ est valeur propre de $u$ s'il existe $x \in E$ non nul tel que $u(x) = \lambda x$. Tout vecteur non nul vérifiant $u(x) = \lambda x$ est appelé vecteur propre de $u$ associé à la valeur propre $\lambda$. \\ \\
     Soit $\Phi$ l'application définie sur $C^0$ qui, à toute fonction $f$ de $C^0$, associe la fonction $g = \Phi(f)$ définie par :
     \[
     \forall x \in \R,\ \ g(x) = \int_0^x f(t)\ dt.
     \]
   \item $f$ est continue donc admet des primitives, donc en notant $F$ une primitive de $f$ on a $\Phi (f) (x) = F(x) - F(0)$ est dérivable, de dérivée $f$. \\
   \item Pour tout $f$, $\Phi(f)$ est dérivable sur $\R$ donc continue sur $\R$, et $\Phi (f) \in C^0$. \\
     La linéarité est évidente par linéarité de l'intégrale. \\
   \item La fonction valeur absolue est une fonction continue sur $\R$ et non dérivable sur $\R$, car elle n'est pas dérivable en 0. \\
     $\Phi$ n'est donc pas surjective puisque la fonction valeur absolue, qui est dans $C^0$, ne peut être atteinte par $\Phi$. \\
     Pour l'injectivité, on résout $\Phi (f) = 0$. \\
     Supposons $\Phi (f) = 0$, alors $f = \Phi'(f) = 0$ donc $\ker \Phi = \{ 0 \}$ et $\Phi$ est injective. \\ \\
     Soit $\lambda$ un réel quelconque. On dit que $\lambda$ est une valeur propre de $\Phi$ s'il existe une fonction $f$ non nulle de $C^0$, telle que $\Phi(f) = \lambda f$. Une telle fonction est appelée fonction propre associée à la valeur propre $\lambda$. \\
   \item Recherche des valeurs propres non nulles de $\Phi$. \\
     On suppose, dans cette question, que $\Phi$ admet une valeur propre $\lambda$ non nulle. \\ \\
     Soit $f$ une fonction propre associée à $\lambda$. 
     \begin{enumerate}
     \item $\Phi (f) = \lambda f$ est dérivable sur $\R$ donc $f = \frac{1}{\lambda} \Phi (f)$ est dérivable sur $\R$. \\
     \item $h'(x) = e^{ - \frac{x}{\lambda} } \left( f'(x) - \frac{1}{\lambda} f(x) \right)$. \\
       Or $\Phi(f) '(x) = f(x)$ donc $(\lambda f)'(x) = f(x)$ et enfin $f'(x) = \frac{1}{\lambda} f(x)$, donc $h'(x) = 0$, et $h(x)$ est égale à une constante $K$, et enfin $f(x) = K e^{ \frac{x}{\lambda} }$. \\
       Or on a $\Phi (f) (0) = \lambda f(0) = \int_0^0 f(t)\ dt = 0$ donc $f(0) = 0$, $K=0$ et enfin $f(x)=0$ pour tout $x \in \R$. \\
     \item La seule valeur propre possible est donc 0. \\
       Or on a vu que $\ker \Phi = \{ 0 \}$ donc $0$ n'est pas valeur propre, et $\Phi$ n'admet donc aucune valeur propre. \\ 
     \end{enumerate}
   \item Pour toute fonction $f$ de $C^0$, on pose : $F_0 = \Phi(f)$ et $\forall n \in \N^*,\ F_n = \Phi ( F_{n-1} )$. \begin{enumerate}
     \item Par récurrence évidente on obtient que $F_n$ est de classe $C^n$, puis on écrit $F_n ( 0) = \Phi (F_{n-1} ) (0) = \int_0^0 F_{n-1} (t)\ dt = 0$. Seule $F_0 (0) = f(0)$ peut être différent de 0. \\
       Enfin pour tout $k \in \llb 0 ; n \rrb$, $F_n^{(k)} =F_{n-k}$ donc $F_n^{(k)} (0)= 0$ pour $0 \leq k \leq n-1$ et $F_n^{(n)} (0) = f(0)$. \\
     \item $F_n$ est de classe $C^n$, on utilise la formule de Taylor avec reste intégral à l'ordre $n-1$, qui donne immédiatement le résultat puisque les $n-1$ premières dérivées en 0 de $F_n$ sont nulles. \\
     \end{enumerate}
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   $X$ et $Y$ sont deux variables aléatoires réelles indépendantes définies sur le même espace probabilisé $(\Omega , \mathcal{A} , P)$ et ayant la même loi de densité $\varphi$, définie par :
   \[
   \forall x \in \R,\ \ \ \varphi(x) = k e^{ - \vert x \vert}.
   \]
   \begin{enumerate}
   \item $\phi$ est continue et positive sur $\R$, il faut que $\int_{-\infty}^{+\infty} \phi(t)\ dt = 1$. \\
     Or $\phi$ est paire donc il suffit de prouver que $\int_0^{+\infty} \phi (t)\ dt =\frac{1}{2}$. \\
     On a $\int_0^{+\infty} \phi (t)\ dt = k \int_{0}^{+\infty} e^{-t}\ dt = k$ (loi exponentielle de paramètre $\lambda =1$), donc il faut que $k = \frac{1}{2}$. \\  
   \item Pour tout $x \leq 0$, on a $F (x) = \int_{-\infty}^x \phi(t)\ dt = \int_{-\infty}^0 f(t)\ dt + \int_0^x \phi(t)\ dt = \frac{1}{2} + \frac{1}{2} \int_0^x e^{t}\ dt = \frac{1}{2} + \frac{1}{2} ( e^x - 1) = \frac{1}{2} e^x$. \\
     Pour tout $x > 0$, $F(x) =  \int_{-\infty}^x \phi(t)\ dt = \int_{-\infty}^0 f(t)\ dt + \int_0^x \phi(t)\ dt = \frac{1}{2} + \frac{1}{2} \int_0^x e^{-t}\ dt = \frac{1}{2} + \frac{1}{2} ( -e^{-x} + 1) = 1 - \frac{1}{2} e^{-x}$. \\
   \item Par parité de $f$ on obtient que le moment d'ordre 2 existe si et seulement si $\int_0^{+\infty} \phi (t)\ dt$ existe, ce qui est le cas (on reconnaît le moment d'ordre deux de la loi exponentielle de paramètre $\lambda = 1$). \\
     On en déduit que $e(X)$ et $\V(X)$ existent. \\
     De plus la fonction $t \rightarrow t \phi(t)\ dt$ est impaire donc $\E(X) = \int_{-\infty}^{+\infty} t \phi (t)\ dt = 0$. \\
     Enfin la fonction $t \rightarrow t^2 \phi(t)\ dt$ est paire donc $\E(X^2) = 2 \int_0^{+\infty} t^2 \phi (t)\ dt = \int_0^{+\infty} t^2 e^{-t}\ dt = \E(Y^2) = \V(Y) + [ (\E(Y)]^2$, où $Y \suit \mathcal{E} (1)$, donc : \\
     $\V(X) = \E(X^2) - [\E(X)]^2 = \V(Y) + [\E(Y)]^2 = \frac{1}{1^2} + \frac{1}{1^2} = 2$. 
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~\\
   Pour tout nombre réel $a$, on note $A(a)$ la matrice
   \[
   A(a) = \begin{smatrix} 2 & 1 & a \\ 1 & 1+a & 1 \\ a & 1 & 2 \\ \end{smatrix} 
   \]
   \begin{enumerate}
   \item \begin{enumerate}
     \item Une matrice est diagonalisable si et seulement si elle est semblable à une matrice diagonale. \\
     \item Soit $M$ une matrice diagonalisable, et $D$ diagonale et $P$ inversible telles que $M = P D P^{-1}$. \\
       On a alors ${}^t M ={}^t(P D P^{-1} ) = {}^t (P^{-1}) {}^t D {}^t P = ({}^t P)^{-1} D {}^t P$ donc ${}^tM$ est diagonalisable car elle est semblable à la matrice diagonale $D$. \\
     \end{enumerate} 
   \item 
     \begin{enumerate}
     \item $A(a)$ est symétrique donc diagonalisable. \\
     \item $A(a) - a I = \begin{smatrix} 2 & 1 & a \\ 1 & 1+a & 1 \\ a & 1 & 2 \\ \end{smatrix}$ n'est pas inversible car $C_1 + C_3 - 2C_2 = 0$ donc $a$ est valeur propre de $A(a)$. \\
       D'autre part la matrice obtenue est de rang 1 si et seulement si les trois colonnes sont colinéaires ce qui donne (la 2e ligne vaut 1 pour les trois) que les trois colonnes sont égales et donc $a = 2- a =1$. \\
       D'où pour $a=1$ la matrice est de rang 1 et le sous-espace propre est de dimension 2 par théorème du rang, égale à $\begin{smatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \\ \end{smatrix}$ donc vérifie que $f(e_2 - e_1) = f(e_3 - e_1) = 0$ et $(e_2 - e_1 , e_3 - e_1)$ est libre car échelonnée donc c'est une base de $E_{a} (A(a) )$. \\
       Pour $a \neq 1$ la matrice est de rang 2 donc $E_a (A(a) )$ est de dimension 1, et la relation $f(e_1) + f(e_3) - 2 f(e_2) = 0$ donne $f(e_1 - 2 e_2 + e_3)=0$ donc $(e_1 - 2e_2 + e_3)$ famille libre (car d'un vecteur non nul) de $E_a (A(a) )$ donc c'est une base de $E_a (A(a) )$.
       \\
     \item $A(a) \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix} = (3+a) \begin{smatrix} 1 \\ 1 \\ 1 \\ \end{smatrix}$ et $A(a) \begin{smatrix} 1 \\ 0 \\ -1 \\ \end{smatrix} = (2-a) \begin{smatrix} 1 \\ 0 \\ -1 \\ \end{smatrix}$. \\
     \item Dans tous les cas, la famille $[ ( 1 , -2 , 1) , (1 , 1 , 1) , (1, ,0 , -1)]$ est une base de vecteurs propres de $A$ donc on peut diagonaliser dans cette base : \\
       On pose $P =  \begin{smatrix} a & 0 & 0 \\ 0 & 3+a & 0 \\ 0 & 0 & 2-a \\ \end{smatrix}$ et $D =  \begin{smatrix} 2 & 1 & a \\ 1 & 1+a & 1 \\ a & 1 & 2 \\ \end{smatrix}$ et on a $A(a) = P D P^{-1}$. \\
     \end{enumerate}
   \item Soit $(x_n)_{n \in \N}$, $(y_n)_{n \in \N}$, $(z_n)_{n \in \N}$ trois suites réelles vérifiant, pour tout $n$ entier naturel,
     \[
     \left\{ \begin{array}{l} x_{n+1} = 2 x_n + y_n \\ y_{n+1} = x_n + y_n + z_n \\ z_{n+1} = y_n + 2 z_n \\ \end{array} \right. 
     \]
     \begin{enumerate}
     \item $X_{n+1} = A(0) X_n$. \\
     \item Les valeurs propres de $A(0)$ sont 0 , 2 et 3; or on aura $X_n = A(0)^n X_0 = P D^n P^{-1} X_0$ avec $D^n$ comportant les valeurs 0 , $2^n$ et $3^n$ sur la diagonale; il faut donc que celles-ci ne rentrent pas en compte. \\
       Chaque suite s'écrit comme combinaison linéaire de $0^n$, $2^n$ et $3^n$, il faut donc qu'elles ne soient combinaisons linéaires que de $0^n$, donc que $x_n = y_n = z_n = 0$ pour $n \geq 1$, c'est-à-dire que $X_1 = A (0) X_0 =0$ donc $X_0 \in \ker A(0)$, donc $X_0 \in \Vect[ (1 , -2 , 1)]$. \\
       La condition cherchée est donc $x_0 + z_0 - 2 y_0 = 0$. \\
     \end{enumerate}
   \item
     \begin{enumerate} 
     \item $B = P B' P^{-1}$ et $C^2 = B$, donc $B' = P^{-1} B P = P^{-1} C^2 P = ( P^{-1} C P)^2$ donc avec $C' = P^{-1} C P$ on a bien $C'^2 = B'$. \\
     \item $ BC = C^2 C = C^3 = C C^2 = C B$. \\
     \item $\begin{smatrix} 3 & 0 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & -1 \\ \end{smatrix} \begin{smatrix} a & b & c \\ d & e & f \\ g & h & i \\ \end{smatrix} = \begin{smatrix} a & b & c \\ d & e & f \\ g & h & i \\ \end{smatrix} \begin{smatrix} 3 & 0 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & -1 \\ \end{smatrix} \Leftrightarrow \begin{smatrix} 3a & 3b & 3c \\ 6d & 6e & 6f \\- g & -h & -i \\ \end{smatrix} = \begin{smatrix} 3a & 6b & -c \\ 3d & 6e & -f \\ 3g & 6h & - i \\ \end{smatrix} \Leftrightarrow b=c=d=f=g=h=0 \Leftrightarrow \begin{smatrix} a & b & c \\ d & e & f \\ g & h & i \\ \end{smatrix} = \begin{smatrix} a & 0 & 0 \\ 0 & e & 0 \\ 0 & 0 & i \\ \end{smatrix} $ est diagonale. \\
     \item D'après les questions précédentes cela revient à cherchez une matrice $N$ vérifiant $N^2 = \begin{smatrix} 3 & 0 & 0 \\ 0 & 6 & 0 \\ 0 & 0 & -1 \\ \end{smatrix} = D$ qui est semblable à $A(3)$, et $N$ commute alors avec $D$, donc d'aptrèsd la question précédente il faut la chercher diagonale. \\
       En posant $N= \operatorname{diag} ( a , b , c)$ on obtient $N^2 = \operatorname{diag}(a^2 , b^2 , c^2) = \operatorname{diag}( 3 , 6 , -1)$ ce qui est impossible car un carré est toujours positif. Il n'y a donc pas de solution à l'équation matricielle $M^2 = A(3)$.  \\
     \end{enumerate} 
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\
   \begin{enumerate}
   \item A priori on en demande pas de vérifier que c'est une fonction de répartition de variable à densité. \\
     Il faut prouver que $F$ est croissante (évident avec sa dérivée), de limites 0 en $-\infty$ (évident) et 1 et $+\infty$ (évident encore) et continue à droite (évident puisque $F$ est continue sur $\R$). Cela ne coûte pas grand-chose de préciser que $F$ est de classe $c^1$ sur $\R$ et donc continue sur $\R$ donc que c'est une fonction de répartition de variable à densité. \\
   \item Pour cette question classique on obtient $G(x) = F(x)^2 = \frac{1}{ (1+ e^{-x} )^2}$ et $G_n (x) = F(x)^n = \frac{1}{(1+ e^{-x})^n}$. 
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice}{\it (Exercice avec préparation)}~
   \begin{noliste}{1.}
   \item $f(b) = \Sum{k=0}{n} \frac{f^{(k)} (a)}{k!} (b-a)^k + \int_a^b \frac{(b-t)^n}{n!} f^{n+1} (t)\ dt$. \\ \\
     Soit $f$ la fonction définie sur $\R$ par $f(x) = e^{-x^2}$, et $F$ la primitive de $f$ qui vérifie $F(0) = 0$. \\
   \item $F$ est dérivable sur $\R$, et $F'(x) = f(x) = e^{ - x^2} > 0$ donc $F$ est strictement croissante sur $\R$, de plus par parité de $f$, $F(x) =\int_0^x f(t)\ dt$ est impaire, passe par 0 en 0 et sa limite en $+\infty$ vaut $\int_0^{+\infty} e^{-x^2}\ dt = \frac{\sqrt{\pi}}{\sqrt{2} }$ (avec la loi normale). \\
   \item \begin{enumerate}
     \item L'intégrale n'est pas généralisée et la fonction intégrée est continue, l'intégrale existe bien. \\
       On définit alors la fonction $G$ par :
       \[
       G(x) = \int_0^1 e^{-(xt)^2}\ dt. 
       \]
     \item Le changement de variable $u = xt$ donne $G(x) =\int_0^x \frac{1}{x} e^{ -u^2}\ du = \frac{1}{x} \int_0^x e^{-u^2}\ du = \frac{F(x)}{x}$. \\
       On en déduit que $G$ est dérivable sur $\R^*$ et que $G'(x) = \frac{x F'(x) - F(x) }{x^2} = \frac{x e^{-x^2} - F(x)}{x^2}$. \\
       Le signe de $G'$ est celui de $x e^{-x^2} - F(x) = x e^{-x^2} - \int_0^x e^{-t^2}\ dt$, qui est impaire comme somme de deux fonctions impaires. Etudions son signe sur $\R_+^*$ :  \\
       Pour tout $t \in [0 ; x]$, $ 0 \leq t \leq x$ donc $0 \leq t^2 \leq x^2$ et $- x^2 \leq -t^2 \leq 0$, on compose par exp qui est croissante pour obtenir $e^{-x^2} \leq e^{-t^2}$ et enfin $\int_0^x e^{-x^2}\ dt = x e^{-x^2} \leq \int_0^x e^{-t^2}\ dt = F(x)$, donc $G'(x) \leq 0$. \\
       On en déduit par imparité que $G'(x) \geq 0$ sur $\R_-*$, puis que $G$ est croissante sur $\R_-^*$ et décroissante sur $\R_+^*$. \\
     \item $\dlim{x \rightarrow 0, x \neq 0} G(x) = \dlim{x \rightarrow 0, x \neq 0} \frac{F(x)}{x} = \dlim{x \rightarrow 0, x \neq 0} \frac{ F(x) - F(0)}{x-0}$, or $F$ est dérivable en 0 donc $\dlim{x \rightarrow 0, x \neq 0} G(x)$ existe et vaut $\dlim{x \rightarrow 0, x \neq 0} G(x) = F'(0) = f(0) = 1$. \\
       D'autre part $G(0) = \int_0^1 e^{0}\ dt = 1$ donc $G$ est continue en 0. \\
       D'autre part $\dlim{x \rightarrow +\infty} F(x) = \sqrt{ \frac{ \pi }{2} }$ est finie donc par quotient de limites, $\dlim{x \rightarrow +\infty} G(x) = \dlim{x \rightarrow +\infty} \frac{F(x)}{x} = 0$. \\
     \item On étudie $\dlim{x \rightarrow 0, x \neq 0} G'(x)$ pour conclure avec le théorème de prolongement de la dérivée. \\
       On a $G'(x) = \frac{ \int_0^x ( e^{-x^2} - e^{-t^2} )\ dt }{x^2}$, on étudie donc la fonction $f(x) - f(t)$. \\
       La formule de Taylor avec reste intégral à l'ordre 2 donne $f(t) -f(x) = f'(x) (t-x) + f''(x) \frac{(t-x)^2}{2} + \int_x^t \frac{(u-x)^2}{2} f''(u)\ du$ donc $f(x) - f(t) = f'(x) (x-t) - f''(x) \frac{(x-t)^2}{2} + g(x,t)$, avec $\vert g(x) \vert \leq \int_t^x \frac{x^2}{2} \times M\ du \leq \frac{x^3 M}{2}$, où $M$ est un majorant de $\vert f''(u) \vert$ sur $[0 ; x]$ ou $[x ; 0]$. \\ \\
       On obtient alors $\int_0^x ( e^{-x^2} - e^{-t^2} )\ dt = f'(x) \int_0^x (x-t)\ dt - \frac{f''(x)}{2} \int_0^x ( x-t)^2\ dt + \int_0^x g(x,t)\ dt = f'(x) \frac{x^2}{2} - \frac{f''(x)}{2} \frac{x^3}{3} + h(x)$, avec $\vert h(x) \vert \leq \vert \int_0^x M \frac{x^3}{2} \vert = M \frac{ \vert x^4 \vert}{2}$. \\
       Enfin on obtient $G'(x) = \frac{f'(x)}{2} + o (1)$, donc $ \dlim{x \rightarrow 0, x \neq 0} G'(x) = \frac{f'(0)}{2} = 0$. \\ \\
       La fonction $G'$ admet donc une limite finie en 0 à gauche et à droite et celles-ci sont égales; le théorème de prolongement de la dérivée permet de conclure, et donne $G'(0)= 0$. \\ \\ \\
       Autre possibilité : écrire puis sommer les développements limités de $x f(x)$ et $F(x)$ obtenus à l'aide de la formule de Taylor.
     \end{enumerate}
   \item
     \begin{enumerate} 
     \item Pour tout $x \neq 0$, on a $x G'(x) + G(x) = \frac{x f(x) - F(x)}{x} + \frac{F(x)}{x} = \frac{x f(x)}{x} = f(x)$. \\
       De plus comme les fonction $x \rightarrow x G'(x) + G(x)$ et $x \rightarrow f(x)$ sont continues, on obtient par continuité en 0 la relation en $x =0$. \\
     \item On veut prouver que $G$ est l'unique fonction $g$ dérivable sur $\R$ telle que : 
       \[
       \forall x \in \R,\ x g'(x) + g(x) = f(x) \ \ \ (E). 
       \]
       Soit $G_1$ une fonction réelle dérivable sur $\R$ et vérifiant l'équation $(E)$. On pose $H = G - G_1$. Déterminer $H(x)$ pour $x > 0$ puis pour $x < 0$. conclure en utilisant la continuité de $H$ en 0. \\ \\
       On a $H(x) = G (x) - G_1 (x)$ vérifie $x H'(x) + H(x) = 0$, donc en posant $A(x) = x H(x)$ on a $ A'(x) = x H'(x) + H(x) = 0$, donc $A(x)$ est constante égale à $K$ sur $\R$. \\ \\
       On en déduit que $H(x) = \frac{K}{x}$ sur $\R_+^*$ et sur $\R_-^*$, et la continuité de $H$ en 0 impose que $\frac{K}{x}$ admette une limite finie en 0. Or si $K \neq 0$, on obtient une limite infinie : ceci impose que $K = 0$, puis $H(x) = 0$ sur $\R^*$ puis sur $\R$ par continuité de $H$. \\ \\
       Enfin on obtient $G_1 (x) = G(x)$, et $G$ est bien l'unique solution de l'équation différentielle $(E)$. \\
     \end{enumerate} 
   \end{enumerate}
   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Les variables aléatoires considérées dans cet exercice sont définies sur un espace probabilisé $(\Omega , \mathcal{A} , P)$. Soit $a$ un réel strictement positif et $X$ une variable aléatoire de loi uniforme sur $[0 ; 2a]$. \begin{enumerate}
   \item Soit $n \in \N^*$. On considère $n$ variables aléatoires indépendantes $X_1,\ \dots\ , X_n$ qui ont toutes la même loi que $X$. On pose : 
     \[
     M_n = \max ( X_1 ,\ \dots\ , X_n). 
     \]
     De manière classique $F_{M_n} (x) = F_X(x)^n = \begin{cases} 0 \text{ si } x \leq 0 \\ \frac{x^n}{(2a)^n} \text{ si } 0 \leq x \leq 2a \\ 1 \text{ si } x \geq 1 \end{cases}$. \\
     On en déduit que $f_{M_n} (x) = 0$ si $x \notin [ 0 ; 2a]$, et $f_{M_n} (x) = n \frac{x^{n-1}}{(2a)^n}$ sinon. \\ \\
     Ensuite on a $\E(M_n) = \frac{n}{(2a)^n} \int_0^{2a} x^n\ dx = \frac{ n (2a)^{n+1} }{ (n+1) (2a)^n} = \frac{2a n}{n+1}$. \\
     De même $\E(M_n^2) = \frac{ n (2a)^{n+2} }{(n+2) (2a)^n} = \frac{4 a^2 n}{n+2}$, et enfin : \\
     $\V(M_n) = \frac{4 a^2 n}{n+2} - \frac{4a^2 n^2}{(n+1)^2}$ donc $\V(M_n) = 4 a^2 n \left( \frac{1}{n+2} - \frac{n}{(n+1)^2} \right)$. \\
   \item On a $\E(U_n) = \frac{n+1}{2n} \E(M_n) = a = \E(X)$ donc $U_n$ est un estimateur sans biais de $a = \E(X)$. \\
     Pour comparer les estimateurs, on compare leurs risques quadratiques : \\
     $R (V_n) = V (V_n) = \frac{1}{n^2} \times n \V(X) = \frac{\V(X)}{n} = \frac{ 4 a^2}{12 n} \rightarrow 0$ et proportionnel à $\frac{1}{n}$. \\
     D'autre part $R(U_n) = V ( U_n) = \frac{(n+1)^2}{4 n^2} \V(M_n) = \frac{a^2}{n} \left( \frac{(n+1)^2}{n+2} - n \right) = a^2 \left(  \frac{(n+1)^2 - n (n+2)}{n (n+2)} \right) = a^2 \left(  \frac{1}{n (n+2)} \right) \rightarrow 0$ mais proportionnel à $\frac{1}{n^2}$, donc il tend plus vite vers 0. \\
     $U_n$ est donc un meilleur estimateur que $V_n$ de $a = \E(X)$.
   \end{noliste}
 \end{exercice}

 \newpage

 \section{Annales 2007}

 %\setcounter{exercice}{0}

 \begin{exercice} \indent \\ \\
   Question de cours : un estimateur d'un paramètre $\theta$ de la loi $P_X$ d'une variable aléatoire $X$ dont on dispose d'un échantillon $(X_n)$ est une suite de variables aléatoires $(T_n)$ où pour tout $n$, $T_n$ est une fonction des variables $X_1 ,\ \dots\ , X_n)$. \\
   On définit alors son biais comme $\E(X - \theta)$, et son risque quadratique comme l'espérance des écarts à $\theta$ mis au carré : \\ $R = E ( [X - \theta]^2)$. \\ \\
   On considère $n$ ($n>2$) variables aléatoires réelles indépendantes
   $X_{1}$, ... , $X_{n}$ de m\^{e}me loi de densité
   \begin{equation*}
     f_{0}\left( x\right) =\left \{ 
       \begin{array}{cc}
         \dfrac{3x^{2}}{\theta ^{3}} & \text{si }x\in \left[ 0,\theta \right] \\ 
         0 & \text{sinon}%
       \end{array}%
     \right.
   \end{equation*}%
   o\`{u} $\theta $ est un paramètre strictement positif. On pose%
   \begin{equation*}
     S=X_{1}+\cdots +X_{n}\text{\quad et\quad }T=\max \left( X_{1},\cdots
       ,X_{n}\right)
   \end{equation*}

   \begin{enumerate}
   \item $\E(X) = \frac{1}{\theta^3} \int_0^x 3 x^3\ dx = \frac{3}{4} \theta$ donc $\E(S) = \frac{3}{4} n \theta$. \\
     $\E(X^2) = \frac{1}{\theta^3} \int_0^x x^4\ dx = \frac{3}{5} \theta^2$. \\
     Enfin $\V(X) = \theta^2 \left( \frac{3}{5} - \frac{9}{16} \right) = \theta^2 \frac{48 - 45}{80} = \frac{3}{80} \theta^2$ donc $\V(S) = n \V(X) = \frac{3}{80} n \theta^2$. \\

   \item $\Prob(\Ev{ T \leq t}) = P \Ev{ X \leq t}^n = 0$ si $x <0$, $1$ si $x > \theta$ et $\left( \frac{x}{\theta} \right)^{3n}$ si $0 \leq x \leq \theta$. \\ \\
     On en déduit une densité $f_T(x) = 0$ si $x \notin [0 ; \theta]$ et $f_T(x) = \frac{3n x^{3n-1} }{ \theta^{3n} }$. \\ \\
     Ensuite $\E(T) = \int_0^{\theta} t f_T(t)\ dt =  \frac{3n \theta }{(3n+1) }$. \\ \\
     De même $\E(T^2) =  \frac{3 n  \theta^{2} }{ (3n+2) }$, et enfin : \\ \\
     $\V(T) =\frac{3n \theta^{2} }{ (3n+2) } -   \frac{9 n^2  \theta^{2} }{ (3n+1)^2 }$. \\

   \item On suppose maintenant que $\theta $ est un paramètre inconnu qu'on
     se propose d'estimer.

     \begin{enumerate}
     \item Les variables $S' = \frac{4}{3n}  S$ et $T' = \frac{3n+1}{3n} T$ sont des estimateurs sans biais de $\theta$. \\
       On a $\V(S') =  \frac{16}{9 n^2} \V(S) = \frac{\theta^2}{15 n}$ et $\V(T') = \frac{(3n+1)^2}{9 n^2} \V(T) =  \frac{ (3n+1)^2 \theta^{2} }{ 3n (3n+2) } -  \theta^{2} = \theta^2  \frac{(3n+1)^2 - 3n (3n+2)  }{3n (3n+2)}$. \\
       $\V(T') = \frac{ \theta^2}{3n (3n+2)}$. \\

     \item Les deux risques quadratiques tendent vers 0 mais le premier est équivalent à $\frac{\theta^2}{15 n}$ et le deuxième à $\frac{\theta^2}{ 9 n^2}$, donc $\V(T_n')$ converge plus rapidement vers 0, et $T'$ est donc le meilleur des deux estimateurs.
     \end{enumerate}
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Un agriculteur souhaite améliorer le rendement de son exploitation en
   utilisant de l'engrais. Une étude a montré que le rendement, en
   tonnes par hectare, pour la variété de blé cultivée est donn%
   é par%
   \begin{equation*}
     f\left( B,N\right) =120B-8B^{2}+4BN-2N^{2}
   \end{equation*}%
   $B$ désigne la, quantité de semences de blé utilisée, $N$ la
   quantité d'engrais utilisée.

   \begin{enumerate}
   \item $f$ est de classe $C^2$, et on a : \\
     $f_B'(B,B) = 120 - 16 B+ 4N$, $f_N'(B,N) = 4 B - 4 N$, $f_{B,B}''(B,N) = -16$, $f_{B,N}''(B,N) = 4$ et $f_{N,N}'' (B,N) = -4$. \\ \\
     Les points vérifient $120 - 16 B + 4 N =0$ et $4B - 4 N=0$, donc $B=N$ et $120-12B=0$, et enfin $B=N=10$. \\
     Au point $(10,10)$ on a $r = -16$, $s = 4$ et $t = -4$ donc $rt - s^2 = 64 - 16 = 48 > 0$ et $r = -16 < 0$, c'est donc un maximum local. \\
     De plus on peut écrire $f(B,N) = 120 B - 6 B^2 -2 ( B^2 - 2 B N + N^2) = 120 B - 6 B^2 - 2 ( B- N)^2 = -6 [ (B-10)^2 -100]- 2 (B-N)^2 = -6 (B-10)^2 - 2 (B-N)^2 + 600 = -6 (B-10)^2 - 2 (B-N)^2 + f (10,10)$ donc $(10,10)$ est un maximum global de $f$. \\

   \item On traduit la contrainte : on a $B = 23 - 2N$ donc le rendement est donné par $g(N) = f( 23 - 2N , N) = 120 (23 - 2N) - 8 (23 - 2 N)^2 + 4 (23 - 2N) N - 2 N^2 = N^2 ( -32 -8 - 2) + N ( - 240 + 736 + 92) + (120 - 184) \times 23 = -42 N^2 + 588 N - 64 \times 23$. \\
     On dérive : $g'(N) = - 84 N + 588$, qui s'annule pour $N = \frac{588}{84} = \frac{294}{42} = \frac{147}{21} = \frac{21}{3} = 7$ et $B = 23 - 14 = 9$ donc le rendement optimum vaut $f( 9 , 7) = 1080 - 648 + 252 - 98 = 432 + 154 = 586$.
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice} \indent \\ \\
   Soit $E$ un $\R$-espace vectoriel de dimension finie. On note pour
   tout endomorphisme $u$ de $E$ et pour tout $\in \N^*,$ $u^{0}=%
   \mathrm{Id}_{E}$ et 
   \begin{equation*}
     u^{r}=\underset{r\text{ termes}}{\underbrace{u\circ \cdots \circ u}}
   \end{equation*}

   \noindent On commence par considérer un endomorphisme non nul de $E$ tel que pour
   tout $x\in E$ il existe $r\left( x\right) \in \N^*$ tel que $%
   u^{r\left( x\right) }=0$

   \begin{enumerate}
   \item Une matrice carrée d'ordre $n$ admettant $n$ valeurs propres distinctes est diagonalisable. Une matrice telle que la somme des dimensions de ses sous-espaces propres vaut $n$ est aussi diagonalisable. On peut encore citer la base de vecteurs propres. \\

   \item On se donne une base $(e_1 ,\ \dots\ , e_n)$ de $E$, et on pose $r_0 = \max \{ r( e_i)\ \vert\ 1 \leq i \leq n \}$. \\
     Alors pour tout $i$, $u^{r_0} (e_i) = 0$ et pour tout $x \in E$ s'écrivant $\Sum{i=1}{n} x_i e_i$, on a : \\
     $u^{r_0} (x) = \Sum{i=1}{n} x_i u^{r_0} (e_i) = \Sum{i=1}{n} 0 = 0$. \\
     Soit maintenant $A = \{ k \in \N\ \vert u^k = 0 \}$. \\
     $A$ est un sous-ensemble de $\R$ non vide donc il admet un minimum. On note $r$ ce minimum, on a alors $r-1 \notin A$ donc $u^{r-1} \neq 0$ et $u^r = 0$. \\
     Enfin comme $u \neq 0$ on sait que $r \geq 2$. \\

   \item Le polynôme $P(x) = x^r$ est annulateur de $u$ et admet pour unique racine 0 donc on a $\spc (u) \subset \{ 0 \}$. \\
     De plus 0 est valeur propre car $u$ n'est pas inversible. \\
     En effet, sinon en composant $r-1$ fois par $u^{-1}$ l'égalité $u^r = 0$ on aurait $u = 0$, ce qui est absurde. \\
     Enfin comme $u$ admet pour unique valeur propre 0, on montre par l'absurde que si il était diagonalisable, il admettrait pour matrice la matrice nulle et serait nul. $u$ n'est donc pas diagonalisable. \\

   \item On suppose que $v(x) = 0$, on a alors $\Sum{k=0}{r-1} \frac{u^k (x)}{k!} = 0$. \\
     On compose par $u^{r-1}$, cela donne $u^0 (x) =0$ donc $x = 0$. \\
     L'application $v$ est donc injective, et comme c'est un endomorphisme, c'est bien un isomorphisme de $E$. \\
     L'idée pour l'inverse est de remarquer l'analogie avec la série exponentielle, et comme $e^x \times e^{-x} = 1$, on va poser $w = \Sum{k=0}{r-1} \frac{(-u)^k }{k!}$, et calculer $w \circ v$. \\
     On a $w \circ v = \Sum{k=0}{r-1} \Sum{i=0}{r-1} (-1)^i \frac{u^{k+i}}{k! i!} \\ \\ = \Sum{k=0}{r-1} \Sum{i=0}{r-1} \frac{1}{(i+k)!} \binom{i+k}{i} (-1)^i  u^{k+i}$. \\ \\
     On réordonne la somme en posant $n = i+k$, on a $i \leq n$ car $k \geq 0$, cela donne : \\
     $w \circ v = \Sum{n=0}{r-1} \Sum{i=0}{n} \frac{1}{n!} \binom{n}{i} (-1)^i u^n $ car $u^n = 0$ pour $n > r-1$.  \\ 
     $w \circ v = \Sum{n=0}{r-1}  \frac{u^n}{n!} \Sum{i=0}{n} \binom{n}{i} (-1)^i \times 1^{n-i}$. \\
     $w \circ v = \frac{u^0}{0!} + \Sum{n=1}{r-1} \frac{u^n}{n!} (1 - 1)^n = \id$. \\ \\
     On obtient alors $v^{-1} = w = \Sum{k=0}{r-1} \frac{(-u)^k }{k!}$. \\

   \item Si $u(x) = 0$, alors $v(x) = x + \Sum{k=1}{n} u^k (x) = x$ donc $( v -\id) (x) = 0$. \\
     On obtient $\ker u \subset \ker (v - \id)$. \\

   \item 0 est valeur propre de $u$ donc $\ker u \neq \{0 \}$, donc $\ker (v - \id) \neq 0$, et 1 est valeur propre de $v$. \\
     Plus généralement, on se donne $\lambda \neq 1$, et on étudie : \\
     $v(x) = \lambda x \Leftrightarrow \lambda x = \Sum{k=0}{r-1} \frac{u^k (x)}{k!}$. \\
     On compose par $u^{r-1}$ et on obtient : \\
     $\lambda u^{r-1} (x) = u^{r-1} (x)$, donc $( \lambda -1) u^{r-1} (x) = 0$. \\
     On a  $\lambda \neq 1$ donc $u^{r-1} (x) = 0$. \\
     On a alors $(\lambda -1) x = \Sum{k=1}{r-2} \frac{u^k (x)}{k!}$, et on compose par $u^{r-2}$, cela donne : \\
     $(\lambda -1) u^{r-2} (x) = 0$, donc $u^{r-2} (x) = 0$ et ainsi de suite .... \\
     On obtient finalement $u(x) = 0$, puis $\lambda x = x$, $(\lambda -1) x =0$ et enfin $x=0$ car $\lambda -1 \neq 0$. \\
     Il n'y a donc pas de solutions autres que 0, et la seule valeur propre de $v$ est 1. \\
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soient $n$ et $N$ des entiers non nuls. \\
   Une urne contient $n$ jetons numérotés de 1 \`{a} $n$. On effectue $%
   N $ tirages avec remise dans cette urne.

   \begin{enumerate}
   \item $F_i \suit \mathcal{B} \left(N , \frac{1}{n} \right)$.

     On pose : 
     \begin{equation*}
       F=\sum_{i=1}^{n}F_{i}
     \end{equation*}
     $F$ est une variable certaine égale à $N$, car elle compte le nombre total de jetons tirés. \\
     On en déduit que $\E(F) = N$ et $\V(F) = 0$. \\

     Les variables $F_i$ ne sont pas deux à deux indépendantes. \\
     Par exemple pour $i \neq j$, $\Prob( F_i =N, F_j = N) = 0 \neq \Prob(\Ev{ F_i =N}) \Prob(\Ev{ F_j = N})$ (on ne peut pas tirer $2 N$ jetons). \\

   \item $X_i$ est une variable de Bernouilli de paramètre $\Prob(\Ev{ X_i = 1}) = 1 - \Prob(\Ev{ X_i = 0}) = 1 - P \Ev{ F_i = 0} = 1 - \left( 1 - \frac{1}{n} \right)^n$. \\
     D'où $\E(X_i) = p = 1 - \left( 1 - \frac{1}{n} \right)^n$ et $\V(X_i) = p (1-p) = \left[1 - \left( 1 - \frac{1}{n} \right)^n \right] \times \left( 1 - \frac{1}{n} \right)^n$. \\ \\
     $P_{\Ev{X_i = 0}} \Ev{X_j = 0} = P_{F_i = 0} \Ev{F_j = 0} = \left( 1 - \frac{1}{N-1} \right)^n$ car on tire parmi $N-1$ boules (on sait que la boule $i$ n'est jamais tirée, on peut faire les calculs en considérant qu'elle n'est pas là. \\
     on obtient $P_{\Ev{X_i = 0}} \Ev{X_j = 0}  \neq \Prob(\Ev{ X_j = 0})$ donc $X_i$ et $X_j$ ne sont pas indépendantes.
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice} \indent \\ 

   \begin{enumerate}
   \item Une densité de probabilité est positive, continue sauf en un nombre fini de points (c'est la cas ici à condition que $c \geq 0$ et vérifie $\int_{-\infty}^{+\infty} f(t)\ dt =1$. \\
     Ici on calcule $\int_{10}^x \frac{x}{t^2}\ dt = \left[ \frac{-c}{t} \right]_{10}^x = \frac{-c}{x} + \frac{c}{10} \xrightarrow[x \rightarrow +\infty]{} \frac{c}{10}$ donc il faut $c =10$, et $f$ est bien positive. \\

   \item Cela signifie que $F(m) = 1 - F(m)$, donc $F(m) = \frac{1}{2}$. \\
     Or $F(x) = 0$ pour $x < 10$ (pas de solution dans cet intervalle) et $F(x) = 1 - \frac{10}{x}$ sinon. \\
     D'où $m$ vérifie $\frac{10}{m} = \frac{1}{2}$, et enfin $m = 20$. \\

   \item On définit la loi binomiale de paramètres $5$ et $\Prob(\Ev{ X \geq 15}) = \frac{10}{15} = \frac{2}{3}$. \\
     On cherche $\Prob(\Ev{ X \geq 3}) = \binom{5}{3} \left( \frac{2}{3} \right)^3 \left( \frac{1}{3} \right)^2 + \binom{5}{4} \left( \frac{2}{3} \right)^4 \left( \frac{1}{3} \right) + \binom{5}{5} \left( \frac{2}{3} \right)^5 \left( \frac{1}{3} \right)^0 = 10 \frac{8}{3^5} + 5 \frac{16}{3^5} + \frac{32}{3^5} = \frac{80 + 80 + 32}{3^5} = \frac{192}{3^5} = \frac{64}{3^4} = \frac{64}{81}$. \\
     \\
     Deux machines $A$ et $B$ sont équipées de composants du type préc%
     édent. Plus précisément.

     \begin{itemize}
     \item $A$ contient deux composants et cesse de fonctionner dès que l'un
       de ces composants est défectueux,

     \item $B$ contient également deux composants mais un seul de ces
       composants suffit \`{a} la faire fonctionner
     \end{itemize}

     On note $T_{A}$, $T_{B}$ les durées de fonctionnement de ces machines.

   \item $T_A = \min (X_1 , X_2)$ donc $F_{T_A} (x) = 1 - \left( 1 - F_X(x) \right)^2 = 1 - \frac{100}{x^2}$ si $x \geq 10$ et $0$ sinon, et $f_{T_A} (x) =  \frac{200}{x^{3}}$ si $x \geq 10$ et $0$ sinon. \\ \\
     $T_B = \max (X_1 , X_2)$ donc $F_{T_B} (x) = ( F(x) )^n = \left( 1 - \frac{10}{x} \right)^2$ si $x \geq 10$ et $0$ sinon, et $  f_{T_B} (x) = \frac{ 20}{x^2} \left( 1 - \frac{10}{x} \right)$ si $x \geq 10$ et $0$ sinon. \\

   \item Pour $T_A$ comme $T_B$, le seul problème est en $+\infty$. \\
     Or $x t_{T_A} (x) = \frac{200}{x^2}$ est une fonction intégrable en $+\infty$ (Riemann) et $x f_{T_B} (x) \sim \frac{20}{x}$ en $+\infty$, qui n'est pas intégrable en $+\infty$ (Riemann à nouveau) donc par théorème de comparaison des intégrales de fonctions positives, l'intégrale de $t f(t)$ diverge. \\
     D'où $T_A$ admet une espérance, mais pas $T_B$. \\
     Un calcul d'intégrale facile donne $\E(T_A) = 20$.
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   \begin{enumerate}
   \item Les vecteurs $(1,0)$, $(0,1)$ et $(1,1)$ remplissent cette condition. \\ \\
     1er cas : il y a deux valeurs propres distinctes, alors les sous-espaces propres sont de dimension 1. Alors si $e_1$ est vecteur propre associé à $\lambda_1$, $E_{\lambda_1} = \Vect{ e_1}$ et les deux autres ne sont pas vecteurs propres associés à $\lambda_1$. Si l'un des deux autres est vecteur propre associé à $\lambda_2$, le troisième ne sera alors pas vecteur propre. \\
     2e cas : une seule valeur propre, et un sous-espace propre associé de dimension 2 ($u = \lambda \id$, alors les trois vecteurs (et tout vecteur non nul de $\R^2$) sont vecteurs propres associé à la valeur propre $\lambda$. \\

   \item Soit $f$ un endomorphisme de $\R^{n}$ et $\mathcal{F}$ une
     famille de $n+1$ vecteurs propres de $f$ s'il en existe.

     \begin{enumerate}
     \item Non, car $\Card \mathcal{F} > \dim \R^n$. \\

     \item Supposons qu'il existe plusieurs valeurs propres distinctes ($k$ valeurs propres), alors il existe une valeur propre $\lambda$ telle que $\dim E_{\lambda} \leq n-1$. \\
       Soit $a$ un vecteur propre de $\mathcal{F}$ associé à $\lambda$, alors la famille restante est une base de vecteurs propres. \\
       Il y a donc une sous-famille de cette famille qui est une base de $E_{\lambda}$, et comme $a \in E_{\lambda}$, $a$ s'écrit $a = \sum a_i e_i$ avec les $a_i$ non tous nuls car $a \neq 0$. \\
       On obtient que la famille $(a , e_1 ,\ \dots\ , e_p)$ est liée, avec $p \leq n-1$ donc il y a moins de $n$ vecteurs. On la complète avec d'autres vecteurs de $\mathcal{F}$ et on obtient une sous-famille de $\mathcal{F}$ constituée de $n$ vecteurs et liée, ce qui contredit l'hypothèse de départ. \\
       On en déduit que $f$ admet une unique valeur propre, et comme elle admet des bases de vecteurs propres, elle est diagonalisable et $f = \lambda \id$.
     \end{enumerate}
   \end{enumerate}

 \end{exercice}

 \newpage

 \begin{exercice} \indent \\ \\
   \textbf{Question de cours.} Soit $(A_i)_{i \in I}$ un système complet d'évènements, c'est-à-dire une famille d'évènements deux à deux incompatibles, dont la réunion est $\Omega$ l'univers de l'expérience et dont aucun n'est négligeable (ils ont des probabilités non nulles). \\
   Alors pour tout évènement $B \in \Omega$, $\Prob(B) = \sum\limits_{i \in I} \Prob( B \cap A_i) = \sum\limits_{i \in I} \Prob(A_i) P_{A_i} (B)$.
   \\ \\
   On lance deux pièces truquées : La pièce 1 donne pile avec une
   probabilité $p_{1}$ et la pièce 2 donne pile avec une probabilité%
   , $p_{2}$. \\
   On effectue les lancers de la fa\c{c}on suivante : on choisit une pièce
   uniformément au hasard et on lance la pièce choisie. Si on obtient
   pile, on relance la m\^{e}me pièce et ainsi de suite jusqu'a ce que l'on
   obtienne face; \`{a} ce moment on change de pièce - plus géné%
   ralement, dès que l'on obtient face, on change de pièce. On suppose
   que $p_{1}$ et $p_{2}$ sont dans $\left] 0,1\right[ $

   \begin{enumerate}
   \item On pose $A_n$, de probabilité $a_n$, l'évènement : "lancer la pièce 1 au $n$-ième lancer" et $B_n$, de probabilité $b_n$, l'évènement : "lancer la pièce 2 au $n$-ième lancer". \\
     Comme $(A_{n} , B_{n})$ est un système complet d'évènements on a : \\
     $a_{n+1} =\Prob(A_n)  P_{A_n} (A_{n+1}) +\Prob(B_n)  P_{B_n} (A_{n+1}) = a_n p_1 + (1-a_n) (1-p_2) = a_n (p_1 + p_2 -1) + 1 - p_2$. \\ \\
     On reconnaît une suite arithmético-géométrique, on résout $k = k (p_1 + p_2 -1) + 1 - p_2 \Leftrightarrow k ( 2 - p_1 - p_2) = 1 - p_2 \Leftrightarrow k= \frac{1-p_2}{2 - p_1-p_2}$. \\
     La suite $u_n = a_n - k$ est alors géométrique de raison $(p_1 + p_2 -1)$, donc pour tout $n$ on a :\\
     $a_n = u_n +\frac{1-p_2}{2 - p_1-p_2} = (p_1 + p_2 -1)^{n-1} u_1 + \frac{1-p_2}{2 - p_1-p_2} = (p_1 + p_2 -1)^{n-1} \left(a_1 -\frac{1-p_2}{2 - p_1-p_2} \right)+ \frac{1-p_2}{2 - p_1-p_2} = (p_1 + p_2 -1)^{n-1} \left(\frac{1}{2}-\frac{1-p_2}{2 - p_1-p_2} \right)+ \frac{1-p_2}{2 - p_1-p_2}$. \\

   \item Avec le même système complet on a en posant $P_n$ : "on obtient pile au $n$-ième tirage" : \\
     $r_n = \Prob(A_{n} ) P_{A_{n} } (P_n) + \Prob(B_{n}) P_{B_{n} } (P_n) = a_{n} p_1 + (1 - a_{n} ) p_2 = a_{n} ( p_1 - p_2) + p_2 = (p_1 - p_2) \left[ (p_1 + p_2 -1)^{n-1} \left(\frac{1}{2}-\frac{1-p_2}{2 - p_1-p_2} \right)+ \frac{1-p_2}{2 - p_1-p_2} \right] + p_2$. \\

   \item On a $0 < p_1 < 1$ et $0 < p_2 < 1$ donc $0 < p_{1} + p_2 < 2$ et enfin $-1 < 1 - p_1 - p_2 < 1$. \\
     D'où $\lim\  (p_1 + p_2 -1)^{n-1} =0$, et on obtient $L = \frac{(p_1 - p_2) (1- p_2)}{2 - p_1 - p_2} + p_2$. \\

   \item On calcule $L = \frac{ \frac{1}{6} \times \frac{5}{6} }{ {3}{2} } + \frac{1}{6} = \frac{5}{36} \frac{2}{3} + \frac{1}{6} = \frac{5}{54} + \frac{9}{54} = \frac{14}{54} = \frac{7}{27}$. \\
     Ensuite on simplifie $r_n = \frac{1}{6} \left[ \left( - \frac{1}{2} \right)^{n-1} \frac{-1}{18} + \frac{5}{9} \right] + \frac{1}{6}$. \\
     Petit problème avec l'énoncé : on ne peut pas calculer un rang à partir duquel ... avec un programme! On peut par contre calculer la première valeur telle que ..., mais sans assurance que les suivantes vérifieront toutes la propriété (penser au fait que la suite n'est pas monotone). Par contre si on veut un rang à partir duquel... il faut réaliser une étude mathématique pour majorer ( ou calculer) $\vert r_n - L \vert = \frac{1}{54} \left( \frac{1}{2} \right)^{n-1}$; dans ce cas l'intervention de l'informatique est bien inutile : un petit passage au logarithme permet de déterminer $n$ sans problème. \\
     On obtient le programme suivant : \\
     var n : integer;  r : real ; \\
     n : = 1; r:= 1/54 ;\\
     repeat \\
     n : = n + 1; r : = r/2 ; \\
     until r < exp (-6 * ln ( 10) ) ; \\
     writeln (n) ; \\
     (programme qui sert à calculer une valeur qu'une calculatrice et un calcul de logarithme donnerait sans problème...) \\
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soit $u$ et $v$ deux endomorphismes de $\R^{2}$ dont les matrices
   respectives dans la base canonique $\left( e_{1},e_{2}\right) $ de $\mathbb{R%
   }^{2}$ sont notées $A$ et $B$. On suppose que $AB=%
   \begin{smatrix}
     0 & 0 \\ 
     0 & 0%
   \end{smatrix}%
   $ et $BA=%
   \begin{smatrix}
     0 & 1 \\ 
     0 & 0%
   \end{smatrix}%
   $

   \begin{enumerate}
   \item Si $v$ était bijectif, $B$ serait inversible et on aurait $A B B^{-1} = 0 B^{-1}$ donc $A = 0$, puis $B A = B 0 = 0$ ce qui est absurde. \\
     Donc $v$ n'est pas bijectif. \\
     On en déduit que $\Im v$ est de dimension inférieure ou égale à 1. \\
     D'autre part on a $\Im v \circ u \subset \Im v$ et $\Im v \circ u = \Vect{ e_1}$ est de dimension 1. \\
     On en déduit que $\dim (\Im v) \geq 1$, puis $\dim (\Im v) = 1$ et avec l'égalité des dimensions, $\Im v = \Im v \circ u = \Vect{ e_1}$. \\

   \item De la même manière $u$ n'est ni bijectif ni nul donc $\dim \ker u = 1$. \\
     Or $\ker (v \circ u) \subset \ker u$ est de dimension 1, donc $\ker u = \ker (v \circ u)$ et $e_1 \in \ker (v \circ u)$, donc $(e_1)$ (famille libre car constituée d'un vecteur non nulle, et de cardinal égal à la dimension) est une base de $\ker (v \circ u)$, et enfin $\ker u = \Vect{ e_1} = \Im (v)$. \\

   \item On a $u(e_1) = 0$ donc $A = \begin{smatrix} 0 & a \\ 0 & b \\ \end{smatrix}$ et $\Im v =\Vect{ e_1}$ donc $B =\begin{smatrix} c & d \\ 0 & 0 \\ \end{smatrix}$. \\
     En calculant $AB$ et $BA$ on obtient bien $AB = 0$ et $BA = \begin{smatrix} 0 & ac + bd \\ 0 & 0 \\ \end{smatrix}$ donc il faut rajouter la condition $ac+bd=1$ pour satisfaire les hypothèses.
   \end{enumerate}
 \end{exercice}


 \newpage


 \begin{exercice} \indent \\ \\
   Soit $\alpha >0,$ $x_{0}>0$ et $f$ la fonction de $\R$ dans $\mathbb{%
     R}$ définie par :%
   \begin{equation*}
     f\left( x\right) =\frac{\alpha }{x_{0}}\left( \frac{x_{0}}{x}\right)
     ^{\alpha +1}\text{ si }x\geq x_{0}\text{\quad et\quad \ }f\left( x\right) =0%
     \text{ sinon}
   \end{equation*}

   \begin{enumerate}
   \item 
     \begin{enumerate}
     \item Une variable aléatoire réelle de fonction de répartition $F$ est dite à densité s'il existe une fonction $f$ définie sur $\R$ positive, continue sauf éventuellement en un nombre fini de points, et vérifiant $\int_{-\infty}^{+\infty} f(t)\ dt = 1$, telle que pour tout $x \in \R$, $F(x) = \int_{-\infty}^{+\infty} f(t)\ dt$. \\ \\
       Ici on a $\int_{-\infty}^{+\infty} f(t)\ dt = \dlim{x \rightarrow +\infty} \int_{x_0}^x \frac{\alpha}{x_0} \left( \frac{x_0}{t} \right)^{\alpha + 1}\ dt = \dlim{x \rightarrow +\infty}  x_0^{\alpha} \left[ - \frac{1}{t^{\alpha}} \right]_{x_0}^x = \dlim{x \rightarrow +\infty} \left( - \frac{x_0^{\alpha} }{x^{\alpha} } + \frac{x_0^{\alpha}}{x_0^{\alpha}} \right) = 1$. \\ \\
       Ensuite on a $F(x)= 0$ si $x < x_0$, et $F(x) = 1 - \left( \frac{x_0}{x} \right)^{\alpha}$ si $x \geq x_0$. \\

     \item Seule l'intégrabilité en $+\infty$ pose problème. \\
       De plus on a $t f(t) = \alpha x_0^{\alpha} \frac{1}{x^{\alpha}}$ qui est intégrable en $+\infty$ si et seulement si $\alpha > 1$ (Riemann) donc $X$ admet une espérance si et seulement si $\alpha > 1$. \\
       De même $X$ admet un moment d'ordre 2 donc une variance si et seulement si $\alpha > 2$. \\
       Enfin si $\alpha > 1$, un calcul d'intégrale facile donne $\E(X) = \frac{\alpha}{\alpha - 1} x_0$. \\

     \item $M_X(x) = \frac{ \E(X) - \int_{-\infty}^x t f(t)\ dt }{ 1 - F_X(x) } = \frac{\E(X) - \int_{x_0}^x t f(t)\ dt}{ \left( \frac{x_0}{x} \right)^{\alpha}} = \frac{ \alpha x_0^{\alpha}  }{ (\alpha - 1) x^{\alpha -1}  } \times  \frac{x^{\alpha}}{x_0^{\alpha}}  = \frac{ \alpha}{\alpha -1} x$. \\
     \end{enumerate}

   \item On se propose d'établir une réciproque de la propriété
     précédente. Soit $x_{0}>0$ et $Y$ une variable aléatoire \`{a}
     valeurs dans $\left[ x_{0},+\infty \right[ $ de densité $h$ continue, 
     \`{a} valeurs strictement positives, admettant une espérance et telle
     qu'il existe un réel $k>1$ vérifiant : 
     \begin{equation*}
       \forall x>x_{0},\quad M_{Y}\left( x\right) =\frac{\int_{x}^{+\infty
         }th\left( t\right) dt}{\int_{x}^{+\infty }h\left( t\right) dt}=kx
     \end{equation*}

     \begin{enumerate}
     \item On pose, pour tout $x>x_{0}$%
       \begin{equation*}
         G\left( x\right) =\int_{x}^{+\infty }h\left( t\right) dt
       \end{equation*}%
       On a $G'(x) = - h(x)$, puis en dérivant l'égalité $\int_{x}^{+\infty }t h\left( t\right) dt = k x \int_{x}^{+\infty }h\left( t\right) dt$ on a $- x h(x) = - k x h(x) + k G(x)$, donc $ x G'(x) - k x G'(x) = k G(x)$ en enfin $G(x) = \frac{1-k}{k} x G'(x)$. \\

     \item $A'(x) = x^{ \frac{k}{k-1} } G'(x) + \frac{k}{k-1} x^{ \frac{k}{k-1} - 1} G(x) = G'(x) x^{ \frac{k}{k-1} } \left( 1 + \frac{k}{k-1} \frac{1}{x} \frac{1-k}{k} x \right) = G'(x) x^{ \frac{k}{k-1} } ( 1 - 1) = 0$. \\ \\
       On en déduit que pour tout $x \in \R$, $A(x) = K$ une constante réelle, puis que $G(x) = K x^{-\frac{k}{k-1}}$. \\
       Or en la valeur $x = x_0$, on a $G(x_0) = 1$ donc $K =  x_0^{ \frac{k}{k-1} } $, et enfin : \\
       $F_Y(x) = 1 - G(x) = 1 - \left( \frac{x_0}{ x} \right)^{\frac{k}{k-1}}$, et on obtient la loi de Pareto de paramètres $x_0$ et $\frac{k}{k-1}$. \\ \\
       Enfin avec la valeur en $x_0$ de $M_Y$ on peut retrouver $\frac{\E(Y)}{1} = k x_0$, donc $\E(Y) = k x_0$ (on a bien $\frac{ \frac{k}{k-1 } }{ \frac{k}{k-1} - 1} = k$, et on retrouve les résultats précédents.
     \end{enumerate}
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soient $X$ et $Y$ deux variables aléatoires binomiales de paramètres 
   $\left( n,1/2\right) $ indépendantes. Calculer $\mathrm{P}\left(
     X=Y\right) .$ \\ \\
   On paramètre par le système complet d'évènements $\Ev{X=k}_{0 \leq k \leq n}$ : \\
   $\Prob(\Ev{X=Y}) = \Sum{k=0}{n} \Prob( X=k , Y=k) = \Sum{k=0}{n}  \binom{n}{k} ^2 \frac{1}{2^n} \frac{1}{2^n} = \frac{1}{4^n} \Sum{k=0}{n}  \binom{n}{k} ^2$. \\ \\
   On ne sait pas calculer cette somme, la méthode échoue. L'idée est en fait d'utiliser le fait que $\Prob(\Ev{ X=k}) = \Prob(\Ev{ X = n-k})$. \\
   On a alors $\Prob(\Ev{ X = Y}) =  \Sum{k=0}{n} \Prob(\Ev{ X=k}) \Prob(\Ev{ Y=k}) =  \Sum{k=0}{n} \Prob(\Ev{ X=k }) \Prob(\Ev{ Y=n-k}) = \Prob(\Ev{ X+Y = n})$. \\
   On sait que $X + Y \suit \mathcal{B} \left( 2n , \frac{1}{2} \right)$, on en déduit que $\Prob(\Ev{ X = Y}) = \binom{2n}{n} \frac{1}{2^{2n}} = \binom{2n}{n} \frac{1}{4^n}$. \\
 \end{exercice}

 \newpage

 \begin{exercice} \indent \\
   \begin{enumerate}
   \item Une famille $(f_i)_{i \in I}$ de vecteurs de $E$ est dite génératrice de $E$ si $\Vect ( (f_i)_{i \in I} ) = E$, c'est-à-dire que tout vecteur de $E$ peut s'écrire comme combinaison linéaire de la famille. \\
     Elle est dite libre si toute combinaison linéaire est unique, c'est-à-dire : $\sum\limits_{i \in I} a_i f_i = 0 \Rightarrow \forall i \in I,\ a_i = 0$. \\

     \hspace{-1cm}Dans l'espace vectoriel $C^{O}\left( \R\right) $ des
     fonctions continues de $\R$ dans $\R$, on considère les
     trois fonctions \[
     f_{1}:x\rightarrow 1\quad f_{2}:x\rightarrow x\quad \text{et}
     \quad f_{0}:x\rightarrow \left \{ 
       \begin{array}{cc}
         x\ln \left \vert x\right \vert & \text{si }x\neq 0 \\ 
         0 & \text{si }x=0%
       \end{array}%
     \right. 
     \]

     \hspace{-1cm}Soit $E$ le sous-espace vectoriel de $C^{0}\left( \R%
     \right) $ engendré par $\left( f_{1},f_{2},f_{3}\right) $

   \item Elle est génératrice de $E$ par définition, montrons qu'elle est libre. \\
     Soient $a , b ,c$ tels que $a f_1 + b f_2 + c f_3 = 0$, donc $a + b x + c x \ln \vert x \vert = 0$ pour tout $x \neq 0$, et $a + 0 + 0 =0$ pour $x = 0$. \\
     On vient d'obtenir $a=0$, il reste $b x + c x \ln \vert x \vert =0$ pour tout $x \neq 0$. \\
     On a alors $x ( b  + c \ln \vert x \vert) = 0$, donc $b + c x \ln \vert x \vert =0$ pour tout $x \neq 0$. \\
     En $x =1$ on obtient $b + c \ln 1 = b = 0$, donc $b = 0$. \\
     On en déduit que $c \ln \vert x \vert = 0$, pour $x \neq 0$, on prend $x = 2$ en on obtient $c \ln 2 =0$, donc $c =0$. \\

   \item A toute fonction $f$ de $E$ on associe la fonction $\Phi \left(
       f\right) $ définie par $\Phi \left( f\right) =\left( xf\right) ^{\prime
     },$ dérivée de la fonction $x\rightarrow x~f\left( x\right) $ \\ \\
     On a $\Phi (f_1) (x) = 1$ donc $\Phi (f_1) = f_1 \in E$, $\Phi (f_2) (x) = 2x$ donc $\Phi (f_2) = 2 f_2 \in E$, et $\Phi (f_3) (x) = 2 x \ln \vert x \vert + x^2 \frac{1}{x} = 2 x \ln \vert x \vert + x$ pour $x \neq 0$ (qui est pour l'instant un abus de notation car on n'a pas prouvé que $x \rightarrow x f_3 (x)$ est dérivable sur $\R$. \\
     On vérifie alors que cette dérivée est prolongeable par continuité, on en déduit par prolongement de la dérivée que $x \rightarrow x f_3 (x)$ est dérivable sur $\R$, de dérivée $2 f_3(x) + f_2 (x)$. \\
     D'où $\Phi (f_3)$ est bien définie et $\Phi (f_3) = 2 f_3 + f_2 \in E$. \\
     Toute combinaison linéaire de $f_1$, $f_2$ et $f_3$ admet donc une image par $\Phi$ et cette image est combinaison linéaire (car $\Phi$ est trivialement linéaire) de $\Phi (f_1) , \Phi (f_2) , \Phi (f_3)$ qui sont tous dans $E$, donc elle est dans $E$. \\
     On obtient bien que $\Phi$ est un endomorphisme de $E$, et on a : $M = \begin{smatrix} 1 & 0 & 0 \\ 0 & 2 & 1 \\ 0 & 0 & 2 \\ \end{smatrix}$. \\

     \begin{enumerate}
     \item $M$ est inversible car triangulaire sans 0 sur la diagonale. \\
       Pour son inverse, on peut réaliser : \\
       - un pivot très rapide. \\
       - conjecturer que $M^{-1} = \begin{smatrix} 1 & 0 & 0 \\0 & \frac{1}{2} & a \\ 0 & 0 & \frac{1}{2} \\ \end{smatrix}$ et montrer que $a=-\frac{1}{4}$ en calculant $M M^{-1}$. \\
       - calculer $M^2$ et essayer d'en tirer un polynôme annulateur (cela échoue). \\

     \item L'équation $\Phi (f) = g$ est équivalente à $\Phi^{-1} (g) = f$, donc $Mat (f) = M^{-1} Mat (g)= \begin{smatrix} 1 & 0 & 0 \\ 0 & \frac{1}{2} & -\frac{1}{4}  \\ 0 & 0 & \frac{1}{2} \\ \end{smatrix} \begin{smatrix} a \\ b \\ 1 \\ \end{smatrix} = \begin{smatrix} a \\ \frac{1}{2} b - \frac{1}{4} \\ \frac{1}{2} \\ \end{smatrix}$. \\
       La solution de l'équation est donc $f = a f_1 + \left( \frac{1}{2} b - \frac{1}{4} \right) f_2 + \frac{1}{2} f_3$. \\ \\
       En particulier on voit que $\Phi \left( \frac{1}{2} f_3 - \frac{1}{4} f_2 \right) = f_3$, donc une primitive de $f_3$ est : \\
       $h(x) = \frac{1}{2} x \ln \vert x \vert - \frac{1}{4} x^2$. \\
     \end{enumerate}
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation}} \\ \\
   Soient $X$ et $Y$ deux variables aléatoires binomiales de paramètres 
   $\left( n,1/2\right) $ indépendantes. Calculer $\mathrm{P}\left(
     X=Y\right) .$ \\ \\
   On paramètre par le système complet d'évènements $\Ev{X=k}_{0 \leq k \leq n}$ : \\
   $\Prob(\Ev{X=Y}) = \Sum{k=0}{n} \Prob( X=k , Y=k) = \Sum{k=0}{n}  \binom{n}{k} ^2 \frac{1}{2^n} \frac{1}{2^n} = \frac{1}{4^n} \Sum{k=0}{n}  \binom{n}{k} ^2$. \\ \\
   On ne sait pas calculer cette somme, la méthode échoue. L'idée est en fait d'utiliser le fait que $\Prob(\Ev{ X=k}) = \Prob(\Ev{ X = n-k})$. \\
   On a alors $\Prob(\Ev{ X = Y}) =  \Sum{k=0}{n} \Prob(\Ev{ X=k}) \Prob(\Ev{ Y=k}) =  \Sum{k=0}{n} \Prob(\Ev{ X=k }) \Prob(\Ev{ Y=n-k}) = \Prob(\Ev{ X+Y = n})$. \\
   On sait que $X + Y \suit \mathcal{B} \left( 2n , \frac{1}{2} \right)$, on en déduit que $\Prob(\Ev{ X = Y}) = \binom{2n}{n} \frac{1}{2^{2n}} = \binom{2n}{n} \frac{1}{4^n}$. \\
 \end{exercice}

 \newpage

 \begin{exercice} \indent \\ \\
   \textbf{Question de cours :} Une variable aléatoire suit la loi uniforme sur $[ a ;b]$ si c'est une variable à densité dont une densité est une fonction constante sur $[ a ; b]$ et nulle ailleurs. \\
   De plus pour que ce soit une densité de probabilité, un calcul d'intégrale trivial donne cette constante égale à $\frac{1}{b-a}$. \\ \\
   On obtient alors que la fonction $f(x) = \begin{cases} \frac{1}{b-a} \text{ si } x \in [a ;b ] \\ 0  \text{ sinon} \end{cases}$ est une densité de $X$ et la fonction de répartition de $X$ est $F(x) = \begin{cases} 0  \text{ si } x \leq a \\ \frac{x-a}{b-a} \text{ si } x \in [a ;b ] \\ 1  \text{ si } x \geq b\end{cases}$.
   \\
   Soit $X$ une variable aléatoire suivant une loi uniforme sur $\left[ 0,1%
   \right] $. On pose $Y=\min \left( X,1-X\right) $ et $Z=\left( X,1-X\right) $.

   \begin{enumerate}
   \item On a $Y (\Omega) = \left[ 0 ; \frac{1}{2} \right]$ car si $x \geq \frac{1}{2}$, alors $1- x \leq \frac{1}{2}$ donc le minimum est toujours plus petit que $\frac{1}{2}$. \\
     On a donc $F_Y(x) = 0$ si $x < 0$ et $1$ si $x > \frac{1}{2}$, et pour tout $ x \in \left[ 0 ; \frac{1}{2} \right]$ on a : \\
     $F_Y(x) = 1 - P( \min ( X , 1- X) \geq x ) =1 - \Prob( X \geq x , 1- X \geq x) =1 - \Prob( X \geq x , X \leq 1-x) = 1 -  \Prob( x \leq X \leq 1-x) = 1 - (1-x-x) = 2x$. \\
     On en déduit que $f_Y(x) = 2$ si $0 \leq x \leq \frac{1}{2}$ et 0 sinon est une densité de $Y$, donc $Y$ suit la loi uniforme sur $\left[ 0 ; \frac{1}{2} \right]$, et $\E(Y) = \frac{1}{4}$. \\

   \item On peut utiliser la loi du max et refaire le même genre de raisonnement qu'à la question précédente. \\
     On peut aussi remarquer que $Z = 1 - Y$ donc $Z (\Omega) = \left[ \frac{1}{2} ; 1\right]$ et pour tout $x \in \left[ \frac{1}{2} ; 1\right]$, $\Prob(\Ev{ Z \leq x}) = \Prob( 1 - Y \leq x) = \Prob( Y \geq 1 - x) = 1 - \Prob(\Ev{ Y \leq 1-x}) = 1 - 2 (1-x) = 2x - 1$, et $\Prob(\Ev{ Z \leq x}) = 0$ sinon. \\
     On en déduit que $f_Z(x) = 2$ si $\frac{1}{2} \leq x \leq 1$ et 0 sinon est une densité de $Z$, qui suit la loi uniforme sur $\left[ \frac{1}{2} ; 1\right]$ et vérifie donc $\E(Z) = \frac{3}{4}$. \\

   \item Comme $Y$ et $Z$ sont positives, et $Z \geq Y$, $ \frac{Y}{Z} (\Omega) \subset [ 0 ; 1]$. \\
     Pour $x < 0$, $\Prob(\Ev{ R \leq x}) = 0$, et pour $x > 1$, $\Prob(\Ev{ R \leq x}) = 1$. \\
     De plus on a pour tout $x \in [ 0 ; 1]$ : \\
     $\Prob(\Ev{ R \leq x}) = \Prob(\Ev{ Y \leq x Z}) = \Prob( Y \leq x - x Y) = P \left( Y \leq \frac{x}{1+x} \right) = 2 \frac{x}{1+x}$. \\ \\
     On en déduit que la fonction $f_R(x) = \frac{2(1+x) - 2x}{(1+x)^2} = \frac{2}{(1+x)^2}$ si $x \in [ 0 ; 1]$ et 0 sinon est une densité de $R$. \\
     Enfin $\E(R) = \int_0^1 2 \frac{x}{ (1+x)^2}\ dx = 2 \int_1^2 \frac{ x-1}{x^2}\ dx = 2 \left[ \ln x + \frac{1}{x} \right]_1^2 = 2 \ln 2 -1$. \\

   \item var x , y , z : real ; \\
     begin ; \\
     randomize ; \\
     x : = random; \\
     if x <= 1-x then begin 
     y : = x ; z : = 1-x ; end ;\\
     else  begin y:=1-x ; z := x ; end; \\
     end. \\
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation}}
   \\ \\
   On considère l'endomorphisme $f$ de $\R^{4}$ dont la matrice
   dans la base canonique de $\R^{4}$ est la matrice 
   \begin{equation*}
     M=%
     \begin{smatrix}
       -1 & -1 & -1 & 1 \\ 
       -1 & -1 & 1 & -1 \\ 
       -1 & 1 & -1 & -1 \\ 
       1 & -1 & -1 & -1%
     \end{smatrix}%
   \end{equation*}

   \begin{enumerate}
   \item $M$ est symétrique donc diagonalisable, et $f$ est donc diagonalisable. \\

   \item Il reste à calculer les sous-espaces propres associés aux valeurs propres 2 et $-2$ (amusez-vous bien!) pour obtenir $P$ et $D$, puis on écrit $M^n = P D^n P^{-1}$ et on ne va au bout des calculs (calcul de $P^{-1}$, calcul explicite très lourd de $M^n$) que si c'est demandé par l'examinateur. \\
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice} \indent \\
   \textbf{Question de cours.} Soit $(A_i)_{i \in I}$ un système complet d'évènements, c'est-à-dire une famille d'évènements deux à deux incompatibles, dont la réunion est $\Omega$ l'univers de l'expérience et dont aucun n'est négligeable (ils ont des probabilités non nulles). \\
   Alors pour tout évènement $B \in \Omega$, $\Prob(B) = \sum\limits_{i \in I} \Prob( B \cap A_i) = \sum\limits_{i \in I} \Prob(A_i) P_{A_i} (B)$.
   \\ \\
   La formule de Bayes, utilisée ci-dessus, donne $P_A(B) = \frac{ \Prob( A \cap B) }{\Prob(A)}$ pour tous évènements $A$ et $B$ tels que $\Prob(A) \neq 0$. \\ \\
   Une coccinelle se déplace sur un tétraèdre régulier PQRS
   (une pyramide) en longeant les ar\^{e}tes. Elle est placée \`{a}
   l'instant $n=0$ sur le sommet $P$. On suppose que, si. elle se, trouve sur
   un sommet \`{a} l'instant $n$, elle sera sur l'un des trois autres sommets 
   \`{a} l'instant  $n+1$ de, fa\c{c}on équiprobable.
   \\
   Pour tout $n\in \N$ on note $p_{n}$ (respectivement $q_{n},\ r_{n}$%
   et $s_{n}$) la probabilité que la coccinelle se trouve sur le sommet $P$
   (respectivement $Q$, $R$ et $S$) \`{a} l'instant $n$.

   \begin{enumerate}
   \item On définit la matrice colonne $X_{n}$ par 
     \begin{equation*}
       X_{n}=\left( 
         \begin{array}{c}
           p_{n} \\ 
           q_{n} \\ 
           r_{n} \\ 
           s_{n}%
         \end{array}%
       \right) 
     \end{equation*}%
     \\
     $A = \frac{1}{3} \begin{smatrix} 0 & 1 & 1 & 1 \\ 1 & 0 & 1 & 1 \\ 1 & 1 & 0 & 1 \\ 1 & 1 & 1 & 0 \\ \end{smatrix}$. \\

   \item Elle est symétrique donc diagonalisable. \\

   \item Pour simplifier le calcul des valeurs propres, on cherche un polynôme annulateur en calculant $A^2$ : \\
     On obtient $A^2 = \frac{1}{3} ( 2A + I)$ donc le polynôme $3 X^2 - 2 X - 1$, est annulateur, et les seules valeurs propres possibles sont ses racines $\frac{2-4}{6} = - \frac{1}{3}$ et $\frac{2+4}{6} = 1$. \\
     On résout alors les équations $A X = X$ et $A X = \frac{1}{3} X$, on en tire les vecteurs propres et les valeurs propres, puis $A = P D P^{-1}$ et enfin $A^n = P D^n P^{-1}$ et enfin un calcul explicite de $A^n$ (qui peut attendre éventuellement, on va voir pourquoi). \\
     On obtient ensuite par récurrence que $X_n = A^n X_0 = A^n \begin{smatrix} 1 \\ 0 \\ 0 \\ 0 \\ \end{smatrix}$ qui est la première colonne de $A^n$, qu'il faut donc calculer (attention, si on ne veut calculer que la première colonne de la matrice intermédiaire, il faut faire le calcul dans le bon ordre). \\

   \item $T (\Omega) = \llb 2 ; +\infty \llb$. \\
     De plus à l'instant 0 on est en P, et à l'instant 1 on est sur un des trois autres sommets, donc on a visité deux sommets de manière certaine. \\
     A partir de la, la coccinelle fait des allers retours entre $P$ et le deuxième sommet, jusqu'au moment ou elle change. \\
     On a donc $\Prob(\Ev{ T = k}) = 1 \times \left( \frac{1}{3} \right)^{k-2} \times \frac{2}{3} = \frac{2}{3^{k-1}}$. \\
     On a ensuite $\E(T) = \Sum{k=2}{+\infty} \frac{2k}{3^{k-1}} = 2 \left( \Sum{k=1}{+\infty} \frac{k}{3^{k-1}} -1 \right) = 2 \left( 1 - \frac{1}{ \left( \frac{2}{3} \right)^2} \right) = \frac{10}{4} = \frac{5}{2}$. \\

   \item On a $U (\Omega) = \llb 3 ; +\infty \llb$ et : \\
     $\Prob(\Ev{ U = l }) = \Sum{k=2}{l-1} \Prob(\Ev{ T=k}) P_{\Ev{T=k}} \Ev{ U = l} = \Sum{k=2}{l-1} \frac{2}{3^{k-1}}  \left(\frac{2}{3} \right)^{l-k-1} \frac{1}{3} =\Sum{k=2}{l-1} \frac{ 2^{l-k} }{3^{k-1+l-k-1 + 1} } =\frac{2^l}{3^{l-1} } \Sum{k=2}{l-1} \frac{ 1 }{2^k} = \frac{2^l}{3^{l-1} } \frac{1}{4} \frac{1 - \left( \frac{1}{2} \right)^{l-2}}{ \frac{1}{2} } = \frac{2^{l-1}}{3^{l-1} } - \frac{2}{3^{l-1}} = \frac{2^{l-1} - 2}{3^{l-1}}$. \\

   \item $\E(U) = \Sum{l=3}{+\infty} l \left( \frac{2}{3} \right)^{l-1} - 2  \Sum{l=3}{+\infty} l \left( \frac{1}{3} \right)^{l-1} = \frac{1}{ \left( \frac{1}{3} \right)^2} - 1 - \frac{4}{3} - 2  \frac{1}{ \left( \frac{2}{3} \right)^2} + 2 + \frac{4}{3} = 9 - \frac{9}{2} +1 = \frac{11}{2}$.
   \end{enumerate}

   \newpage

   \noindent \textbf{\underline{Exercice sans préparation} }
   \\ \\
   Un agriculteur souhaite améliorer le rendement de son exploitation en
   utilisant de l'engrais. Une étude a montré que le rendement, en
   tonnes par hectare, pour la variété de blé cultivée est donn%
   é par%
   \begin{equation*}
     f\left( B,N\right) =120B-8B^{2}+4BN-2N^{2}
   \end{equation*}%
   $B$ désigne la, quantité de semences de blé utilisée, $N$ la
   quantité d'engrais utilisée.

   \begin{enumerate}
   \item $f$ est de classe $C^2$, et on a : \\
     $f_B'(B,B) = 120 - 16 B+ 4N$, $f_N'(B,N) = 4 B - 4 N$, $f_{B,B}''(B,N) = -16$, $f_{B,N}''(B,N) = 4$ et $f_{N,N}'' (B,N) = -4$. \\ \\
     Les points vérifient $120 - 16 B + 4 N =0$ et $4B - 4 N=0$, donc $B=N$ et $120-12B=0$, et enfin $B=N=10$. \\
     Au point $(10,10)$ on a $r = -16$, $s = 4$ et $t = -4$ donc $rt - s^2 = 64 - 16 = 48 > 0$ et $r = -16 < 0$, c'est donc un maximum local. \\
     De plus on peut écrire $f(B,N) = 120 B - 6 B^2 -2 ( B^2 - 2 B N + N^2) = 120 B - 6 B^2 - 2 ( B- N)^2 = -6 [ (B-10)^2 -100]- 2 (B-N)^2 = -6 (B-10)^2 - 2 (B-N)^2 + 600 = -6 (B-10)^2 - 2 (B-N)^2 + f (10,10)$ donc $(10,10)$ est un maximum global de $f$. \\

   \item On traduit la contrainte : on a $B = 23 - 2N$ donc le rendement est donné par $g(N) = f( 23 - 2N , N) = 120 (23 - 2N) - 8 (23 - 2 N)^2 + 4 (23 - 2N) N - 2 N^2 = N^2 ( -32 -8 - 2) + N ( - 240 + 736 + 92) + (120 - 184) \times 23 = -42 N^2 + 588 N - 64 \times 23$. \\
     On dérive : $g'(N) = - 84 N + 588$, qui s'annule pour $N = \frac{588}{84} = \frac{294}{42} = \frac{147}{21} = \frac{21}{3} = 7$ et $B = 23 - 14 = 9$ donc le rendement optimum vaut $f( 9 , 7) = 1080 - 648 + 252 - 98 = 432 + 154 = 586$.
   \end{enumerate}
 \end{exercice}

 \newpage

 \begin{exercice} \indent \\
   \textbf{Question de cours :} Une matrice carrée d'ordre $n$ est diagonalisable si et seulement si la somme des dimensions de ses sous-espaces propres vaut $n$.
   \\ \\
   On considère l'endomorphisme de $\R^{4}$ dont la matrice dans la
   base canonique de $\R^{4}$ est la matrice 
   \begin{equation*}
     M=%
     \begin{smatrix}
       -1 & -1 & -1 & 1 \\ 
       -1 & -1 & 1 & -1 \\ 
       -1 & 1 & -1 & -1 \\ 
       1 & -1 & -1 & -1%
     \end{smatrix}%
   \end{equation*}

   \begin{enumerate}
   \item $M$ est symétrique donc diagonalisable, donc $f$ aussi. \\

   \item $M + 2 I$ est de rang 1 (ses quatre colonnes sont colinéaires) donc $\ker ( f + 2 \id)$ est de dimension 3. \\
     $-2$ est donc valeur propre et $E_{-2} (f)$ est de dimension 3. \\

   \item $f\left( 1,-1,-1,1\right) = 2 ( 1 , -1 , -1 , 1)$ donc $2$ est valeur propre de $M$ et de $f$. \\

   \item Pour $-2$, on trouve facilement que $(e_1 + e_2 , e_1 + e_3 , e_1 - e_4)$ est une base du sous-espace propre; pour $2$ on prend $(1 , 1 , -1 , 1)$ et on obtient une base de vecteurs propres, donc une base dans laquelle la matrice de $f$ est diagonale, égale à $\operatorname{diag} (-2 , -2 , -2 , 2)$ donc $M'^2 = \operatorname{diag}( 4 , 4 , 4, ,4 ) = 4 I$. \\

   \item Pour tout $n$, $M^n = P M'^n P^{-1}$. \\
     Si $n$ est pair, égal à $2k$, cela donne $M^{2k} = P ( M'^{2} )^{k} ) P^{-1} = P ( 4^k I ) P^{-1} = 4^k I = 2^{2k} I = 2^n I$. \\
     Si $n$ est impair égal à $2m+1$, $M^{2k+1} = M^{2k} M = 4^k I M = 2^{2k} M = 2^{n-1} M$. \\

   \item En posant $X_n$ le vecteur colonnes constitué des quatre suites, on a $X_{n+1} = \frac{1}{4} M X_n$, donc $X_n = \frac{1}{4^n} M^n X_0 = \frac{1}{2^n} X_0$ si $n$ est pair et $\frac{1}{2^{n+1}} M X_0$ si $n$ est impair. \\
     On voit que ces suites convergent toutes vers 0. \\
   \end{enumerate}

   \bigskip

   \noindent \textbf{\underline{Exercice sans préparation }}
   \\ \\
   Soient $A$, $B$, $C$, des événements de m\^{e}me probabilité $p$
   et tels que $\mathrm{P}\left( A\cap B\cap C\right) =0$

   \begin{enumerate}
   \item On a $\Prob( \overline{ A \cap B \cap C} ) = \Prob( \overline{A} \cup \overline{B} \cup \overline{C} ) = 1$ et $ \Prob( \overline{A} \cup \overline{B} \cup \overline{C} ) \leq \Prob( \overline{A}) + \Prob(\overline{B}) + \Prob( \overline{C}) = 3(1-p)$. \\
     On en déduit que $1 \leq 3(1-p)$, $1-p \geq \frac{1}{3}$ et enfin $p \leq \frac{2}{3}$. \\

   \item Il faut pour cela que les trois évènements $\overline{A} , \overline{B}$ et $\overline{C}$ soient incompatibles et de probabilité $\frac{1}{3}$, formant un système complet d'évènement. \\
     Par exemple en prenant $X \suit \mathcal{U} ( \{ 0 ; 1 ; 2 \} )$, les évènements $A = (X \neq 0)$, $B = (X \neq 1)$ et $C = (X \neq 2)$ vérifient $\Prob(A) = \Prob(B) = \Prob(C) = \frac{2}{3}$ et $P ( A \cap B \cap C) = 0$. \\

   \item On utilise la formule du crible pour faire apparaître $A \cap B$, $A \cap C$ et $B \cap C$ : \\
     $\Prob(A \cup B \cup C) = P ( A) + \Prob( B) + \Prob(C) - \Prob(A \cap B) - \Prob( A \cap C) - \Prob( B \cap C) + \Prob(A \cap B \cap C)$. \\
     D'où $P ( A \cup B \cup C) = 3 p - 3 p^2 = 3p (1-p)$. \\
     Or la fonction $f(p) = 3 p (1-p)$ atteint son maximum en $p = \frac{1}{2}$, où elle vaut $\frac{1}{4}$. \\ 
     On en déduit que $P ( A \cup B \cup C) \leq \frac{3}{4}$. \\
     Pour réutiliser l'hypothèse, on passe au contraire pour faire apparaître des intersections : \\ \\
     $\Prob( \overline{ A \cup B \cup C}) = P ( \overline{A} \cap \overline{B} \cap \overline{C} ) \geq \frac{1}{4}$. \\ \\
     On a de plus $( \overline{A} \cap \overline{B} \cap \overline{C} ) \subset ( \overline{A} \cap \overline{B} )$, donc : \\
     $\frac{1}{4} \leq P ( \overline{A} \cap \overline{B} \cap \overline{C} ) \leq P ( \overline{A} \cap \overline{B} ) = (1-p)^2$. \\ \\
     Enfin cela donne $1-p \geq \frac{1}{2}$, et $p \leq \frac{1}{2}$. \\
   \item Il faut avoir $\Prob( A) = \Prob( B) = \Prob( C)$, $\Prob( A \cap B) = \Prob( A \cap C) = \Prob( B \cap C) = \frac{1}{4}$, ces trois derniers étant deux à deux incompatibles (faire un dessin !!!!). \\ \\
     Il faut séparer $\Omega$ en quatre évènements de probabilité $\frac{1}{4}$ : le premier est $A \cap B$, le deuxième $A \cap C$ et le troisième $B \cap C$ : \\
     Exemple : $X \suit \mathcal{U} ( \{ 1 ; 2 ; 3 ; 4 \} )$. \\
     On pose $A = ( X \in \{ 1 ; 2 \} )$, $B = ( X \in \{ 1 ; 3\})$ et $C = ( X \in \{ 2 ; 3\} )$. \\
     On a $\Prob( A) = P (B) = \Prob( C) = \frac{1}{2}$, $\Prob( A \cap B \cap C) = 0$ et $\Prob( A \cap B) = \Prob( A \cap C) = \Prob( B \cap C) = \frac{1}{4}$ donc les évènements $A$, $B$ et $C$ sont deux à deux indépendants.
   \end{enumerate}
 \end{exercice}


\end{document}
