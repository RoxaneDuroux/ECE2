\documentclass[11pt]{article}%
\usepackage{geometry}%
\geometry{a4paper,
 lmargin = 2cm,rmargin = 2cm,tmargin = 2.5cm,bmargin = 2.5cm}

\input{../macros_Livre.tex}

\pagestyle{fancy} %
\lhead{ECE2 \hfill Mathématiques\\
} %
\chead{\hrule} %
\rhead{} %
\lfoot{} %
\cfoot{} %
\rfoot{\thepage} %

\renewcommand{\headrulewidth}{0pt}% : Trace un trait de séparation
 % de largeur 0,4 point. Mettre 0pt
 % pour supprimer le trait.

\renewcommand{\footrulewidth}{0.4pt}% : Trace un trait de séparation
 % de largeur 0,4 point. Mettre 0pt
 % pour supprimer le trait.

\setlength{\headheight}{14pt}

\title{\bf \vspace{-2cm} HEC 2017} %
\author{} %
\date{} %
\begin{document}

\maketitle %
\vspace{-1.4cm}\hrule %
\thispagestyle{fancy}

\vspace*{.2cm}

%%DEBUT

% 

\section*{EXERCICE}

\noindent
Pour tout $n\in\N^*$, on note $\M{n}$ l'ensemble des matrices carrés à
$n$ lignes et $n$ colonnes à coefficients réels et $\Bc{n}$ l'ensemble
des matrices de $\M{n}$ dont tous les coefficients sont égaux à $0$ ou
à $1$.
\begin{noliste}{1.}
  \setlength{\itemsep}{4mm}
\item {\it Exemple 1}. Soit $A$ la matrice de $\Bc{2}$ définie par :
  $A = 
  \begin{smatrix} 
    0 & 1 \\ 
    1 & 0
  \end{smatrix}$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Calculer la matrice $A^2$.
    
    \begin{proof}~\\[-.6cm]%
      \conc{$A^2 = %
        \begin{smatrix}
          0 & 1 \\
          1 & 0
        \end{smatrix} 
        \begin{smatrix}
          0 & 1 \\ 
          1 & 0
        \end{smatrix} = I_2$.}~\\[-1cm]
    \end{proof}
    
  \item Quelles sont les valeurs propres de $A$ ?
	
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item D'après la question \itbf{1.a)}, $A^2 - I_2 = 0$.\\
        On en déduit que le polynôme $P(X) = X^2-1 = (X-1)(X+1)$ est
        un polynôme annulateur de $A$. Le polynôme $P$ admet donc $1$
        et $-1$ comme racines.\\
        Or, le spectre de $A$ est inclus dans l'ensemble des racines
        de $P$. %
        \conc{Autrement dit : $\spc(A) \subset \{-1, 1\}$.}

      \item Vérifions que $1$ est valeur propre de $A$.
	\[
	\det (A-I_2) = \det
        \left(
        \begin{smatrix}
          -1 & 1 \\
          1 & -1
	\end{smatrix}
        \right) = 1 - 1 = 0
	\]
	Ainsi, $A-I_2$ n'est pas inversible. Donc $1$ est valeur
        propre de $A$.
      \item Vérifions que $-1$ est valeur propre de $A$.
	\[
	\det (A+I_2) = \det \left(
          \begin{smatrix} 
            1 & 1 \\
            1 & 1
          \end{smatrix} \right) = 1 - 1 = 0
	\]
        Donc $A+I_2$ n'est pas inversible. Donc $-1$ est valeur propre
        de $A$.
      \end{noliste}
      \conc{$\spc(A)=\{-1,1\}$.}~\\[-1.4cm]      
      ~\\[-1.4cm]
    \end{proof}
	

    %\newpage


  \item La matrice $A$ est-elle diagonalisable ?
	
    \begin{proof}~\\
      La matrice $A$ est carrée d'ordre $2$ et admet $2$ valeurs
      propres {\bf distinctes}. %
      \conc{Ainsi, la matrice $A$ est diagonalisable.}~\\[-1cm]
    \end{proof}
  \end{noliste}

\item {\it Exemple 2}. Soit $B$ la matrice de $\Bc{3}$ définie par :
  $B =
  \begin{smatrix} 
    0 & 1 & 0 \\
    1 & 0 & 0 \\
    0 & 0 & 1
  \end{smatrix}$.\\
  On considère les instructions et la sortie ({\tt
    ans}) % (-$\rightarrow$)
  \Scilab{} suivantes :
  \begin{scilab}
    & B = [0,1,0;1,0,0;0,0,1] \nl %
    & P = [1,1,0;1,-1,0;0,0,1] \nl %
    & inv(P) \Sfois{} B \Sfois{} P \nl %
  \end{scilab}
  % \hspace*{1cm}   -$\rightarrow$ \\
  % \\
  % \hspace*{1cm} $ \quad\begin{matrix}
  %   1. & 0. & 0.\\
  %   0. & -1. & 0.\\
  %   0. & 0. & 1.
  % \end{matrix}$
  \[
  \begin{console}
    \lDisp{\qquad ans \ =} \nl %
    \lDisp{\qquad \qquad 1. \quad 0. \quad 0.} \nl %
    \lDisp{\qquad \qquad 0. \quad \moins 1. \quad 0.} \nl %
    \lDisp{\qquad \qquad 0. \quad 0. \quad 1.} \nle %
  \end{console}
  \]

  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Déduire les valeurs propres de $B$ de la séquence \Scilab{}
    précédente.
    
    \begin{proof}~\\[.2cm]
      Notons $D = 
      \begin{smatrix} 
        1 & 0 & 0 \\ 
        0 & -1 & 0 \\ 
        0 & 0 & 1
      \end{smatrix}$ et $P = 
      \begin{smatrix} 
        1 & 1 & 0 \\ 
        1 & -1 & 0 \\ 
        0 & 0 & 1
      \end{smatrix}$.\\[.2cm]
      D'après la séquence \Scilab{}, $P^{-1}BP = D$, d'où $B =
      PDP^{-1}$. \\
      Ainsi, $B$ est semblable à une matrice diagonale.\\
      Elle est donc diagonalisable et ses valeurs propres sont les
      coefficients diagonaux de $D$. %
      \conc{$\spc(B) = \{-1,1\}$}~\\[-1cm]
    \end{proof}
   	
  \item Déterminer une base de chacun des sous-espaces propres de $B$.
   	
    \begin{proof}~%
      \begin{noliste}{$\sbullet$}
      \item Le programme \Scilab{} précédent nous fournit la
        diagonalisation de la matrice $B$ :
        \[
        B = PDP^{-1}
        \]
        La famille ${\cal F} = \left(
          \begin{smatrix}
            1 \\
            1 \\
            0
          \end{smatrix},
          \begin{smatrix}
            1 \\
            -1 \\
            0
          \end{smatrix},
          \begin{smatrix}
            0 \\
            0 \\
            1
          \end{smatrix}\right)$ constituée des colonnes de la matrice
        $P$ est une base de vecteurs propres de $B$. Plus
        précisément, pour tout $i \in \llb 1, 3 \rrb$, le $\eme{i}$
        vecteur de ${\cal F}$ est un vecteur propre associé au
        coefficient diagonal $d_{i,i}$.

      \item On en déduit :
        \[
        E_{-1}(B) \supset \Vect{
          \begin{smatrix}
            1 \\
            -1 \\
            0
          \end{smatrix}} 
        \qquad \text{ et } \qquad
        E_1(B) \supset \Vect{
          \begin{smatrix}
            1 \\
            1 \\
            0
          \end{smatrix},
          \begin{smatrix}
            0 \\
            0 \\
            1
          \end{smatrix}}
        \]


        %\newpage


      \item Ainsi, $\dim(E_{-1}(B)) \geq 1$ et $\dim(E_{1}(B)) \geq
        2$.\\
        Or, comme $B$ est diagonalisable :
        \[
        \dim(E_{-1}(B)) + \dim(E_{1}(B)) = \dim(\M{3,1}) = 3
        \]
        On en déduit : $\dim(E_{-1}(B)) = 1$ et $\dim(E_{1}(B)) = 2$.%
        \conc{Ainsi : $E_{-1}(B) = \Vect{
            \begin{smatrix}
              1 \\
              -1 \\
              0
            \end{smatrix}}$ \quad et \quad $E_1(B) = \Vect{
            \begin{smatrix}
              1 \\
              1 \\
              0
            \end{smatrix},
            \begin{smatrix}
              0 \\
              0 \\
              1
            \end{smatrix}}$.}
      \end{noliste}
      ~\\[-1.4cm]
      \end{proof}
    \end{noliste}


%\newpage


\item 
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Combien existe-t-il de matrices appartenant à $\Bc{n}$ ?
	
    \begin{proof}~\\%
      Une matrice $M$ de $\Bc{n}$ est une matrice de $\M{n}$ dont
      chaque coefficient vaut soit $0$ soit $1$.\\
      Une telle matrice est entièrement déterminée par :
      \begin{noliste}{$\stimes$}
      \item le choix du coefficient $m_{11}$ : 2 possibilités.
      \item \ldots
      \item le choix du coefficient $m_{1n}$ : 2 possibilités.
      \item le choix du coefficient $m_{21}$ : 2 possibilités.
      \item \ldots
      \item le choix du coefficient $m_{2n}$ : 2 possibilités.
      \item \ldots
      \item le choix du coefficient $m_{n1}$ : 2 possibilités.
      \item \ldots
      \item le choix du coefficient $m_{nn}$ : 2 possibilités.
      \end{noliste}
      Il y a donc $2^{n^2}$ telles matrices.%
      \conc{On en déduit : $\Card(\Bc{n}) = 2^{n^2}$.}~\\[-1cm]
    \end{proof}
    
  \item Combien existe-t-il de matrices de $\Bc{n}$ dont chaque ligne
    et chaque colonne comporte exactement un coefficient égal à $1$
    ?
	
    \begin{proof}~\\
      Une matrice $M$ de $\Bc{n}$ dont chaque ligne et chaque colonne
      comporte exactement un coefficient égal à $1$ est entièrement
      déterminée par :
      \begin{noliste}{$\stimes$}
      \item la position du $1$ sur la $\ere{1}$ ligne : $n$
        possibilités.\\
        {\it (on peut le placer sur n'importe quelle colonne)}
      \item la position du $1$ sur la $\eme{2}$ ligne : $n-1$
        possibilités.\\
        {\it (on peut le placer sur n'importe quelle colonne non déjà
          choisie)}
      \item la position du $1$ sur la $\eme{3}$ ligne : $n-2$
        possibilités.\\
        {\it (on peut le placer sur n'importe quelle colonne non déjà
          choisie)}
      \item \ldots
      \item la position du $1$ sur la $\eme{n}$ ligne : $1$
        possibilité.\\
        {\it (on peut le placer sur n'importe quelle colonne non déjà
          choisie)}
      \end{noliste}
      Il existe donc $n \ (n-1) \ (n-2) \cdots 1 = n!$ telles
      matrices.~\\[-.8cm]%
      \concL{L'ensemble des matrices de $\Bc{n}$ dont chaque ligne et
        chaque colonne comporte exactement un coefficient égal à
        $1$ comporte exactement $n!$ éléments.}{15.4}%~\\[-1cm]
      ~\\[-1.4cm]
    \end{proof}
  \end{noliste}


  %\newpage

  
\item Dans cette question, $n$ est un entier supérieur ou égal à $2$.\\
  Soit $E$ un espace vectoriel de dimension $n$ et $u$ un
  endomorphisme de $E$. On note :
  \begin{noliste}{$-$}
  \item $\id$ l'endomorphisme identité de $E$ ;
  \item $F$ le noyau de l'endomorphisme $(u+\id)$ et $G$ le noyau de
    l'endomorphisme $(u-\id)$ ;
  \item $p$ la dimension de $F$ et $q$ la dimension de $G$.
  \end{noliste}
  On suppose que $u \circ u =\id$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier que l'image de $(u-\id)$ est incluse dans $F$.
	
    \begin{proof}~\\
      Il s'agit de montrer que $\im(u-\id) \subset \kr(u + \id) = F$.\\[.2cm]
      Soit $y \in \im(u - \id)$. Par définition, il existe $x\in E$
      tel que $y = (u-\id)(x) = u(x)-x$.\\
      Démontrons que $y \in \kr(u + \id)$.
      \[
      \begin{array}{rcl@{\quad}>{\it}R{3.5cm}}
	(u + \id)(y) & = & u(y) + y
        \\[.2cm]
	& = & u(u(x)-x)+(u(x)-x) & (par définition de $y$) 
        \nl
        \nl[-.2cm]
	& = & u(u(x)) - \bcancel{u(x)}+\bcancel{u(x)}-x & (par
        linéarité de $u$) 
        \nl
        \nl[-.2cm]
	& = & \bcancel{x} - \bcancel{x} \ = \ 0_E & (car $u\circ u=\id$)
        % \nl
        % \nl[-.2cm]
        % & = & 0_E
      \end{array}
      \]
      Donc $y\in\kr(u+\id) = F$.%
      \conc{$\im(u-\id)\subset F$}~\\[-1cm]
    \end{proof}
    
  \item En déduire l'inégalité : $p + q \geq n$.
    
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item D'après la question \itbf{4.a)}, $\im(u-\id) \subset \kr(u
        + \id)$.%
	\conc{Ainsi : $\dim(\im(u-\id)) \leq \dim(\kr(u+\id)) = p$.}

      \item D'après le théorème du rang :
        % appliqué à l'endomorphisme $u-\id$,
	\[
        \begin{array}{ccccc}
          \dim(E) & = & \dim(\kr(u-\id)) & + & \dim(\im(u-\id))
          \\[.2cm]
          \shortparallel & & \shortparallel & & 
          \\[.2cm]
          n & & q & & 
        \end{array}        
	\]
	\conc{$\dim(\im(u-\id)) = n - q$}~

      \item En combinant ces deux résultats, on obtient : $n - q \leq
        p$.\\%
        \conc{Ainsi, on a bien : $p + q \geq n$.}~\\[-1.4cm]
      \end{noliste}
    \end{proof}
    {\it On suppose désormais que $1\leq p<q$.}\\
    Soit $(f_1, f_2, \ldots, f_p)$ une base de $F$ et $(g_1, g_2,
    \ldots, g_q)$ une base de $G$.

  \item Justifier que $(f_1, f_2, \hdots, f_p, \ g_1,g_2,\hdots,g_q)$
    est une base de $E$.
    
    \begin{proof}~%
      \begin{noliste}{$\sbullet$}
      \item On suppose ici que $\dim(\kr(u + \id)) = p \geq 1$. En
        particulier : $\kr(u + \id) \neq \{ 0_E \}$.%
        \conc{Ainsi, $-1$ est valeur propre de $u$ et $F = \kr(u +
          \id) = E_{-1}(u)$.}

      \item De même, $\dim(\kr(u - \id)) = q > 1$. En particulier :
        $\kr(u - \id) \neq \{ 0_E \}$.%
        \conc{Ainsi, $1$ est valeur propre de $u$ et $G = \kr(u -
          \id) = E_{1}(u)$.}


        %\newpage

        
      \item On a alors :
        \begin{noliste}{$\stimes$}
        \item la famille $(f_1, f_2, \ldots, f_p)$ est une base de
          $E_{-1}(u)$.\\
          En particulier, c'est donc une famille libre de $E_{-1}(u)$.
        \item la famille $(g_1, g_2, \ldots, g_q)$ est une base de
          $E_{1}(u)$.\\
          En particulier, c'est donc une famille libre de $E_{1}(u)$.
        \end{noliste}
        La famille ${\cal F} = (f_1, f_2, \ldots, f_p, \ g_1, g_2,
        \ldots, g_q)$ est la concaténation de deux familles libres de
        sous-espaces propres associés à des valeurs propres
        distinctes.%
        \conc{Ainsi, ${\cal F}$ est une famille libre de $E$.}
      \end{noliste}
      


    %\newpage


    \begin{noliste}{$\sbullet$}
    \item La famille ${\cal F}$ est une famille libre de $E$, espace
      vectoriel de dimension $n$. Ainsi :
      \[
      \Card({\cal F}) = p+q \ \leq \ n = \dim(E)
      \]
      Or, d'après la question précédente : $p+q \geq n$.%
      \conc{On en déduit : $p + q = n$.}
      
    \item En résumé :
      \begin{noliste}{$\stimes$}
      \item la famille ${\cal F}$ est libre.
      \item $\Card({\cal F}) = n = \dim(E)$.
      \end{noliste}
      \conc{La famille ${\cal F} = (f_1, f_2, \ldots, f_p, g_1, g_2,
        \ldots, g_q)$ est donc une base de $E$.}~\\[-1.6cm]
    \end{noliste}    
    \end{proof}
  
  \item Calculer $u(g_1-f_1)$ et $u(g_1+f_1)$.
    
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Tout d'abord : 
        \[
        \begin{array}{rcccc@{\quad}>{\it}R{4cm}}
          u(g_1 - f_1) & = & u(g_1) & - & u(f_1) & (par linéarité de $u$) 
          \nl
          \nl[-.2cm]
          & = & 1 \cdot g_1 & - & (-1) \cdot f_1 & (car $g_1 \in
          E_1(u)$ \\ et $f_1 \in E_{-1}(u)$) 
          \nl
          \nl[-.2cm]
          & = & g_1 & + & f_1
        \end{array}
        \]

      \item On démontre de même : $u(g_1+f_1) = u(g_1) + u(f_1) =
        g_1-f_1$.%
      \end{noliste}
      \conc{$u(g_1-f_1) = g_1+f_1$ \quad et \quad $u(g_1+f_1) =
        g_1-f_1$}~\\[-1.2cm]
    \end{proof}
    
  \item Trouver une base de $E$ dans laquelle la matrice de $u$
    appartient à $\Bc{n}$.
	
    \begin{proof}~\\
      On considère $\B'$ la famille :
      \[
      \B' = (g_1 - f_1, \ g_1 + f_1, \ g_2 - f_2, \ g_2 + f_2, \ldots,
      \ g_p - f_p, \ g_p + f_p, \ g_{p+1} , \ g_{p+2}, \ldots, \ g_q)
      \]
      (qui est bien définie car $p < q$)	
      \begin{noliste}{$\sbullet$}
      \item Montrons tout d'abord que $\B'$ est une famille libre.\\
        Soit $(\lambda_1, \hdots, \lambda_p, \mu_1, \hdots, \mu_p,
        \gamma_{p+1},\hdots,\gamma_q)\in\R^n$. Supposons :
        \[
        \lambda_1 \cdot (g_1-f_1) + \mu_1 \cdot (g_1+f_1) + \ldots +
        \lambda_p \cdot (g_p-f_p) + \mu_p \cdot (g_p+f_p) +
        \gamma_{p+1} \cdot g_{p+1} + \ldots + \gamma_q \cdot g_q = 0_E
        \]
        En réordonnant :
        \[
        (\mu_1-\lambda_1) \cdot f_1 + \ldots + (\mu_p-\lambda_p) \cdot
        f_p + (\mu_1+\lambda_1) \cdot g_1 + \ldots + (\mu_p+\lambda_p)
        \cdot g_p + \gamma_{p+1} \cdot g_{p+1} + \ldots + \gamma_q
        \cdot g_q = 0_E
        \]
        Or, d'après la question \itbf{4.c)},
        $(f_1,\hdots,f_p,g_1,\hdots,g_q)$ est une base de $E$.\\
        C'est donc une famille libre. Ainsi :
        \[
        \left\{
          \begin{array}{l}
            \forall i\in\llb 1,p\rrb, \quad \mu_i-\lambda_i=0 \\[.2cm]
            \forall i\in\llb 1,p\rrb, \quad \mu_i+\lambda_i=0 \\[.2cm]
            \forall j\in\llb p+1,q\rrb, \quad \gamma_j=0 
          \end{array}
        \right.
        \]
        Or, pour tout $i\in\llb 1,p\rrb$ :
        \[
        \begin{array}{ccccl}
        \left\{
          \begin{matrix}
            \mu_i & - & \lambda_i & = & 0 \\
            \mu_i & + & \lambda_i & = & 0
          \end{matrix}
        \right. 
        &
        \begin{arrayEq}
          L_2 \leftarrow L_2 - L_1
        \end{arrayEq}
        & 
        \left\{
          \begin{matrix}
            \mu_i & - & \lambda_i & = & 0 \\
            & & 2 \ \lambda_i & = & 0
          \end{matrix}
        \right. 
        &
        \Longleftrightarrow
        &
        \left\{
          \begin{matrix}
            \mu_i & & & = & 0 \\
            & & \lambda_i & = & 0
          \end{matrix}
        \right.
        \\[.4cm]
        & & & & \text{\it (par remontées successives)}
      \end{array}
      \]
      Ainsi : $\lambda_1 = \ldots = \lambda_p = \mu_1 = \ldots = \mu_p
      = \gamma_{p+1} = \ldots = \gamma_q = 0$. %
      \conc{La famille $\B'$ est libre.}


      %\newpage


    \item On a alors : 
      \begin{noliste}{$\stimes$}
      \item la famille $\B'$ est libre.
      \item $\Card(\B') = n = \dim(E)$.
      \end{noliste}~\\[-1cm]
      \conc{Ainsi, $\B'$ est une base de $E$.}
    
    \item Déterminons la matrice de $u$ dans cette base.\\
      Pour plus de lisibilité, notons : $\forall i \in \llb 1,p\rrb$,
      $r_i = g_i - f_i$ \ et \ $s_i = g_i + f_i$.\\
      On démontre, par le même raisonnement que dans la question
      précédente : 
      \[
      \forall i \in \llb 1, p \rrb, \ u(g_i - f_i) = g_i + f_i \quad
      \text{ et } \quad u(g_i + f_i) = g_i - f_i
      \]
      La base $\B'$ s'écrit alors : $\B' = (r_1, \ s_1, \ \ldots, \
      r_i, \ s_i, \ldots, \ r_p, \ s_p, \ g_{p+1}, \ \ldots, \ g_q)$
      et :
      \[
      \left\{
        \begin{array}{l}
          \forall i\in\llb 1, p \rrb, \quad u(r_i) = s_i
          \\[.2cm]
          \forall i \in\llb 1, p \rrb, \quad u(s_i) = r_i 
          \\[.2cm]
          \forall j \in\llb p+1, q \rrb, \quad u(g_j) = g_j
        \end{array}
      \right.      
      \]
      On en déduit la matrice représentative de $u$ dans la base $\B'$
      :
      \[
      \Mat_{\B'}(u) = %
      \scalebox{.8}{$
        \begin{array}{l}
          \begin{array}{C{.cm}*{8}{>{$}C{.8cm}<{$}}>{$}C{1cm}<{$}*{2}{>{$}C{.8cm}<{$}}}
            & u(r_1) & u(s_1) & \ldots & u(r_i) & u(s_i) & \ldots & u(r_p)
            & u(s_p) & u(g_{p+1}) & \ldots & u(g_q)
          \end{array}
          \\[.6cm]
          \left(
            \begin{array}{*{8}{>{$}C{.8cm}<{$}}>{$}C{1cm}<{$}*{2}{>{$}C{.8cm}<{$}}}
              0 & 1 & & & & & & & 0 & \cdots & 0
              \nl
              \nl[-.2cm]
              1 & 0 & & & & & & & \vdots & & \vdots
              \nl
              \nl[-.2cm]
              & & \ddots & & & & & & \vdots & & \vdots
              \nl
              \nl[-.2cm]
              & & & 0 & 1 & & & & \vdots & & \vdots
              \nl
              \nl[-.2cm]
              & & & 1 & 0 & & & & \vdots & & \vdots
              \nl
              \nl[-.2cm]
              & & & & & \ddots & & & \vdots & & \vdots
              \nl
              \nl[-.2cm]
              & & & & & & 0 & 1 & \vdots & & \vdots
              \nl
              \nl[-.2cm]
              & & & & & & 1 & 0 & 0 & \cdots & 0
              \nl
              \nl[-.2cm]
              0 & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots
              & 0 & 1 & &  
              \nl
              \nl[-.2cm]
              \vdots & & & & & & & \vdots & & \ddots & 
              \nl
              \nl[-.2cm]
              0 & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots
              & 0 & & & 1 
            \end{array}
          \right)
          \begin{array}{>{$}C{.8cm}<{$}}
            r_1 \phantom{0}
            \nl
            \nl[-.2cm]
            s_1 \phantom{\vdots}
            \nl
            \nl[-.2cm]
            \vdots 
            \nl
            \nl[-.2cm]
            r_i \phantom{\vdots}
            \nl
            \nl[-.2cm]
            s_i \phantom{\vdots}
            \nl
            \nl[-.2cm]
            \vdots
            \nl
            \nl[-.2cm]
            r_p \phantom{\vdots}
            \nl
            \nl[-.2cm]
            s_p \phantom{0}
            \nl
            \nl[-.2cm]
            g_{p+1} \phantom{0}
            \nl
            \nl[-.2cm]
            \vdots 
            \nl
            \nl[-.22cm]
            g_q \phantom{0}
          \end{array}
        \end{array}
        $}
    \]%~\\[-.4cm]
    \conc{Dans la base $\B'$, la matrice de $u$ appartient à
      $\Bc{n}$.}
  \end{noliste}
  ~\\[-1.4cm]
\end{proof}
\end{noliste}
%       \[
%       \Mat_{\B'}(u) = %
%       \begin{array}{l}
%         \left(
%         \begin{array}{cccccccccccccc}        
%           0 & 1 & 0 & 0 & \cdots & 0 & 0
%           & 0 & 0 & \cdots &
%           0 &  0 &  \cdots &  0 \\
%           1 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & 0 & \cdots & 0 & 0 &
%           \cdots & 0 \\
%           0 & 0 & 0 & 1 & \cdots & 0 & 0 & 0 & 0 & \cdots &
%           0 &  0 &  \cdots &  0 \\
%           0 & 0 & 1 & 0 & \cdots & 0 & 0 & 0 & 0 & \cdots & 0 & 0 &
%           \cdots & 0 \\
%           \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots &
%           \vdots & \vdots & \vdots & & \vdots & &
%           \vdots \\
%           0 & 0 & 0 & 0 & \cdots & 0 & 1 & 0 & 0 & \cdots &
%           0 &  0 &  \cdots &  0 \\
%           0 & 0 & 0 & 0 & \cdots & 1 & 0 & 0 & 0 & \cdots &
%           0 &  0 &  \cdots &  0 \\
%           0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & 1 & \cdots &
%           0 &  0 &  \cdots &  0 \\
%           0 & 0 & 0 & 0 & \cdots & 0 & 0 & 1 & 0 & \cdots &
%           0 &  0 &  \cdots &  0 \\
%           \vdots & \vdots & \vdots & \vdots & & \vdots & \vdots & \vdots
%           & \vdots & \ddots & \vdots & \vdots &
%           &  \vdots \\
%           0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & 0 & \cdots &
%           1 &  0 &  \cdots &  0 \\
%           0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & 0 & \cdots &
%           0 &  1 &  \cdots &  0 \\
%           \vdots & \vdots & \vdots & \vdots & & \vdots & \vdots & \vdots
%           & \vdots & & \vdots & \vdots &
%           \ddots &  \vdots \\
%           0 & 0 & 0 & 0 & \cdots & 0 & 0 & 0 & 0 & \cdots &
%           0 &  0 &  \cdots &  1 \\
%         \end{array}
%       \right)
%       \begin{array}{c}
%         g_1-f_1 \\ 
%         g_1+f_1 \\ 
%         g_2-f_2 \\
%         g_2+f_2 \\ 
%         \vdots \\ 
%         g_i-f_i \\
%         g_i+f_i \\ 
%         g_{i+1}-f_{i+1} \\
%         g_{i+1}+f_{i+1} \\ 
%         \vdots \\ 
%         g_{p+1}\\ 
%         g_{p+2}\\ 
%         \vdots \\ 
%         g_q
%       \end{array}
%     \end{array}
%     \]


%\newpage


%L}
\end{noliste}


%\newpage


\section*{PROBLÈME}

\noindent %
{\it Les tables de mortalité sont utilisées en démographie et en
  actuariat pour prévoir l'espérance de vie des individus d'une
  population. On s'intéresse dans ce problème à un modèle qui permet
  d'ajuster la durée de vie à des statistiques portant sur les décès
  observés au sein d'une génération.}\\
{\bf Dans tout le problème}, on note :
\begin{noliste}{$\sbullet$}
\item $a$ et $b$ deux réels strictement positifs ;
\item $(\Omega,\mathcal{A},\Prob)$ un espace probabilisé sur lequel sont
  définies toutes les variables aléatoires du problème ;
\item $G_{a,b}$ la fonction définie sur $\R_+$ par :
  $G_{a,b}(x)=\exp\left(-ax-\dfrac{b}{2}x^2\right)$.
\end{noliste}

\subsection*{Partie I. Loi exponentielle linéaire}

\begin{noliste}{1.}
\item
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Montrer que la fonction $G_{a,b}$ réalise une bijection de
    $\R_+$ sur l'intervalle $]0,1]$.
    
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item La fonction $G_{a, b}$ est de classe $\Cont{\infty}$ sur $[0,
        +\infty[$ car elle est la composée $G_{a, b} = g_2 \circ g_1$ 
	où :
      \end{noliste}
      \begin{liste}{$\stimes$}
      \item $g_1 : x \mapsto -a x - \dfrac{b}{2} x^2$ est :
        \begin{noliste}{-}
        \item de classe $\Cont{\infty}$ sur $[0, +\infty[$ car
          polynomiale,
        \item telle que : $g_1([0, +\infty[) \subset \R$.
        \end{noliste}
      \item $g_2 : x \mapsto \exp(x)$, de classe $\Cont{\infty}$ sur $\R$.
      \end{liste}

      \begin{noliste}{$\sbullet$}
      \item Pour tout $x \in [0, +\infty[$ :
        \[
        \begin{array}{cccccc}
          G'_{a, b}(x) = (- a - b x) \ \exp\left( -a x - \dfrac{b}{2}
            x^2 \right) & = & - & (a+bx) & \exp\left( -a x - \dfrac{b}{2} x^2
          \right) & < \ 0 \\[-.5cm]
          & & & \bbacc{1.4cm} & \bbacc{2.4cm} & \\[.4cm]
          & & & > 0 & > 0 
        \end{array}
        \]
        La fonction $G_{a, b}$ est donc strictement décroissante sur
        $[0, +\infty[$.

      \item La fonction $G_{a,b}$ est :
        \begin{noliste}{$\stimes$}
        \item continue sur $[0, +\infty[$,
        \item strictement décroissante sur $[0, +\infty[$.        
        \end{noliste}
        Ainsi, $G_{a,b}$ réalise une bijection de $[0, +\infty[$ sur
        $G_{a,b}([0,+\infty[) = \ ]\dlim{x \tend +\infty} G_{a,b}(x),
        G_{a,b}(0)]$.\\
        Enfin : $G_{a, b}(0) = \exp(0) = 1$.\\
        Et : $\dlim{x \tend +\infty} G_{a,b}(x) = 0$ car $\dlim{x
          \tend +\infty} \left(-a x - \dfrac{b}{2} x^2\right) = - 
	\infty$.
      \end{noliste}
      \conc{Ainsi, $G_{a, b}$ réalise une bijection de $[0, +\infty[$
        sur $]0, 1]$.}~\\[-1.2cm]
    \end{proof}
    

    %\newpage


  \item Pour tout réel $y>0$, résoudre l'équation d'inconnue $x\in\R$
    : $ax + \dfrac{b}{2} x^2 = y$.

    \begin{proof}~\\
      Soit $y > 0$. Notons : $P(x) = \dfrac{b}{2} \ x^2 + a x - y$.
      \begin{noliste}{$\sbullet$}
      \item Calculons le discriminant du polynôme $P$ :
        \[
        \Delta = a^2 - 4 \times \dfrac{b}{2} \times (-y) = a^2 + 2by \
        > \ 0
        \]

      \item On en déduit que $P$ admet exactement deux racines notées
        $r_+$ et $r_-$ :
        \[
        r_+ = \dfrac{-a + \sqrt{\Delta}}{2 \frac{b}{2}} = \dfrac{-a +
          \sqrt{a^2 + 2by}}{b} \qquad \mbox{ et } \qquad r_- =
        \dfrac{-a - \sqrt{\Delta}}{2 \frac{b}{2}} = - \dfrac{a +
          \sqrt{a^2 + 2by}}{b}
        \]

      \item Or $b > 0$ et $y > 0$ donc $a^2 + 2by > a^2$ et $\sqrt{a^2
          + 2by} > \sqrt{a^2} = \vert a \vert = a$. \\
          On en déduit que $r_+ > 0$.\\[.2cm]
        D'autre part, $r_- < 0$ car $a > 0$, $\sqrt{a^2 + 2by} > 0$ et
        $b > 0$.
      \end{noliste}
      \conc{L'équation $ax + \dfrac{b}{2} x^2 = y$ admet deux
        solutions : $r_+ > 0$ et $r_- < 0$.}~\\[-1cm]
    \end{proof}

  \item On note $G_{a,b}^{-1}$ la bijection réciproque de
    $G_{a,b}$. \\
    Quelle est, pour tout $u\in[0,1[$, l'expression de
    $G_{a,b}^{-1}(1-u)$ ?

    \begin{proof}~\\
      Soit $u \in [0, 1[$. 
      \begin{noliste}{$\sbullet$}
      \item On remarque tout d'abord que $1 - u \in \ ]0, 1]$. Notons
        alors : $v = G_{a,b}^{-1}(1-u)$. \\
        Par définition de $G_{a,b}$ et $G^{-1}_{a,b}$ :
        \[
        \begin{array}{crcl}
          & v & = & G^{-1}_{a,b}(1-u) \\[.4cm]
          \Leftrightarrow & G_{a,b}(v) & = & 1 - u \\[.4cm]
          \Leftrightarrow & \exp\left(- a v - \dfrac{b}{2} v^2 \right) &
          = & 1 - u \\[.4cm] 
          \Leftrightarrow & - a v - \dfrac{b}{2} v^2 & = & \ln(1 - u) \\[.4cm]
          \Leftrightarrow & a v + \dfrac{b}{2} v^2 & = & -\ln(1 - u) 
        \end{array}
        \]

      \item Notons alors : $y = -\ln(1-u)$. Comme $1-u \in \ ]0,1]$,
        $\ln(1-u) \in \ ]-\infty, 0]$ et donc $y \geq 0$.\\
        On retombe alors sur l'équation de la question précédente dont
        la résolution est valable pour $y = 0$ (car on a toujours dans
        ce cas $\Delta > 0$).\\
        Cette équation admet pour solution :
        \[
        r_+ = \dfrac{-a + \sqrt{a^2 + 2by}}{b} = \dfrac{-a + \sqrt{a^2
            - 2b \ \ln(1-u)}}{b} \ \geq 0 \quad \mbox{ et } \quad r_- < 0
        \]

      \item Or, comme $G^{-1}_{a,b}$ est à valeurs dans $[0, +\infty[$,
        on en déduit :
        \[
        v = G^{-1}_{a,b}(1-u) \ \Leftrightarrow \ v = \dfrac{-a +
          \sqrt{a^2 - 2b \ \ln(1-u)}}{b}
        \]
      \end{noliste}
      \conc{Pour tout $u \in [0,1[$, $G^{-1}_{a,b}(1-u) = \dfrac{-a +
          \sqrt{a^2 - 2b \ \ln(1-u)}}{b}$.}~\\[-1cm]
    \end{proof}
  \end{noliste}


  %\newpage


\item
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier la convergence de l'intégrale $\dint{0}{+\infty}
    G_{a,b}(x)\dx$.

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item L'intégrale $\dint{0}{1} G_{a,b}(x) \dx$ est bien définie
        comme intégrale sur le segment $[0,1]$ de la fonction
        $G_{a,b}$ continue sur $[0,1]$.

      \item
        \begin{noliste}{$\stimes$}
        \item Or : $G_{a,b}(x) = \exp\left(-ax - \dfrac{b}{2} \
            x^2\right) = \oox{+\infty} \left( \dfrac{1}{x^2}
          \right)$.\\
          En effet :
          \[
          x^2 \ G_{a,b}(x) = x^2 \ \exp\left(-ax - \dfrac{b}{2} \
            x^2\right) = \dfrac{x^2}{(\ee^a)^x} \times
          \dfrac{1}{\ee^{\frac{b}{2} x^2}} \tendx{+\infty} 0 \times 0
          = 0
          \]
          par croissances comparées.

        \item $\forall x \in [1, +\infty[$, $G_{a,b}(x) \geq 0$ \quad
          et \quad $\dfrac{1}{x^2} \geq 0$.

        \item L'intégrale impropre $\dint{1}{+\infty} \dfrac{1}{x^2}
          \dx$ est convergente en tant qu'intégrale de Riemann
          impropre en $+\infty$, d'exposant $2$ ($2 > 1$).
        \end{noliste}%~\\
        Par critère d'équivalence des intégrales généralisées de
        fonctions continues positives, l'intégrale impropre
        $\dint{1}{+\infty} G_{a,b}(x) \dx$ est elle aussi convergente.
      \end{noliste}
      \conc{L'intégrale $\dint{0}{1} G_{a,b}(x) \dx$ est convergente.}~\\[-1cm]
    \end{proof}

  \item Soit $f$ la fonction définie sur $\R$ par : $f(x) =
    \sqrt{\dfrac{b}{2\pi}}\times \exp \left(-\dfrac{1}{2} b
      \left(x+\dfrac{a}{b}\right)^2 \right)$.\\
    Montrer que $f$ est une densité d'une variable aléatoire suivant
    une loi normale dont on précisera les paramètres (espérance et
    variance).

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item On rappelle qu'une \var $X$ suit la loi normale de
        paramètres $(m, \sigma^2)$ si :
        \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
        \item $X(\Omega) = \R$,
        \item $X$ admet pour densité la fonction $x \mapsto
          \dfrac{1}{\sigma \ \sqrt{2 \pi}} \ \ee^{-\frac{1}{2}
            (\frac{x-m}{\sigma})^2}$ (définie sur $\R$).
        \end{noliste}

      \item Soit $x \in \R$.
        \[
        \begin{array}{rcl}
          f(x) & = & \sqrt{\dfrac{b}{2\pi}} \times \exp \left(-\dfrac{1}{2} b
            \left(x+\dfrac{a}{b}\right)^2 \right) \\[.6cm]
          & = & \dfrac{\sqrt{b}}{\sqrt{2\pi}} \times \exp
          \left(-\dfrac{1}{2} b \left(\dfrac{bx + a}{b}\right)^2
          \right) \\[.6cm]  
          & = & \dfrac{1}{\frac{1}{\sqrt{b}} \ \sqrt{2\pi}} \times \exp
          \left(-\dfrac{1}{2} \left(\sqrt{b} \ \dfrac{\bcancel{b}(x +
                \frac{a}{b})}{\bcancel{b}}\right)^2 \right) \\[.6cm]
          & = & \dfrac{1}{\frac{1}{\sqrt{b}} \ \sqrt{2\pi}}
          \times \exp \left(-\dfrac{1}{2} \left(\dfrac{x +
                \frac{a}{b}}{\frac{1}{\sqrt{b}}}\right)^2 \right)  
        \end{array}        
        \]
        \conc{On en déduit que $f$ est la densité d'une \var $X$ qui
          suit la loi $\Norm{- \frac{a}{b} }{ \frac{1}{b} }$.\\
          Ainsi, $\E(X) = - \dfrac{a}{b}$ et $\V(X) =
          \dfrac{1}{b}$.}~\\[-1.2cm]
      \end{noliste}
    \end{proof}


    %\newpage


  \item Soit $\Phi$ la fonction de répartition de la loi normale
    centrée réduite. \\
    Déduire de la question \itbf{2.b)}, l'égalité :
    \[
    \dint{0}{+\infty} G_{a,b}(x)\dx = \sqrt{\frac{2\pi}{b}}\times
    \exp\left(\frac{a^2}{2b}\right) \times
    \Phi\left(-\frac{a}{\sqrt{b}}\right)
    \]

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Soit $x \in [0, +\infty[$. Remarquons tout d'abord :
        \[
        \begin{array}{rcl}
          -a x - \dfrac{b}{2} \ x^2 & = & - \dfrac{1}{2} \ b \ \left(
            2 \ \dfrac{a}{b} \ x + x^2 \right) \\[.4cm]
          & = & - \dfrac{1}{2} \ b \ \left( \left( x + \dfrac{a}{b}
            \right)^2 - \dfrac{a^2}{b^2} \right) % \\[.4cm]
          \ = \ - \dfrac{1}{2} \ b \ \left( x + \dfrac{a}{b}
          \right)^2 \ + \ \dfrac{1}{2} \dfrac{a^2}{b}
        \end{array}
        \]

      \item On en déduit : 
        \[
        \begin{array}{rcl}
          G_{a, b}(x) & = & \exp\left( -ax - \dfrac{b}{2} x^2 \right)
          \\[.4cm]
          & = & \exp\left( - \dfrac{1}{2} \ b \ \left( x + \dfrac{a}{b}
            \right)^2 \ + \ \dfrac{1}{2} \dfrac{a^2}{b} \right)
          \\[.4cm]
          & = & \exp\left( \dfrac{a^2}{2b} \right) \times \exp\left( -
            \dfrac{1}{2} \ b \ \left( x + \dfrac{a}{b} \right)^2 \right)
          \\[.4cm]
          & = & \sqrt{\dfrac{2 \pi}{b}} \times \exp\left(
            \dfrac{a^2}{2b} \right) \times \sqrt{\dfrac{b}{2 \pi}}
          \times \exp\left( - \dfrac{1}{2} \ b \ \left( x +
              \dfrac{a}{b} \right)^2 \right) 
          \\[.4cm]
          & = & \sqrt{\dfrac{2 \pi}{b}} \times \exp\left(
            \dfrac{a^2}{2b} \right) \times f(x)
        \end{array}
        \]

      \item Et ainsi : 
        \[
        \dint{0}{+\infty} G_{a,b}(x) \dx = \sqrt{\dfrac{2 \pi}{b}}
        \times \exp\left( \dfrac{a^2}{2b} \right) \times
        \dint{0}{+\infty} f(x) \dx
        \]

      \item On rappelle : $X \suit \Norm{-\frac{a}{b}}{\frac{1}{b}} \
        \Leftrightarrow \ X^* = \dfrac{X -
          (-\frac{a}{b})}{\frac{1}{\sqrt{b}}} \suit
        \Norm{0}{1}$.\\[.2cm]
        On effectue alors le changement de variable $u =
        \dfrac{x+\frac{a}{b}}{\frac{1}{\sqrt{b}}}$, autrement dit
        $\Boxed{u = \sqrt{b} \ \left(x + \dfrac{a}{b} \right)}$ :
        \[
        \left|
          \begin{array}{P{11cm}}
            $u = \sqrt{b} \ \left(x + \dfrac{a}{b} \right)$ \quad (et donc
            $x =
            \dfrac{1}{\sqrt{b}} \ u - \dfrac{a}{b}$) \nl    
            $\hookrightarrow$ $du = \sqrt{b} \, \dx$ \quad et \quad $dx
            = \dfrac{1}{\sqrt{b}} \ du$ \nl
            \vspace{-.4cm}
            \begin{noliste}{$\sbullet$}
            \item $x = 0 \ \Rightarrow \ u = \dfrac{a}{\sqrt{b}}$
            \item $x = +\infty \ \Rightarrow \ u = +\infty$ %
              \vspace{-.4cm}
            \end{noliste}
          \end{array}
        \right.
        \]
        Ce changement de variable est valide car la fonction $\varphi
        : u \mapsto \dfrac{1}{\sqrt{b}} u - \dfrac{a}{b}$ est
        $\Cont{1}$ sur $[\frac{a}{\sqrt{b}}, +\infty[$.


        %\newpage


        \noindent
        Et donc : 
        \[
        \begin{array}{rcl}
          \dint{0}{+\infty} f(x) \dx & = & \sqrt{\dfrac{b}{2 \pi}} \ 
          \dint{0}{+\infty} \exp\left( -\dfrac{1}{2} \ b \ \left( x +
              \dfrac{a}{b} \right)^2 \right) \dx \\[.6cm]
          & = & \sqrt{\dfrac{b}{2 \pi}} \
          \dint{\frac{a}{\sqrt{b}}}{+\infty} \exp\left( - \dfrac{1}{2}
            \ u^2 \right) \ \dfrac{1}{\sqrt{b}} \ du \ = \
          \dint{\frac{a}{\sqrt{b}}}{+\infty} \dfrac{1}{\sqrt{2 \pi}} \
          \ee^{-\frac{u^2}{2}} \ du
          \\[.6cm]
          & = & \dint{\frac{a}{\sqrt{b}}}{+\infty} \varphi(u) \ du \ =
          \ 1 - \dint{+\infty}{\frac{a}{\sqrt{b}}} \varphi(u) \ du \ =
          \ 1 - \Phi\left( \dfrac{a}{\sqrt{b}} \right) \ = \
          \Phi\left( -\dfrac{a}{\sqrt{b}} \right)
        \end{array}      
        \]
      \end{noliste}
      \conc{$\dint{0}{+\infty} G_{a,b}(x)\dx =
        \sqrt{\frac{2\pi}{b}}\times \exp\left(\frac{a^2}{2b}\right)
        \times \Phi\left(-\frac{a}{\sqrt{b}}\right)$}
      ~\\[-1.4cm]
    \end{proof}
  \end{noliste}

\item Pour tout $a>0$ et pour tout $b>0$, on pose : $f_{a,b}(x) =
  \left\{
    \begin{array}{cR{1.4cm}}
      (a+bx) \ \exp\left(-ax-\dfrac{b}{2}x^2\right) & si $x\geq 0$ \nl
      0 & si $x<0$
    \end{array}
  \right.$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier que la fonction $f_{a,b}$ est une densité de
    probabilité.\\
    {\it On dit qu'une variable aléatoire suit la loi exponentielle
      linéaire de paramètres $a$ et $b$, notée
      $\mathcal{E}_\ell(a,b)$, si elle admet $f_{a,b}$ pour densité.}

    \begin{proof}~\\
      On vérifie les trois propriétés des densités de probabilité.
      \begin{nonoliste}{(i)}
      \item La fonction $f_{a,b}$ est :
        \begin{noliste}{$\stimes$}
        \item continue sur $]-\infty, 0[$ car constante sur cet intervalle,
        \item continue sur $]0, +\infty[$ comme composée et produit de
          fonctions continues sur $]0,+\infty[$.
        \end{noliste}

      \item D'autre part, pour tout $x \in \R$, $f_{a, b}(x) \geq 0$
        car :
        \begin{noliste}{$\stimes$}
        \item \dashuline{si $x \geq 0$} : $f_{a, b}(x) =
          (a+bx)\exp\left(-ax-\dfrac{b}{2}x^2\right) > 0$ (car $a>0$ et 
	  $b>0$),
        \item \dashuline{si $x < 0$} : $f_{a, b}(x) = 0 \geq 0$.
        \end{noliste}
        
      \item $\dint{-\infty}{+\infty} f_{a, b}(x) \dx =
        \dint{0}{+\infty} f_{a, b}(x) \dx$ car $f_{a, b}$ est nulle en
        dehors de $[0, +\infty[$.\\
        La fonction $f_{a, b}$ est continue par morceaux sur $[0, +\infty[$.\\
        Soit $A \in [0, +\infty[$.
        \[
        \begin{array}{rcl}
          \dint{0}{A} f_{a, b}(x) \dx & = & \dint{0}{A} (a+bx) \
          \exp\left(-ax-\dfrac{b}{2}x^2\right) \dx \\[.6cm]
          & = & - \Prim{ \exp\left(-ax-\dfrac{b}{2}x^2\right) }{0}{A}
          \\[.6cm]
          & = & - \left( \exp\left(-a A - \dfrac{b}{2} A^2\right) -
            \exp(0) \right) \\[.6cm]
          & = & 1 - \ee^{-a A} \times \ee^{-\frac{b}{2} A^2} \
          \tendd{A}{+\infty} 1
        \end{array}
        \]


        %\newpage


        \noindent
        Ainsi, l'intégrale impropre $\dint{-\infty}{+\infty} f_{a,
          b}(x) \dx$ est convergente et vaut $1$.
      \end{nonoliste}
      \conc{On en conclut que $f_{a, b}$ est une densité de
        probabilité.}~\\[-1cm] 
    \end{proof}

  \item Soit $X$ une variable aléatoire suivant la loi
    $\mathcal{E}_\ell(a,b)$. À l'aide d'une intégration par parties,
    justifier que $X$ admet une espérance $\E(X)$ telle que : $\E(X) =
    \dint{0}{+\infty} G_{a,b}(x)\dx$.

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item La \var $X$ admet une espérance si et seulement si
        l'intégrale $\dint{-\infty}{+\infty} x \ f_{a, b}(x) \dx$
        converge absolument, ce qui équivaut à démontrer la
        convergence pour des calculs de moment du type $\dint{-\infty}
        {+\infty} x^n f_{a,b}(x) \dx$.

      \item La fonction $f_{a, b}$ est nulle en dehors de $[0,
        +\infty[$. Donc :
        \[
        \dint{-\infty}{+\infty} x f_{a, b}(x) \dx = \dint{0}{+\infty}
        x f_{a, b}(x) \dx
        \]

      \item Soit $A \in [0, +\infty[$.\\
        La fonction $x \mapsto x \ f_{a, b}(x)$ est continue par
        morceaux sur $[0, A]$.\\
        On procède par intégration par parties (IPP).
        \[
        \renewcommand{\arraystretch}{2}
        \begin{array}{|rcl@{\qquad}rcl}
          u(x) & = & x & u'(x) & = & 1 \\
          v'(x) & = & f_{a, b}(x) & v(x) & = & - G_{a, b}(x)
        \end{array}
        \]
        Cette IPP est valide car les fonctions $u$ et $v$ sont 
	de classe $\Cont{1}$
        sur $[0, A]$. On obtient :
        \[
        \begin{array}{rcl}
          \dint{0}{A} x \ f_{a, b}(x) \dx & = & \Prim{-x \ G_{a,
              b}(x)}{0}{A} + \dint{0}{A} G_{a, b}(x) \dx \\[.6cm]
          & = & - (A \ G_{a, b}(A) - 0) + \dint{0}{A} G_{a, b}(x) \dx 
%           \\[.6cm]
%           & \tendd{A}{+\infty} & 0 \ + \ \dint{0}{+\infty} G_{a, b}(x) 
% 	    \dx
        \end{array}
        \]
        De plus, par croissances comparées :
        \[
        A \ G_{a,b}(A) = A \ \exp\left(-a A - \dfrac{b}{2} \
          A^2\right) = \dfrac{A}{(\ee^a)^A} \times
        \dfrac{1}{\ee^{\frac{b}{2} A^2}} \tendd{A}{+\infty} 0 \times 0
        = 0
        \]
        Et, comme l'intégrale $\dint{0}{+\infty} G_{a, b}(x) \dx$ est
        convergente d'après la question \itbf{2.a)} : 
        \[
        \dint{0}{A} x \ f_{a, b}(x) \dx \tendd{A}{+\infty} 0+
        \dint{0}{+\infty} G_{a, b}(x) \dx
        \]
      \end{noliste}
      \conc{On en déduit que $X$ admet une espérance et que $\E(X) =
        \dint{0}{+\infty} G_{a, b}(x) \dx$.}~\\[-1.2cm]
    \end{proof}
  \end{noliste}


  %\newpage


\item Soit $Y$ une variable aléatoire suivant la loi exponentielle de
  paramètre $1$. On pose : $X = \dfrac{-a+\sqrt{a^2+2bY}}{b}$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier que pour tout réel $x\in\R_+$, on a : $\Prob(\Ev{X\geq
      x}) = G_{a,b}(x)$.

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item D'après la question \itbf{1.c)} :
        \[
        \forall u \in [0,1[, \ G^{-1}_{a,b}(1-u) = \dfrac{-a +
          \sqrt{a^2 - 2b \ \ln(1-u)}}{b}
        \]

%       \item Considérons $u\in [0,1[$ et $y \geq 0$. et notons $u = 1 - 
% \ee^{-y}$. Comme
%         $y \geq 0$, $\ee^{-y} \in \ ]0, 1]$ et donc $u \in [0, 1[$.\\
%         On peut donc appliquer la formule précédente.\\
        On remarque :
        \[
         -\ln(1-u) = y \ \Leftrightarrow \ \ln(1-u) = -y \ 
	 \Leftrightarrow \ 1 - u = \ee^{-y} \ \Leftrightarrow \
	 u = 1 - \ee^{-y}
        \]
        Si $y \geq 0$, $\ee^{-y} \in \ ]0, 1]$ et donc $u \in [0, 1[$.\\
        On peut donc appliquer la formule précédente et
        on obtient :
        \[
        G^{-1}_{a,b}(\ee^{-y}) = \dfrac{-a + \sqrt{a^2 + 2b \ y}}{b}
        \]

      \item Comme $Y \suit \Exp{1}$, $Y(\Omega) = [0,
        +\infty[$. Ainsi, d'après ce qui précède : $X =
        G^{-1}_{a,b}(\ee^{-Y})$.\\[.2cm]
        On a alors, pour tout $x \in [0, +\infty[$ :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          \Prob(\Ev{X \geq x}) & = &
          \Prob\left(\Ev{G^{-1}_{a,b}(\ee^{-Y}) \ \geq \ x} \right) \\[.2cm]
          & = & \Prob\left(\Ev{\ee^{-Y} \ \leq \ G_{a,b}(x)}\right) & (par
          stricte décroissance \\ de $G_{a, b}$ sur $[0, +\infty[$)
          \nl
          \nl[-.2cm]
          & = & \Prob\left(\Ev{-Y \ \leq \ \ln(G_{a,b}(x))}\right) & (par
          stricte croissance \\ de $\ln$ sur $]0, 1]$)
          \nl
          \nl[-.2cm]
          & = & \Prob\left(\Ev{Y \ \geq \ -\ln(G_{a,b}(x))}\right) 
          \\[.4cm]
          & = & 1 - \Prob\left(\Ev{Y \ < \ -\ln(G_{a,b}(x))}\right)
          \\[.2cm]          
          & = & 1 - F_{y}(-\ln(G_{a,b}(x))) & (car $Y$ est une \\ \var à
          densité) \nl
          \nl[-.2cm]
          & = & \bcancel{1} - \left(\bcancel{1} -
            \ee^{-(-\ln(G_{a,b}(x)))} \right) & (car $Y \suit
          \Exp{1}$ \\ et $-\ln(G_{a,b}(x)) \geq 0$) \nl 
          \nl[-.2cm]
          & = & \ee^{\ln(G_{a,b}(x))} \ = \ G_{a,b}(x)
        \end{array}
        \]
      \end{noliste}
      \conc{$\forall x \in [0, +\infty[$, $\Prob(\Ev{ X \geq x }) =
        G_{a, b}(x)$}


      %\newpage


      ~\\[-1.4cm]
    \end{proof}


    %\newpage


  \item En déduire que $X$ suit la loi $\mathcal{E}_\ell(a,b)$.

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Notons $g : x \mapsto \dfrac{-a + \sqrt{a^2 + 2b \
            x}}{b}$. Tout d'abord :
        \[
        \begin{array}{rcl@{\quad}>{\it}R{5cm}}
          X(\Omega) & = & \big( g(Y) \big)
          \hspace{.1cm} (\Omega)
          \\[.2cm]
          & = & g \hspace{.1cm} \big(Y(\Omega) \big)
          \\[.2cm]
          & = & g \hspace{.1cm} \big( [0, +\infty[ \big) 
          % \\[.2cm]
          % & \subset & ]0,1[ & (d'après les implications précédentes)
        \end{array}
        \]
        Or, comme $g$ est continue et strictement croissante sur $[0,
        +\infty[$ :
        \[
        g \hspace{.1cm} \big( [0, +\infty[ \big) = \ [g(0), \dlim{x
          \tend +\infty} g(x)[ \ = \ [0, +\infty[
        \]
        \conc{Ainsi : $X(\Omega) = [0, +\infty[$.}
        
      \item Déterminons alors la fonction de répartition de $X$.\\
        Soit $x \in \R$. Deux cas se présentent.
        \begin{noliste}{$\stimes$}
        \item \dashuline{Si $x < 0$} alors $\Ev{X \leq x} = \emptyset$
          car $X(\Omega) = [0, +\infty[$. Ainsi :
          \[
          F_X(x) = \Prob(\Ev{X \leq x}) = 0
          \]

        \item \dashuline{Si $x \geq 0$} alors :
          \[
          \begin{array}{rcl@{\qquad}>{\it}R{7.4cm}}
            F_X(x) & = & \Prob(\Ev{X \leq x}) \\[.2cm]
            & = & 1 - \Prob(\Ev{X > x}) \\[.2cm]
            & = & 1 - G_{a, b}(x) & (en reprenant la démonstration 
            précédente \\ en remplaçant $\Ev{X \geq x}$ par $\Ev{X > x}$)
          \end{array}
          \]          
        \end{noliste}
        En résumé :
        \[
          F_X \ : \ x  \mapsto 
          \left\{
          \begin{array}{cR{3cm}}
            0 & si $x < 0$ \nl
            1 - G_{a, b}(x) & si $x \geq 0$
          \end{array}
          \right.
        \]

      \item La fonction $F_X$ est :
      \end{noliste}
      \begin{liste}{1)}
      \item continue sur $\R$ puisque :
        \begin{noliste}{$\stimes$}
        \item $x \mapsto 1 - G_{a, b}(x)$ est continue sur $]0,
          +\infty[$ car $G_{a, b}$ l'est.
        \item $x \mapsto 0$ est continue sur $]-\infty, 0[$.
        \item $\dlim{x \tend 0} \left(1 - G_{a,b}(x)\right) = 1 - 1 = 0 
	= F_X(0) = \dlim{x \tend 0} 0$.
        \end{noliste}

      \item de classe $\Cont{1}$ sur $]-\infty, 0[$ et sur $]0, 
      +\infty[$ car :
        \begin{noliste}{$\stimes$}
        \item $x \mapsto 0$ est de classe $\Cont{1}$ sur $]-\infty, 0[$,
        \item $x \mapsto 1 - G_{a, b}(x)$ est de classe $\Cont{1}$ sur 
	$]0, +\infty[$ car $G_{a, b}$ l'est.
        \end{noliste}        
      \end{liste}
      \conc{On en déduit que $X$ est une \var à densité.}


      %\newpage


      \begin{noliste}{$\sbullet$}
      \item On obtient une densité $f_X$ de $X$ en dérivant sur les
        intervalles ouverts.\\
        On {\bf pose} de plus : $f_X(0) = (a + b \times 0) \ \exp(0) =
        a$.
        \[
	  f_X \ : \ x \mapsto
          \left\{
          \begin{array}{cR{3cm}}
            0 & si $x < 0$ \nl
            (a + bx) \ \exp\left( -ax - \dfrac{b}{2} x^2 \right) & si 
	    $x \geq 0$
          \end{array}
          \right.
        \]
      \end{noliste}
      \conc{Ainsi, $f_X$ coïncide avec $f_{a, b}$. On en déduit : $X
        \suit \mathcal{E}_\ell(a,b)$.}
      %~\\[-1.2cm]
    \end{proof}

  \item On note $U$ une variable aléatoire suivant la loi uniforme sur
    $[0,1[$.\\
    Déterminer la loi de la variable aléatoire $G_{a,b}^{-1}(1-U)$.

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Si $U \suit \Ucfo{0}{1}$ et $\lambda > 0$, alors $-
        \dfrac{1}{\lambda} \ln(1-U) \suit \Exp{\lambda}$.\\[.2cm]
        On note alors : $Y = - \ln(1-U)$. On obtient ainsi $Y \suit
        \Exp{1}$.

      \item Or, comme vu en question \itbf{4.a)} et \itbf{4.b)} :
        \[
        G^{-1}_{a, b}(\ee^{-Y}) \suit \mathcal{E}_\ell(a, b)
        \]

      \item On remarque enfin :
        \[
        \ee^{-Y} = \ee^{-(- \ln(1-U))} = \ee^{\ln(1-U)} = 1 - U
        \]       
      \end{noliste}
      \conc{On en déduit que $G^{-1}_{a, b}(1-U) \suit
        \mathcal{E}_\ell(a, b)$.}
      
      
      %\newpage
      
      
      ~\\[-1.4cm]
    \end{proof}
  \end{noliste}


  %\newpage


\item La fonction \Scilab{} suivante génère des simulations de la loi
  exponentielle linéaire.
  \begin{scilab}
    & \tcFun{function} \tcVar{x} =
    grandlinexp(\tcVar{a},\tcVar{b},\tcVar{n}) \nl %
    & \qquad u = rand(\tcVar{n},1) \nl %
    & \qquad y = ............ \nl %
    & \qquad \tcVar{x} = (-\tcVar{a} + sqrt(\tcVar{a}\puis 2 + 2
    \Sfois{} \tcVar{b} \Sfois{} y)) / \tcVar{b} \nl %
    & \tcFun{endfunction} \nl %
  \end{scilab}

  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Quelle est la signification de la ligne de code \ligne{2} ?

    \begin{proof}~\\[-1cm]%
      \concL{L'instruction {\tt rand(n,1)} renvoie un vecteur colonne
        de taille $\mathtt{n} \times 1$ contenant le résultat de la
        simulation de {\tt n} \var aléatoires indépendantes qui
        suivent toutes la même loi $\Uc{0}{1}$.}{15.6}~\\[-.8cm]
    \end{proof}

  \item Compléter la ligne de code \ligne{3} pour que la fonction {\tt
      grandlinexp} génère les simulations désirées.

    \begin{proof}~\\
      D'après la question \itbf{4.c)}, il suffit d'écrire :
      \begin{scilabC}{2}
        & \qquad y = - log(1 - u)
      \end{scilabC}~\\[-1cm]
    \end{proof}

  \end{noliste}

\item De quel nombre réel peut-on penser que les six valeurs générées
  par la boucle \Scilab{} suivante fourniront des valeurs approchées
  de plus en plus précises et pourquoi ?
  \begin{scilab}
    & \tcFor{for} k = 1:6 \nl %
    & \qquad mean(grandlinexp(0, 1, 10\puis{}k) \nl %
    & \tcFor{end} \nl %
  \end{scilab}

  \begin{proof}~
    \begin{noliste}{$\sbullet$}
    \item Soit $X$ une \var telle que $X \suit \mathcal{E}_\ell(0,
      1)$.\\
      Pour $m \in \N^*$, on considère $(X_1, \ldots, X_m)$ un
      $m$-échantillon de la \var $X$.\\
      Autrement dit, on considère $(X_i)_{i \in \N^*}$ une suite de \var
      indépendantes et toutes de même loi $\mathcal{E}_\ell(0, 1)$. On
      note alors :
      \[
      \overline{X_{m}} \ = \ \dfrac{X_1 + \ldots + X_{m}}{m}
      \]
      la \var donnant la moyenne empirique associée à \var $X$.

    \item Pour chaque {\tt k}, l'instruction {\tt mean(grandlinexp(0,
        1, 10\puis{}k))} permet de simuler la \var
      $\overline{X_{10^{\mathtt{k}}}}$.

    \item En vertu de la loi faible des grands nombres, la \var
      $\overline{X_{10^{\mathtt{k}}}}$ converge en probabilité vers la
      variable aléatoire constante égale à $\E(X)$.

    \item L'entier {\tt k} prenant des valeurs de plus en plus grandes 
      (on
      considère une simulation de $\overline{X_{10}}$, puis
      $\overline{X_{100}}$, \ldots, puis $\overline{X_{1000000}}$), on
      peut penser que le résultat sera de plus en plus proche de
      $\E(X)$.
    \end{noliste}
    \concL{Les six valeurs générées par la boucle \Scilab{} fourniront
      des valeurs de plus en plus précises de $\E(X)$.}{15}~\\[-1cm]
  \end{proof}
\end{noliste}


%\newpage


\noindent%
{\it Dans la suite du problème, on note $(X_n)_{n\in\N^*}$ une suite
  de variables aléatoires indépendantes suivant chacune la loi
  exponentielle linéaire $\mathcal{E}_\ell(a,b)$ dont les paramètres
  $a>0$ et $b>0$ sont inconnus.\\
  Soit $h$ un entier supérieur ou égal à $2$. On suit pendant une
  période de $h$ années, une \og cohorte \fg{} de $n$ individus de
  même âge au début de l'étude et on modélise leurs durées de vie
  respectives à partir de cette date par les variables aléatoires
  $X_1$, $X_2$, $\hdots$, $X_n$.}


\subsection*{Partie II. Premier décès et intervalle de confiance de $a$}

\noindent
Pour tout $n\in\N^*$, on définit les variables aléatoires $M_n$, $H_n$
et $U_n$ par :
\[
M_n = \min(X_1,X_2,\hdots,X_n), \quad H_n=\min(h,X_1,X_2,\hdots,X_n)
\quad \mbox{et} \quad U_n=nH_n.
\]
\begin{noliste}{1.}
  \setcounter{enumi}{6}
\item Calculer pour tout $x \in \R_+$, la probabilité 
$\Prob(\Ev{M_n\geq x})$.\\
  Reconnaître la loi de la variable aléatoire $M_n$.

  \begin{proof}~\\
    Soit $n \in \N^*$.
    \begin{noliste}{$\sbullet$}
    \item Tout d'abord : $M_n(\Omega) \subset [0, +\infty[$. En effet :
    $\forall i \in \llb 1,n \rrb$, $X_i(\Omega) = [0,+\infty[$.

    \item Soit $x \in \R$. Deux cas se présentent :
      \begin{noliste}{$\stimes$}
      \item \dashuline{si $x < 0$} alors $\Ev{M_n \geq x} =
        \Omega$ car $M_n(\Omega) \subset [0,+\infty[$.\\[.1cm]
        Ainsi : $\Prob(\Ev{M_n \geq x}) = \Prob(\Omega) = 1$.

      \item \dashuline{si $x \geq 0$} alors :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          \Prob(\Ev{M_n \geq x}) & = & \Prob(\Ev{\min(X_1, \ldots,
            X_n) \geq x}) \\[.2cm]
          & = & \Prob(\Ev{X_1 \geq x} \ \cap \ \ldots \ \cap \ \Ev{X_n
            \geq x} ) \\[.2cm]
          & = & \Prob(\Ev{X_1 \geq x}) \ \times \ \ldots \ \times \
          \Prob(\Ev{X_n \geq x} ) & (car les \var $X_i$ \\ sont
          indépendantes) \nl
          \nl[-.2cm]
          & = & G_{a, b}(x) \ \times \ \ldots \ \times \ G_{a, b}(x) &
          (d'après la question \itbf{4.a)}) \nl
          \nl[-.2cm]
          & = & (G_{a, b}(x))^n
        \end{array}
        \]        
      \end{noliste}
      Enfin, comme $\Prob(\Ev{M_n \leq x}) = 1 - \Prob(\Ev{M_n > x})$
      (on peut remplacer, sans modification du résultat, $\Ev{M_n \geq
        x}$ par $\Ev{M_n > x}$ dans la démonstration ci-dessus), on
      obtient :
      \[
        F_{M_n} \ : \ x \mapsto
        \left\{
          \begin{array}{cR{3cm}}
            0 & si $x < 0$ \nl
            1 - (G_{a, b}(x))^n & si $x \geq 0$
          \end{array}
        \right.
      \]

    \item La fonction $F_{M_n}$ (\cf \itbf{4.b)}) est :
      \begin{noliste}{1)}
      \item continue sur $\R$,
      \item de classe $\Cont{1}$ sur $]-\infty, 0[$ et sur $]0, 
      +\infty[$.
      \end{noliste}
      On en déduit que $M_n$ est une \var à densité.

    \item On obtient une densité $f_{M_n}$ de $M_n$ en dérivant 
    $F_{M_n}$ sur
      les intervalles ouverts.\\
      On choisit de plus : $f_{M_n}(0) = -n \ (G_{a, b}(0))^{n-1} \ 
      G'_{a, b}(0)$. 
      On obtient :\\
      \[
	f_{M_n} \ : \ x \mapsto
        \left\{
          \begin{array}{cR{3cm}}
            0 & si $x < 0$ \nl
            - n \ (G_{a, b}(x))^{n-1} \ G'_{a, b}(x) & si $x \geq 0$
          \end{array}
        \right.
      \]


      %\newpage


      \noindent
      Or : 
      \[
      \begin{array}{rcl}
        - n \ (G_{a, b}(x))^{n-1} \ G'_{a, b}(x) & = & - n \ \left(\exp\left(-ax
            - \frac{b}{2} \ x^2\right)\right)^{n-1} \ (-a - bx) \
        \exp\left(-ax - \frac{b}{2} \ x^2\right)  \\[.2cm]
        & = & n \ (a + bx) \ \left(\exp\left(-ax - \frac{b}{2} \
            x^2 \right) \right)^{n} \\[.2cm]
        & = & ((na) + (nb) x) \ \exp\left(-(na) x - \frac{(nb)}{2} \
            x^2 \right) \ = \ f_{na, nb}(x)
      \end{array}
      \]
    \end{noliste}
    \conc{On en déduit que $M_n \suit \mathcal{E}_{\ell}(na, nb)$.}~\\[-1cm]
  \end{proof}

\item Pour tout $n\in\N^*$, on note $F_{U_n}$ la fonction de
  répartition de la variable aléatoire $U_n$.
  
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Montrer que pour tout $x\in\R$, on a : $F_{U_n}(x) = %
    \left\{
      \begin{array}{lR{2.3cm}}
	0 & si $x < 0$ \nl
	1 - \exp\left(- ax - \dfrac{b}{2n}x^2\right) & si $0\leq x < nh$ \nl
	1 & si $x \geq nh$
      \end{array}
    \right.$.

    \begin{proof}~\\
      Soit $n \in \N^*$.
      \begin{noliste}{$\sbullet$}
      \item Commençons par déterminer $U_n(\Omega)$.\\
        Remarquons tout d'abord :
        \[
        \begin{array}{rcl}
          H_n & = & \min(h, X_1, X_2, \ldots, X_n) \\[.2cm]
          & = & \min(h, \min(X_1, X_2, \ldots, X_n)) \\[.2cm]
          & = & \min(h, M_n) \\[.2cm]
        \end{array}
        \]
        On en déduit : $H_n(\Omega) \subset [0, +\infty[$.%
        \conc{$U_n(\Omega) \subset [0, +\infty[$}

      \item Soit $x \in \R$. Trois cas se présentent :
        \begin{noliste}{$\stimes$}
        \item \dashuline{si $x < 0$} alors $\Ev{U_n > x} =
          \Omega$ car $U_n(\Omega) \subset [0,+\infty[$.\\[.1cm]
          Ainsi, $\Prob(\Ev{U_n > x}) = \Prob(\Omega) = 1$.
          
        \item \dashuline{si $x \in [0, nh[$} alors :
          \[
          \begin{array}{rcl@{\qquad}>{\it}R{5.4cm}}
            \Prob(\Ev{U_n > x}) & = & \Prob(\Ev{n H_n > x}) \\[.2cm]
            & = & \Prob(\Ev{H_n > \frac{x}{n}}) & (puisque $n > 0$)
            \nl
            \nl[-.2cm]
            & = & \Prob(\Ev{\min(h, M_n) > \frac{x}{n}}) \\[.4cm]
            & = & \Prob(\Ev{h > \frac{x}{n}} \cap \Ev{M_n >
              \frac{x}{n}}) \\[.4cm] 
            & = & \Prob(\Ev{h > \frac{x}{n}}) \ \times \ 
            \Prob(\Ev{M_n > \frac{x}{n}}) & (car la \var constante
            $h$ et la \var $M_n$ sont indépendantes) \nl
            \nl[-.2cm]
            & = & 1 \ \times \ \Prob(\Ev{M_n > \frac{x}{n}}) & (car
            $\Ev{x < nh} = \Omega$ \\ puisque $x \in [0, nh[$) \nl
            \nl[-.2cm]
            & = & \left(G_{a, b}(\frac{x}{n}) \right)^n & (d'après la
            question \itbf{7}) 
          \end{array}
          \]        

        \item \dashuline{si $x \geq nh$} alors (en reprenant la
          démonstration ci-dessus) :
          \[
          \begin{array}{rcl@{\qquad}>{\it}R{5.4cm}}
            \Prob(\Ev{U_n > x}) & = & \Prob(\Ev{h > \frac{x}{n}}) \
            \times \ \Prob(\Ev{M_n > \frac{x}{n}}) \\[.2cm]
            & = & 0 \ \times \ \Prob(\Ev{M_n > \frac{x}{n}}) \ = \ 0 & (car
            $\Ev{x < nh} = \emptyset$ \\ puisque $x \geq nh$) \nl
          \end{array}
          \]        
        \end{noliste}
        
        
        %\newpage


      \item Enfin, comme $\Prob(\Ev{U_n \leq x}) = 1 - \Prob(\Ev{U_n >
          x})$, on obtient : %
        \conc{$
          F_{U_n} \ : \ x \mapsto
          \left\{
            \begin{array}{cR{3cm}}
              0 & si $x < 0$ \nl
              \nl[-.2cm]
              1 - \left(G_{a, b}\left( \frac{x}{n} \right) \right)^n &
              si $x \in [0, nh[$ \nl 
              \nl[-.2cm]
              1 & si $x \geq nh$ 
            \end{array}
          \right.
        $}%
      Il suffit alors de remarquer : 
      \[
      \left(G_{a, b}\left(\dfrac{x}{n} \right)\right)^n = \exp\left(
        -a \dfrac{x}{n} - \dfrac{b}{2} \left(\dfrac{x}{n}\right)^2
      \right)^n = \exp\left( -a\bcancel{n} \dfrac{x}{\bcancel{n}} -
        \dfrac{b}{2} \bcancel{n} \ \dfrac{x^2}{n^{\bcancel{2}}}\right)
      = \exp\left( -a x - \dfrac{b}{2n} x^2 \right)
      \]
      \end{noliste}~\\[-1cm]
    \end{proof}

  \item Étudier la continuité de la fonction $F_{U_n}$.

    \begin{proof}~\\
      La fonction $F_{U_n}$ est :
      \begin{noliste}{$\stimes$}
      \item continue sur $]-\infty, 0[$ et sur $]nh, +\infty[$ car
        constante sur chacun de ces intervalles.

      \item continue sur $]0, nh[$ car $G_{a, b}$ l'est sur $[0,
        +\infty[$.

      \item continue en $0$ puisque : 
        \begin{noliste}{1)}
        \item $\dlim{x \tend 0^-} F_{U_n}(x) = \dlim{x \tend 0} 0 = 0$,
        \item $\dlim{x \tend 0^+} F_{U_n}(x) = \dlim{x \tend 0} 1 -
          \left(G_{a, b}\left( \frac{x}{n} \right) \right)^n = 1 -
          \left(G_{a, b}( 0 ) \right)^n = 1 - 1^n = 0$,
        \item $F_{U_n}(0) = 0$.
        \end{noliste}

      \item non continue en $nh$. En effet :
        \[
        \dlim{x \tend (nh)^-} F_{U_n}(x) = \dlim{x \tend nh} 1 -
        \left(G_{a, b}\left( \frac{x}{n} \right) \right)^n = 1 -
        \left(G_{a, b}\left( \frac{nh}{n} \right) \right)^n = 1 -
        \exp\left(-a nh - \dfrac{b}{2\bcancel{n}} n^{\bcancel{2}} \
          h^2 \right) < 1
        \]
        et $F_{U_n}(nh) = 1$ 
      \end{noliste}
      \conc{Ainsi, $F_{U_n}$ est continue uniquement sur $]-\infty,
        nh[$ et sur $]nh, +\infty[$.}~\\[-1.2cm]
    \end{proof}

  \item La variable aléatoire $U_n$ admet-elle une densité ?

    \begin{proof}~%
      \concL{D'après la question précédente, $F_{U_n}$ n'est pas
        continue sur $\R$. Ainsi, $U_n$ n'est pas une variable à
        densité.}{15}~\\[-1cm]
    \end{proof}

  \item Montrer que la suite de variables aléatoires
    $(U_n)_{n\in\N^*}$ converge en loi vers une variable aléatoire
    dont on précisera la loi.

    \begin{proof}~\\
      Deux cas se présentent.
      \begin{noliste}{$\sbullet$}
      \item \dashuline{Si $x < 0$} alors :
        \[
        \dlim{n \tend +\infty} F_{U_n}(x) = \dlim{n \tend +\infty} 0 = 0
        \]

      \item \dashuline{Si $x \geq 0$} alors, pour $n$ suffisamment
        grand (plus précisément pour tout $n > \left\lceil
          \dfrac{x}{h} \right\rceil$) :
        \[
        \dlim{n \tend +\infty} F_{U_n}(x) = \dlim{n \tend +\infty} 
        \left(1 -
        \exp\left( -ax - \dfrac{b}{2n} x^2 \right)\right)  = 1
        - \exp( -ax )
        \]
        
        
        %\newpage


      \item On reconnaît la fonction de répartition d'une variable
        aléatoire $Z$ telle que $Z \suit \Exp{a}$.
      \end{noliste}
      \conc{Ainsi, $U_n \tendL Z$ où $Z \suit \Exp{a}$.}~\\[-1cm]
    \end{proof}
  \end{noliste}

\item Soit $\alpha \in \ ]0,1[$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Soit $Y$ une variable aléatoire qui suit la loi exponentielle
    de paramètre $1$.\\
    Trouver deux réels $c$ et $d$ strictement positifs tels que :
    \[
    \Prob(\Ev{c\leq Y\leq d}) = 1-\alpha \quad \mbox{et} \quad 
    \Prob(\Ev{Y\leq
      c}) = \dfrac{\alpha}{2}
    \]
    
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Comme $Y \suit \Exp{1}$ et que les réels $c$ et $d$ doivent
      être strictement positifs :
        \[
        \begin{array}{rcl}
          \Prob(\Ev{c\leq Y\leq d}) & = & F_Y(d) - F_Y(c) \ = \ (\bcancel{1} -
          \ee^{-d}) - (\bcancel{1} - \ee^{-c}) \ = \ \ee^{-c} - \ee^{-d}
          \\[.6cm]
          \Prob(\Ev{Y \leq c}) & = & F_Y(c) \ = \ 1 - \ee^{-c}
        \end{array}
        \]
        
      \item On obtient ainsi :
        \[
        \begin{array}{rcl}
          \left\{
            \begin{array}{ccccc}
              \ee^{-c} & - & \ee^{-d} & = & 1 - \alpha \\[.4cm]
              -\ee^{-c} & & & = & -1 + \dfrac{\alpha}{2}
            \end{array}
          \right. %
          & \Leftrightarrow &
          \left\{
            \begin{array}{ccccc}
              & - & \ee^{-d} & = & - \dfrac{\alpha}{2} \\[.4cm]
              - \ee^{-c} & & & = & -1 + \dfrac{\alpha}{2}
            \end{array}
          \right. \\[1cm]
          & \Leftrightarrow &
          \left\{
            \begin{array}{l}
              \ee^{-d} = \dfrac{\alpha}{2} \\[.4cm]
              \ee^{-c} = 1 - \dfrac{\alpha}{2}
            \end{array}
          \right. \\[1cm]
          & \Leftrightarrow &
          \left\{
            \begin{array}{l}
              -d = \ln\left( \dfrac{\alpha}{2} \right) \\[.4cm]
              -c = \ln\left(1 - \dfrac{\alpha}{2} \right)
            \end{array}
          \right.
        \end{array}
        \]
        Ainsi, $c = -\ln\left(1 - \dfrac{\alpha}{2} \right) > 0$ car
        $1 - \dfrac{\alpha}{2} \in \ ]0,1[$ puisque $\alpha \in \
        ]0,1[$.\\[.2cm]
        De même, $d = - \ln\left( \dfrac{\alpha}{2} \right) > 0$ car
        $\dfrac{\alpha}{2} \in \ ]0,1[$.
      \end{noliste}
      \conc{$c = -\ln\left(1 - \dfrac{\alpha}{2} \right)$ \quad et
        \quad $d = - \ln\left( \dfrac{\alpha}{2} \right)$}
      ~\\[-1.4cm]
    \end{proof}


    %\newpage


  \item Montrer que $\left[ \dfrac{c}{U_n} \ , \ \dfrac{d}{U_n} \right]$
    est un intervalle de confiance asymptotique de $a$, de niveau de
    confiance $1-\alpha$.

    \begin{proof}~
      \[
      \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
        \Prob\left( a \in \left[ \dfrac{c}{U_n}, \dfrac{d}{U_n} \right]
        \right) & = & \Prob\left(\Ev{ \dfrac{c}{U_n} \leq a \leq \dfrac{d}{U_n}}
        \right) \\[.6cm]
        & = & \Prob\left( \Ev{c \leq a \ U_n \leq d} \right) & (car $U_n >
        0$) \nl
        \nl[-.2cm]
        & = & \Prob\left( \Ev{\dfrac{c}{a} \leq U_n \leq \dfrac{d}{a}}
        \right) & (car $a > 0$) \nl
        \nl[-.2cm]
        & \tendn & \Prob\left( \Ev{\dfrac{c}{a} \leq Z \leq \dfrac{d}{a}}
        \right) & (d'après la question \itbf{8.d)}) \nl
        \nl[-.2cm]         
      \end{array}
      \]
      On constate enfin :
      \[
      \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
        \Prob\left(\Ev{ \dfrac{c}{a} \leq Z \leq \dfrac{d}{a} } \right) & = &
        F_Z\left( \dfrac{d}{a} \right) - F_Z\left( \dfrac{c}{a}
        \right) \\[.4cm]
        & = & (\bcancel{1}- \ee^{-a \frac{d}{a}}) - (\bcancel{1}-
        \ee^{-a \frac{c}{a}}) & (car $Z \suit \Exp{a}$) \nl 
        \nl[-.2cm]
        & = & \ee^{-c} - \ee^{-d} \\[.2cm]
        & = & 1 - \alpha & (d'après la question précédente)
      \end{array}
      \]
      \concL{Ainsi, $\Prob\left( a \in \left[ \dfrac{c}{U_n},
            \dfrac{d}{U_n} \right] \right) \tendn 1 - \alpha$ ce qui
        démontre que $\left[ \dfrac{c}{U_n} \ , \ \dfrac{d}{U_n} 
	\right]$
        est un intervalle \\[.2cm] de confiance asymptotique de $a$, de niveau
        de confiance $1 - \alpha$.}{16}
      ~\\[-1.4cm]
    \end{proof}
  \end{noliste}
\end{noliste}
  

%\newpage


\subsection*{Partie III. Nombre de survivants et estimateur convergent de $b$}

\noindent
Pour tout $i \in \N^*$, soit $S_i$ et $D_i$ les variables aléatoires
telles que :
\[
S_i = \left\{
  \begin{array}{lR{1.6cm}}
    1 & si $X_i\geq h$ \nl
    0 & sinon
  \end{array}
\right. %
\qquad \mbox{ et } \qquad %
D_i = \left\{
  \begin{array}{lR{1.6cm}}
    1 & si $X_i\leq 1$ \nl
    0 & sinon
  \end{array}
\right.
\]
Pour tout $n\in\N^*$, on pose : $\overline{S}_n = \dfrac{1}{n} \
\Sum{i=1}{n} S_i$ et $\overline{D}_n = \dfrac{1}{n} \ \Sum{i=1}{n}
D_i$.
\begin{noliste}{1.}
  \setcounter{enumi}{9}
\item 
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier que pour tout $i\in\llb 1,n\rrb$, on a $\E(S_i) =
    G_{a,b}(h)$ et calculer $\E(S_iD_i)$.

    \begin{proof}~\\
      Soit $i \in \llb 1, n \rrb$.
      \begin{noliste}{$\sbullet$}
      \item Comme $S_i(\Omega) = \{0, 1\}$, la \var $S_i$ suit une loi
      de Bernoulli de paramètre : 
      \[
	p=\Prob(\Ev{X_i \geq h}) = G_{a,b}(h)
      \]
      \conc{Ainsi, la \var $S_i$ admet une espérance et : $\E(S_i) = 
      G_{a, b}(h)$.}
% 
%       \item Par définition, on a alors :
%         \[
%         \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
%           \E(S_i) & = & 1 \times \Prob(\Ev{X_i \geq h}) + 0 \times
%           \Prob(\Ev{X_i < h}) \\[.2cm]
%           & = & \Prob(\Ev{X_i \geq h}) \\[.2cm]
%           & = & G_{a, b}(h) & (d'après la question \itbf{4.a)})
%         \end{array}
%         \]
%         \conc{$\E(S_i) = G_{a, b}(h)$}~

      \item De même, $(S_i D_i)(\Omega) = \{0, 1\}$. Plus précisément :
        \[
        S_i D_i = \left\{
          \begin{array}{lR{3.6cm}}
            1 & si $X_i\geq h$ et $X_i \leq 1$ \nl
            0 & sinon
          \end{array}
        \right.
        \]
        Donc la \var $S_i \, D_i$ suit une loi de Bernoulli de 
        paramètre :
        \[
	  p= \Prob(\Ev{X_i \geq h} \cap \Ev{X_i \leq 1})
	  = \Prob(\Ev{h \leq X_i \leq 1}) = 0
	\]
        En effet, $\Ev{h \leq X_i \leq 1} = \emptyset$ puisque $h \geq
        2$.%
        \conc{On en déduit : $\E(S_i D_i) = 0$.}~\\[-1.4cm]
      \end{noliste}
    \end{proof}

  \item Pour quels couples $(i,j) \in \llb 1,n\rrb^2$, les variables
    aléatoires $S_i$ et $D_j$ sont-elles indépendantes ?

    \begin{proof}~\\
      Soit $i \in \llb 1,n\rrb$. On raisonne comme dans la question
      précédente.
      \begin{noliste}{$\sbullet$}
      \item Comme $D_i(\Omega) = \{0, 1\}$, , la \var $S_i$ suit une loi
      de Bernoulli de paramètre : 
      \[
	p= \Prob(\Ev{X_i \leq h}) = 1-G_{a,b}(1)
      \]
      \conc{Ainsi la \var $D_i$ admet une espérance et : $\E(D_i) =1- 
      G_{a, b}(1)$.}
%         \[
%         \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
%           \E(D_i) & = & 1 \times \Prob(\Ev{X_i \leq 1}) + 0 \times
%           \Prob(\Ev{X_i > 1}) \\[.2cm]
%           & = & \Prob(\Ev{X_i \leq 1}) \\[.2cm]
%           & = & 1 - G_{a, b}(1) & (d'après la question \itbf{4.b)})
%         \end{array}
%         \]
%         \conc{$\E(D_i) = 1 - G_{a, b}(1)$} %
        De plus, $\E(D_i) \neq 0$ puisque $G_{a, b}(1) \in \ ]0,1[$
        (d'après la question \itbf{1.a)}).

      \item On en déduit que $D_i$ et $S_i$ ne sont pas indépendantes
        puisque, d'après ce qui précède :
        \[
        \E(S_i D_i) \ = \ 0 \ \neq \ \E(S_i) \ \E(D_i)
        \]

      \item Considérons maintenant $j \in \llb 1, n\rrb$ tel que $j
        \neq i$. Comme $X_i$ et $X_j$ sont indépendantes :
        \[
        \begin{array}{rcl}
          \Prob(\Ev{S_i = 0} \cap \Ev{D_j = 0}) & = & \Prob(\Ev{X_i < h}
          \cap \Ev{X_j > 1}) \ = \ \Prob(\Ev{X_i < h}) \times 
	  \Prob(\Ev{X_j > 1}) 
	  \\[.2cm]
	  &=& \Prob(\Ev{S_i=0}) \times \Prob(\Ev{D_j=0})
        \end{array}
        \]
        \[
          \begin{array}{rcl}
            \Prob(\Ev{S_i = 0} \cap \Ev{D_j = 1}) & = & \Prob(\Ev{X_i < 
	    h} \cap \Ev{X_j \leq 1}) \ = \ \Prob(\Ev{X_i < h}) \times
           \Prob(\Ev{X_j \leq 1}) 
           \\[.2cm]
           &=& \Prob(\Ev{S_i=0}) \times \Prob(\Ev{D_j=1})
          \end{array}
        \]
        
        
        %\newpage
        
        
        \[
          \begin{array}{rcl}
            \Prob(\Ev{S_i = 1} \cap \Ev{D_j = 0}) & = & \Prob(\Ev{X_i 
	    \geq h} \cap \Ev{X_j > 1}) \ = \ \Prob(\Ev{X_i \geq h}) 
	    \times \Prob(\Ev{X_j > 1}) 
	    \\[.2cm]
	    &=& \Prob(\Ev{S_i=1}) \times \Prob(\Ev{D_j=0})
          \end{array}
        \]
        
        \[
          \begin{array}{rcl}
            \Prob(\Ev{S_i = 1} \cap \Ev{D_j = 1}) & = & \Prob(\Ev{X_i 
	    \geq h} \cap \Ev{X_j \leq 1}) \ = \ \Prob(\Ev{X_i \geq h}) 
	    \times \Prob(\Ev{X_j \leq 1})
	    \\[.2cm]
	    &=& \Prob(\Ev{S_i=1}) \times \Prob(\Ev{D_j=1})
          \end{array}
        \]
        On en déduit que $S_i$ et $D_j$ sont indépendantes.
      \end{noliste}
      \conc{$S_i$ et $D_j$ sont indépendantes si et seulement si $i
        \neq j$.}%~\\[-1.2cm]
        
      ~\\[-1.4cm]
    \end{proof}

  \item Déduire des questions précédentes l'expression de la
    covariance $\cov(\overline{S}_n, \overline{D}_n)$ de
    $\overline{S}_n$ et $\overline{D}_n$ en fonction de $n$,
    $G_{a,b}(h)$ et $G_{a,b}(1)$. Le signe de cette covariance
    était-il prévisible ?

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item D'après ce qui précède, pour tout $(i, j) \in \llb 1, n
        \rrb^2$ tel que $i \neq j$ :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          \Cov(S_i, D_j) & = & \E(S_i D_j) - \E(S_i) \ \E(D_j) \ = \ 0
          & (car $S_i$ et $D_j$ sont indépendantes) \nl
          \nl[-.2cm]
          \Cov(S_i, D_i) & = & \E(S_i D_i) - \E(S_i) \ \E(D_i)
          \\[.2cm]
          & = & 0 - G_{a,b}(h) \ (1 - G_{a, b}(1)) \\[.2cm]
          & = & - G_{a,b}(h) \ (1 - G_{a, b}(1))
        \end{array}
        \]

      \item On a alors :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          \Cov\left( \overline{S_n}, \overline{D_n} \right) & = & 
          \Cov\left( \dfrac{1}{n} \ \Sum{i=1}{n} S_i, \overline{D_n}
          \right) & (par définition de $\overline{S_n}$) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ \Cov\left( \Sum{i=1}{n} S_i, \overline{D_n}
          \right) & (par linéarité à gauche) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ \Sum{i=1}{n} \Cov\left( S_i, \overline{D_n}
          \right) & (par linéarité à gauche) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ \Sum{i=1}{n} \Cov\left( S_i, \dfrac{1}{n} \
            \Sum{j=1}{n} D_j \right) & (par définition de 
	    $\overline{D_n}$) \nl
          \nl[-.2cm]
           & = & \dfrac{1}{n^2} \ \Sum{i=1}{n} \Sum{j=1}{n} \Cov(S_i,
           D_j) & (par linéarité à droite) \nl
          \nl[-.2cm]
        \end{array}
        \]
        

        %\newpage


        \noindent
        Ainsi :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          \Cov\left( \overline{S_n}, \overline{D_n} \right) 
          & = & \dfrac{1}{n^2} \ \Sum{1 \leq i, j \leq n}{} \Cov(S_i,
          D_j) \\[.6cm] 
          & = & \dfrac{1}{n^2} \ \Sum{ %
              \scriptsize
              \begin{array}{c}
                1 \leq i, j \leq n \\
                i = j  
              \end{array}
            }{} \Cov(S_i, D_j) %
            + 
            \bcancel{%
              \dfrac{1}{n^2} \ 
              \Sum{ %
                \scriptsize
                \begin{array}{c}
                  1 \leq i, j \leq n \\
                  i \neq j  
                \end{array}
              }{} \Cov(S_i, D_j) %
            }
            \\[.8cm]
            & = & \dfrac{1}{n^2} \ \Sum{i=1}{n} \Cov(S_i, D_i) \\[.6cm]
            & = & \dfrac{1}{n^2} \ \Sum{i=1}{n} (-G_{a,b}(h) \ (1 -
            G_{a,b}(1))) \\[.6cm]
            & = & \dfrac{1}{n^2} \ n \ (-G_{a,b}(h) \ (1 -
            G_{a,b}(1))) \ = \ - \dfrac{G_{a,b}(h) \ (1 -
              G_{a,b}(1))}{n}
          \end{array}
          \]
          \conc{$\Cov\left( \overline{S_n}, \overline{D_n} \right) = -
            \dfrac{G_{a,b}(h) \ (1 - G_{a,b}(1))}{n}$}~

        \item Comme $G_{a, b}(h) > 0$ et $1 - G_{a, b}(1) > 0$,
          $\Cov(\overline{S_n}, \overline{D_n}) < 0$.\\
          Revenons à la définition de $S_i$ et $D_i$ pour comprendre
          ce signe.
          \begin{noliste}{$\stimes$}
          \item $S_i = 1$ ($0$ sinon) si le $\eme{i}$ individu de la
            cohorte est encore en vie après $h$ années,
          \item $D_i = 1$ ($0$ sinon) si le $\eme{i}$ individu de la
            cohorte est mort au cours de la première année.
          \end{noliste}
          Ainsi, $\overline{S_n}$ représente la proportion d'individus
          encore en vie après $h$ années et $\overline{D_n}$
          représente la proportion d'individus morts au cours de la
          première année. %
          \concL{Lorsqu'une de ces deux proportions augmente, l'autre
            à tendance à diminuer.\\
            Le signe négatif de la quantité était donc
            prévisible.}{15}~\\[-1.2cm]
      \end{noliste}
    \end{proof}
  \end{noliste}

\item
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Montrer que $\overline{S}_n$ est un estimateur sans biais et
    convergent du paramètre $G_{a,b}(h)$.

    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item La \var $\overline{S_n}$ admet une espérance en tant que
        combinaison linéaire des $\var$ $S_1$, \ldots, $S_n$ qui
        admettent toutes une espérance. De plus : 
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          \E(\overline{S_n}) & = & \E\left(\dfrac{1}{n} \ \Sum{i=1}{n}
            S_i \right) \\[.6cm]
          & = & \dfrac{1}{n} \ \Sum{i=1}{n} \ \E\left( S_i \right) &
          (par linéarité de l'espérance) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ \Sum{i=1}{n} \ G_{a, b}(h) & (d'après
          la question \itbf{10.a)}) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ n \ G_{a, b}(h) \ = \ G_{a, b}(h)
        \end{array}
        \]
        Donc : $b(\overline{S_n}) = \E(\overline{S_n}) - G_{a,b}(h)
        =0$.
        \conc{Ainsi, $\overline{S_n}$ est un estimateur sans biais de
          $G_{a, b}(h)$.}


        %\newpage


      \item La \var $\overline{S_n}$ admet un moment d'ordre $2$ en
        tant que combinaison linéaire des $\var$ $S_1$, \ldots, $S_n$
        qui sont {\bf indépendantes} (car les \var $X_1$, \ldots,
        $X_n$ le sont) et admettent toutes un moment d'ordre $2$
        puisqu'elles sont finies.\\
        Ainsi, $\overline{S_n}$ admet un risque quadratique et d'après
        la décomposition biais-variance :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{6.6cm}}
          r(\overline{S_n}) & = & \V(\overline{S_n}) +
          \left(\bcancel{b(\overline{S_n}}) \right)^2 \\[.2cm]
          & = & \V\left( \dfrac{1}{n} \ \Sum{i=1}{n} S_i \right)
          \\[.6cm] 
          & = & \dfrac{1}{n^2} \ \V\left( \Sum{i=1}{n} S_i \right) &
          (par propriété de la variance) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n^2} \ \Sum{i=1}{n} \V(S_i) &
          (par indépendance \\ des \var $S_1$, \ldots, $S_n$) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n^2} \ n \ \V(S_1) & (les \var $X_i$ étant
          toutes de même loi, \\ il en est de même des \var $S_i$) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ \V(S_1) \tendn 0
          & (car $\V(S_1)$ est une constante)
        \end{array}
        \]
        \conc{$\overline{S_n}$ est un estimateur convergent de 
	$G_{a,b}(h)$.}~\\[-1.6cm]
      \end{noliste}
    \end{proof}

  \item De quel paramètre, $\overline{D}_n$ est-il un estimateur sans
    biais et convergent ?
    
    \begin{proof}~
    \begin{noliste}{$\sbullet$}
    \item Montrons que $\overline{D_n}$ est un estimateur sans biais
      de $1-G_{a,b}(1)$.\\
      La \var $\overline{D_n}$ admet une espérance en tant que
      combinaison linéaire des $\var$ $D_1$, \ldots, $D_n$ qui
      admettent toutes une espérance. De plus :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          \E(\overline{D_n}) & = & \E\left(\dfrac{1}{n} \ \Sum{i=1}{n}
            D_i \right) \\[.6cm]
          & = & \dfrac{1}{n} \ \Sum{i=1}{n} \ \E\left( D_i \right) &
          (par linéarité de l'espérance) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ \Sum{i=1}{n} \ (1-G_{a, b}(1)) & (d'après
          la question \itbf{10.b)}) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ n \ (1-G_{a, b}(1)) \ = \ 1-G_{a, b}(1)
        \end{array}
        \]
        Donc : $b(\overline{D_n}) = \E(\overline{D_n}) - \Big(
        1- G_{a,b}(1)\Big)=0$.
    \conc{$\overline{D_n}$ est un estimateur sans biais de 
$1-G_{a,b}(1)$.}



%\newpage



  \item Montrons que $\overline{D_n}$ est un estimateur convergent de
    $1-G_{a,b}(1)$.\\
    La \var $\overline{D_n}$ admet un moment d'ordre $2$ comme
    combinaison linéaire des \var $D_1$, \ldots, $D_n$ qui sont {\bf
      indépendantes} (car les \var $X_1$, \ldots, $X_n$ le sont) et
    admettent toutes un moment d'ordre $2$
    puisqu'elles sont finies.\\
    Ainsi, $\overline{D_n}$ admet un risque quadratique et d'après la
    décomposition biais-variance :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{6.6cm}}
          r(\overline{D_n}) & = & \V(\overline{D_n}) +
          \left(\bcancel{b(\overline{D_n}}) \right)^2 
          \ = \ \V\left( \dfrac{1}{n} \ \Sum{i=1}{n} D_i \right)
          \\[.6cm] 
          & = & \dfrac{1}{n^2} \ \V\left( \Sum{i=1}{n} D_i \right) &
          (par propriété de la variance) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n^2} \ \Sum{i=1}{n} \V(D_i) &
          (par indépendance \\ des \var $D_1$, \ldots, $D_n$) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n^2} \ n \ \V(D_1) & (les \var $X_i$ étant
          toutes de même loi, \\ il en est de même des \var $D_i$) \nl
          \nl[-.2cm]
          & = & \dfrac{1}{n} \ \V(D_1) \tendn 0
          & (car $\V(D_1)$ est une constante)
        \end{array}
        \]
        \conc{$\overline{D_n}$ est un estimateur convergent de $1-G_{a,b}(1)$.}
    \end{noliste}
    
    ~\\[-1.4cm]
    \end{proof}
  \end{noliste}
  

  %\newpage
  
  
  
  


\item On pose : $z(a,b)=\ln(G_{a,b}(1))$ et $r(a,b)=\ln(G_{a,b}(h))$.\\
  Pour tout $n\in\N^*$, on pose : $Z_n = \ln\left(1-\overline{D}_n
    +\dfrac{1}{n}\right)$ et $R_n =
  \ln\left(\overline{S}_n+\dfrac{1}{n}\right)$.\\
  {\it On admet} que $Z_n$ et $R_n$ sont des estimateurs convergents
  de $z(a,b)$ et $r(a,b)$ respectivement.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Soit $\eps$, $\lambda$ et $\mu$ des réels strictement
    positifs.
    \begin{noliste}{(i)}
    \item Justifier l'inclusion suivante :
      \[
      \Ev{ \left\vert (\lambda Z_n-\mu R_n) - (\lambda z(a,b) -\mu
          r(a,b))\right\vert \geq \eps} \subset \Ev{\lambda
        \vert Z_n - z(a,b)\vert + \mu \vert R_n-r(a,b)\vert \geq
        \eps}.
      \]
      
    \begin{proof}~\\
    Soit $\omega \in \Omega$.\\
    Supposons : $\omega \ \in \ \Ev{ \left\vert (\lambda Z_n-\mu R_n) - 
    (\lambda \, z(a,b) -\mu \, r(a,b))\right\vert \geq \eps}$.\\[.1cm]
    Autrement dit : $\left\vert (\lambda Z_n(\omega)-\mu R_n(\omega)) - 
    (\lambda \, z(a,b) -\mu \, r(a,b))\right\vert \geq \eps$.\\
    Or :
    \[
    \begin{array}{rcl@{\qquad}>{\it}R{6.6cm}}
      & &\left\vert \ (\lambda Z_n(\omega)-\mu R_n(\omega)) \ - \ 
      (\lambda \, z(a,b) -\mu \,
        r(a,b)) \ \right\vert \\[.2cm]
      & = & \left\vert \ \lambda Z_n(\omega) - \lambda \, z(a,b)- \mu 
      R_n(\omega) +\mu \,
        r(a,b)) \ \right\vert\\[.2cm]
      & = & \left\vert \ \lambda (Z_n(\omega) - z(a,b)) \ - \ \mu 
      (R_n(\omega) -r(a,b)) \ \right\vert\\[.2cm]
      &\leq&  \left\vert \ \lambda (Z_n(\omega) - z(a,b)) \ \right\vert 
      \ + \ \left\vert \ \mu (R_n(\omega) -r(a,b)) \ \right\vert & (par 
      inégalité triangulaire)
      \nl
      \nl[-.2cm]
      & = & \lambda \left\vert \ (Z_n(\omega) - z(a,b)) \ \right\vert \ 
      + \ \mu \left\vert \ (R_n(\omega) - r(a,b)) \ \right\vert & (car 
      $\lambda>0$ et $\mu>0$)
    \end{array}
    \]
    Donc : $\lambda \left\vert \ (Z_n(\omega) - z(a,b)) \ \right\vert \ 
      + \ \mu \left\vert \ (R_n(\omega) - r(a,b)) \ \right\vert
      \geq \eps$.\\[.1cm]
    Autrement dit : $\omega \in \Ev{\lambda \vert Z_n - z(a,b)\vert + 
      \mu \vert R_n-r(a,b)\vert \geq \eps}$.~\\[-.8cm]
      
    \conc{D'où : $\Ev{ \left\vert (\lambda Z_n-\mu R_n) - (\lambda
          z(a,b) -\mu r(a,b))\right\vert \geq \eps} \subset
      \Ev{\lambda \vert Z_n - z(a,b)\vert + \mu \vert R_n-r(a,b)\vert
        \geq \eps}$.}
        
    ~\\[-1.4cm]
  \end{proof}
	
	
	
	%\newpage
	
      
    \item En déduire l'inégalité suivante :
    \end{noliste}
    \[
    \Prob([\vert(\lambda Z_n-\mu R_n)-\left(\lambda z(a,b) -\mu
      r(a,b)\right)\vert \geq \eps ]) \leq \Prob\left(\left[ \vert
        Z_n-z(a,b)\vert \geq \dfrac{\eps}{2\lambda}\right]\right) +
    \Prob\left(\left[\vert R_n-r(a,b)\vert \geq
        \dfrac{\eps}{2\mu}\right]\right).
    \]
    
    \begin{proof}~
    \begin{noliste}{$\sbullet$}
    \item D'après la question précédente, on a déjà :
    \[
    \Prob(\Ev{\left\vert (\lambda Z_n-\mu R_n) - (\lambda z(a,b) -\mu
          r(a,b))\right\vert \geq \eps}) \ \leq \ \Prob(\Ev{\lambda
        \vert Z_n - z(a,b)\vert + \mu \vert R_n-r(a,b)\vert \geq
        \eps}) \quad (\star)
    \]
    \item On note :
    \[
      \begin{array}{l}
        A \ = \ \Ev{\lambda \vert Z_n - z(a,b)\vert + \mu \vert 
	R_n-r(a,b)\vert \geq \eps}
	\\[.2cm]
	B \ = \ \Ev{\lambda \vert Z_n - 
      z(a,b)\vert \geq \dfrac{\eps}{2}}
      \\[.4cm]
      C \ = \ \Ev{\mu \vert 
      R_n - r(a,b)\vert \geq \dfrac{\eps}{2}}
      \end{array}
    \]
    Si on parvient à démontrer : $A \ \subset \ B \cup C$,
    alors par croissance de $\Prob$ :
    \[
      \Prob(A) \ \leq \ \Prob( B\cup C)
    \]
    Et comme :
    \[
    \begin{array}{rcll}
    \Prob(B\cup C) & = & \Prob(B)+\Prob(C)-\Prob(B\cap C)
    \\[.2cm]
    & \leq & \Prob(B)+\Prob(C)
    \end{array}
    \]
    En utilisant la propriété $(\star)$, on obtient 
    alors :
    \[
    \begin{array}{rcl@{\qquad}>{\it}R{6.6cm}}
    \Prob(\Ev{\left\vert (\lambda Z_n-\mu R_n) - (\lambda z(a,b) -\mu
    r(a,b))\right\vert \geq \eps})
    &\leq& \Prob(A) & (d'après $(\star)$)
    \nl
    \nl[-.2cm]
    &\leq& \Prob\left(B \cup C\right) 
    \\[.2cm]
    &\leq& \Prob\left(B\right) + \Prob\left(C \right)
    \end{array}
    \]
    ce qui permet de conclure la question.

    \item Il reste alors à montrer :
    $A \ \subset \ B \cup C$. Autrement dit :
    \[
      \forall \omega \in \Omega, \ \omega \in A \ \Rightarrow \ \omega 
      \in B \cup C
    \]
    ce qui équivaut par contraposée à :
    \[
      \forall \omega \in \Omega, \ \NON{\omega \in B \cup C} \ 
      \Rightarrow \ \NON{\omega \in A}
    \]
    Soit $\omega \in \Omega$.
    Supposons donc : 
    $
      \NON{\omega \in B \cup C}
    $.\\[.1cm]
    Ainsi : $\omega \in \overline{B \cup C} = \overline{B} \cap 
    \overline{C}$, ou encore :
    \[
      \lambda \vert Z_n(\omega) - z(a,b) \vert < \dfrac{\eps}{2}
      \quad \text{et} \quad \mu \vert R_n(\omega) -r(a,b) \vert 
      < \dfrac{\eps}{2}
    \]
    Alors, en sommant membre à membre :
    \[
      \lambda \vert Z_n(\omega) - z(a,b) \vert + \mu \vert R_n(\omega) 
      -r(a,b) \vert < \eps
    \]
    D'où :
    \[
      \omega \ \in \ \Ev{\lambda \vert Z_n(\omega) - z(a,b) \vert + \mu 
      \vert R_n(\omega) -r(a,b) \vert < \eps} \ = \
      \overline{A}
    \]
    De plus : $\omega \in \overline{A} \ \Leftrightarrow \
    \NON{\omega \in A}$.\\[.1cm]
    On a donc bien démontré : $A \ \subset \ B \cup C$.
    
    {\small
    \hspace*{-1.4cm}\fbox{
      \begin{tabular}{c}
    	$\Prob([\vert(\lambda Z_n-\mu R_n)-\left(\lambda z(a,b) -\mu
      r(a,b)\right)\vert \geq \eps ]) \leq \Prob\left(\left[ \vert
        Z_n-z(a,b)\vert \geq \dfrac{\eps}{2\lambda}\right]\right) +
    \Prob\left(\left[\vert R_n-r(a,b)\vert \geq
        \dfrac{\eps}{2\mu}\right]\right).$
      \end{tabular}
      }}
      \end{noliste}
      
      
      
      %\newpage
      
      
    
      ~\\[-1.4cm]
    \end{proof}
    
  \item Pour tout $n\in\N^*$, on pose : $B_n=\dfrac{2}{h-1}Z_n -
    \dfrac{2}{h(h-1)}R_n$.\\
    Montrer que $B_n$ est un estimateur convergent du paramètre $b$.
    
    \begin{proof}~\\
    On pose $\lambda=\dfrac{2}{h-1}$ et $\mu=\dfrac{2}{h(h-1)}$. \\
    On remarque que $\lambda>0$ et $\mu>0$, car $h\geq 2$, et 
    $B_n=\lambda Z_n-\mu R_n$.\\
    D'après la question précédente et puisqu'une probabilité est 
    toujours positive, on obtient :
    \[
    0 \ \leq \ \Prob([\vert B_n-\left(\lambda z(a,b) -\mu
      r(a,b)\right)\vert \geq \eps ]) \ \leq \ \Prob\left(\left[ \vert
        Z_n-z(a,b)\vert \geq \dfrac{\eps}{2\lambda}\right]\right) +
    \Prob\left(\left[\vert R_n-r(a,b)\vert \geq
        \dfrac{\eps}{2\mu}\right]\right)
    \]
    Or :
    \begin{noliste}{$\stimes$}
    \item par hypothèse, $Z_n$ est un estimateur convergent de 
    $z(a,b)$. \\[.1cm]
    D'où : $\dlim{n\to+\infty} \Prob\left( \left[\vert 
    Z_n-z(a,b)\vert \geq \dfrac{\eps}{2\lambda}\right]\right)=0$.
    \item de plus, $R_n$ est un estimateur convergent de 
    $r(a,b)$.\\[.1cm] 
    D'où :
    $\dlim{n\to+\infty} \Prob\left( \left[\vert 
    R_n-r(a,b)\vert \geq \dfrac{\eps}{2\mu}\right]\right)=0$.
    \end{noliste}
    D'après le théorème d'encadrement, on a donc :
    \[
    \dlim{n\to+\infty} \Prob([\vert B_n-\left(\lambda z(a,b) -\mu
      r(a,b)\right)\vert \geq \eps ])=0
    \]
    c'est-à-dire que $B_n$ est un estimateur convergent de $\lambda 
    z(a,b)-\mu r(a,b)$.\\
    Or :
    \[
    \begin{array}{rcl@{\qquad}>{\it}R{6.6cm}}
      \lambda z(a,b)-\mu r(a,b) & = & \dfrac{2}{h-1}\ln\left(G_{a,b}(1)\right) - \dfrac{2}{h(h-1)} \ln\left(G_{a,b}(h)\right)\\[.4cm]
      & = & \dfrac{2}{h-1}\left[\ln\left(\exp\left(-a-\dfrac{b}{2}\right)\right)-\dfrac{1}{h}\ln\left(\exp\left(-ah-\dfrac{b}{2}h^2\right)\right)\right]\\[.4cm]
      & = & \dfrac{2}{h-1}\left[\left(-a-\dfrac{b}{2}\right)-\dfrac{1}{h}\left(-ah-\dfrac{b}{2}h^2\right)\right]\\[.4cm]
      & = & \dfrac{2}{h-1}\left[-\bcancel{a}-\dfrac{b}{2}+\bcancel{a}+\dfrac{b}{2}h\right]\\[.4cm]
      & = & \dfrac{\bcancel{2}}{\bcancel{h-1}}\left[\dfrac{b}{\bcancel{2}}\bcancel{(h-1)}\right]\\[.4cm]
      & = & b
    \end{array}
    \]
    \conc{$B_n$ est un estimateur convergent de $b$.}
    
    ~\\[-1.4cm]
    \end{proof}
  \end{noliste}
\end{noliste}

% 


\end{document}



