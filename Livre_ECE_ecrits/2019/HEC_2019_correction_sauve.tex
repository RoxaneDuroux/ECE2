\documentclass[11pt]{article}%
\usepackage{geometry}%
\geometry{a4paper,
  lmargin=2cm,rmargin=2cm,tmargin=2.5cm,bmargin=2.5cm}

% \input{../macros_Livre.tex}
\input{../macros.tex}

% \renewcommand{\thesection}{\Roman{section}.\hspace{-.3cm}}
% \renewcommand{\thesubsection}{\Alph{subsection}.\hspace{-.2cm}}
\pagestyle{fancy} %
\lhead{ECE2 \hfill Mathématiques \\} %
\chead{\hrule} %
\rhead{} %
\lfoot{} %
\cfoot{} %
\rfoot{\thepage} %

\renewcommand{\headrulewidth}{0pt}% : Trace un trait de séparation
                                    % de largeur 0,4 point. Mettre 0pt
                                    % pour supprimer le trait.

\renewcommand{\footrulewidth}{0.4pt}% : Trace un trait de séparation
                                    % de largeur 0,4 point. Mettre 0pt
                                    % pour supprimer le trait.

\setlength{\headheight}{14pt}

\title{\bf \vspace{-1.6cm} HEC 2019} %
\author{} %
\date{} %
\begin{document}

\maketitle %
\vspace{-1.2cm}\hrule %
\thispagestyle{fancy}

\vspace*{-.2cm}

%%DEBUT

\section*{Exercice}

\begin{noliste}{1.}
  \setlength{\itemsep}{4mm} %
\item Dans cette question, on considère les matrices $C =
  \begin{smatrix}
    0 \\
    1 \\
    2
  \end{smatrix}
  \in \M{3, 1}$, $L =
  \begin{smatrix}
    1 & 2 & -1
  \end{smatrix}
  \in \M{3, 1}$ et le produit matriciel $M = CL$.

  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm} %
  \item
    \begin{nonoliste}{(i)}
      \setlength{\itemsep}{2mm} %
    \item Calculer $M$ et $M^2$.

      \begin{proof}~%
        \begin{noliste}{$\sbullet$}
        \item Tout d'abord : $M \ = \
          \begin{smatrix}
            0 \\
            1 \\
            2
          \end{smatrix}
          \begin{smatrix}
            1 & 2 & -1
          \end{smatrix}
          \ = \ 
          \begin{smatrix}
            0 & 0 & 0 \\
            1 & 2 & -1 \\
            2 & 4 & -2 
          \end{smatrix}
          $. %
          \conc{$M =
            \begin{smatrix}
              0 & 0 & 0 \\
              1 & 2 & -1 \\
              2 & 4 & -2
            \end{smatrix}$}
          \begin{remarkL}{.98}%
            Si on note $C_1$, $C_2$ et $C_3$ les colonnes de $M$, on
            remarque :
            \[
            C_1 = C, \quad C_2 = 2 \, C \quad \text{ et } \quad C_3 =
            - \, C
            \]
            La matrice $M$ est donc obtenue par concaténation de
            copies, à coefficients multiplicatifs près, de la colonne
            $C$. L'objectif de l'énoncé est l'étude des propriétés de
            telles matrices.
          \end{remarkL}

        \item $
          \begin{array}[t]{R{1.4cm}rcl@{\qquad}>{\it}R{5cm}}
            Ensuite : & M^2 & = & M \times M \ = \ CL \times CL 
            \\[.2cm]
            & & = & C \times (LC) \times L
            & (par associativité)
            \nl
            % \nl[-.2cm]
            & & = & (LC) \cdot C \times L
            & (car comme $L \in \M{1, 3}$ et $C \in \M{3, 1}$, $LC$
            est un réel)
            \nl
            %\nl[-.2cm]
            & & = & 0 \cdot M \ = \ 0_{\M{3}}
          \end{array}
          $\\
          En effet : $LC \ = \
          \begin{smatrix}
            1 & 2 & -1
          \end{smatrix}
          \begin{smatrix}
            0 \\
            1 \\
            2
          \end{smatrix}
          \ = \ 1 \times 0 + 2 \times 1 - 1 \times 2 = 2 - 2 = 0$. %
          \conc{Ainsi : $M^{2} = 0_{\M{3}}$.}
        \end{noliste}
        \begin{remarkL}{.98}%
          Évidemment, on peut aussi effectuer le calcul de $M^2$
          directement :
          \[
          M^2 \ = \             
          \begin{smatrix}
            0 & 0 & 0 \\
            1 & 2 & -1 \\
            2 & 4 & -2
          \end{smatrix}
          \begin{smatrix}
            0 & 0 & 0 \\
            1 & 2 & -1 \\
            2 & 4 & -2
          \end{smatrix}
          \ = \ 
          \begin{smatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 0
          \end{smatrix}
          \]
          Un tel calcul permet assurément d'obtenir tous les points de
          la question mais n'est pas dans l'esprit de la construction
          très particulière de la matrice $M$. Il s'agit ici de faire
          apparaître sur un exemple simple de petite taille
          (manipulation d'une matrice ligne et d'une matrice colonne à
          $3$ éléments), des propriétés qu'on généralisera à des
          matrices de tailles quelconques.
        \end{remarkL}~\\[-1.4cm]
      \end{proof}


      \newpage


    \item Déterminer le rang de $M$.

      \begin{proof}~%
        \[
        \begin{array}{rcl}
          \rg\left( 
            \begin{smatrix} 
              0 & 0 & 0 \\
              1 & 2 & -1 \\
              2 & 4 & -2 
            \end{smatrix}
          \right)
          & = & 
          \rg\left( 
            \begin{smatrix} 
              0 \\ 
              1 \\
              2 
            \end{smatrix}
            ,
            \begin{smatrix} 
              0 \\ 
              2 \\
              4 
            \end{smatrix}
            ,
            \begin{smatrix} 
              0 \\ 
              -1 \\
              -2 
            \end{smatrix}
          \right)
          \\[1cm]
          & = &
          \rg\left( 
            \begin{smatrix} 
              0 \\ 
              1 \\
              2 
            \end{smatrix}
            ,
            2 \, 
            \begin{smatrix} 
              0 \\ 
              1 \\
              2 
            \end{smatrix}
            ,
            - \,
            \begin{smatrix} 
              0 \\ 
              1 \\
              2 
            \end{smatrix}
          \right)
          \ = \ 
          \rg\left( 
            \begin{smatrix} 
              0 \\ 
              1 \\
              2 
            \end{smatrix}
          \right) \ = \ 1
        \end{array}
        \]
        \conc{$\rg(M) = 1$}~\\[-1.2cm]
      \end{proof}

    \item La matrice $M$ est-elle diagonalisable ?

      \begin{proof}~%
        \begin{noliste}{$\sbullet$}
        \item D'après la question \itbf{1.a)(i)}, le polynôme $Q(X) =
          X^2$ est un polynôme annulateur de la matrice $M$. Ainsi :
          \[
          \spc(M) \subset \{\text{racines de $Q$}\} = \{0\}
          \]
          \conc{Le réel $0$ est la seule valeur propre possible de
            $M$.}

        \item D'après la question précédente : $\rg(M) = 1 \neq 3 =
          \dim\big( \M{3,1} \big)$. %
          \conc{Ainsi, la matrice $M$ n'est pas inversible et $0$ est
            la seule valeur propre de $M$.}

        \item Notons $\B$ la base canonique de $\R^3$. Considérons
          l'endomorphisme $f \in \LL{\R^3}$ dont la représentation
          dans la base $\B$ est $M$. Par le théorème du rang :
          \[
          \begin{array}{ccccc}
            \dim\big( \R^3 \big) & = & \dim\big( \kr(f) \big) & + &
            \dim\big( \im(f) \big)
            \\[.2cm]
            \shortparallel & & \shortparallel & & \shortparallel
            \\[.2cm]
            3 & & \dim\big(  E_0(f) \big) & & \rg(f)
            \\[.2cm]
            \shortparallel & & \shortparallel & & \shortparallel
            \\[.2cm]
            \dim\big( \M{3,1} \big) & = & \dim\big( E_0(M) \big) & + & 
            \rg(M)
          \end{array}
          \]
          \conc{Ainsi : $\dim\big( E_0(M) \big) = \dim\big( \M{3,1}
            \big) - \rg(M) = 3 - 1 = 2$.} %
        \end{noliste}
        \conc{Comme $\dim\big( E_0(M) \big) = 2 \neq 3 = \dim\big(
          \M{3,1} \big)$, la matrice $M$ n'est pas diagonalisable.}
        \begin{remarkL}{.98}
          \begin{noliste}{$\sbullet$}
          \item On a démontré que la matrice $M$ possédait une unique
            valeur propre. Dans ce cas, il est classique de procéder
            par l'absurde pour démontrer que $M$ n'est pas
            diagonalisable.

          \item Supposons que $M$ est diagonalisable.\\
            Il existe donc une matrice inversible $P \in \M{3}$ et une
            matrice diagonale $D \in \M{3}$ dont les coefficients
            diagonaux sont les valeurs propres de $M$ telles que $% =
            PDP^{-1}$.\\
            Or $0$ est la seule valeur propre de $M$. Ainsi $D =
            0_{\M{3}}$ et :
            \[
            M = PDP^{-1} = P \, 0_{\M{3}} \, P^{-1} = 0_{\M{3}}
            \]
            Absurde !
          \end{noliste}
        \end{remarkL}~\\[-1.4cm]
      \end{proof}
    \end{nonoliste}


    \newpage


    \begin{remarkL}{.99}
      \begin{noliste}{$\sbullet$}
      \item Il était aussi possible de déterminer le sous-espace
        propre de $M$ associé à la valeur propre $0$.\\
        Soit $X =
        \begin{smatrix}
          x \\ 
          y \\
          z
        \end{smatrix} 
        \in \M{3,1}$.\\[-.1cm]
        \[
        \begin{array}{rcl}
          X\in E_{0}(M)
          & \Longleftrightarrow & M \, X = 0_{\M{3}}
          \\[.2cm]
          & \Longleftrightarrow & 
          \begin{smatrix}
            0 & 0 & 0 \\
            1 & 2 & -1 \\
            2 & 4 & -2
          \end{smatrix}
          \begin{smatrix}
            x \\
            y \\
            z
          \end{smatrix}
          =
          \begin{smatrix}
            0 \\
            0 \\
            0
          \end{smatrix}
          \\[.6cm]
          & \Longleftrightarrow & 
          \left\{
            \begin{array}{rcrcrcl}
              & & & & 0 & = & 0 \\
              x & + & 2 \, y & - & z & = & 0 \\
              2 \, x & + & 4 \, y & - & 2 \, z & = & 0
            \end{array}
          \right.
          \\[.6cm]
          & 
          \begin{arrayEq}
            L_3 \leftarrow L_3 - 2 \, L_2
          \end{arrayEq}
          & 
          \left\{
            \begin{array}{rcrcrcl}
              & & & & 0 & = & 0 \\
              x & + & 2 \, y & - & z & = & 0 \\
              & & & & 0 & = & 0 
            \end{array}
          \right.
          \\[.6cm]
          &
          \Longleftrightarrow
          &
          \left\{
            \begin{array}{rcrcr}
              x & = & -2 \, y & + & z \\
            \end{array}
          \right.
        \end{array}
        \]        
        Finalement on obtient l'expression de $E_0(M)$ suivante :\\[-.2cm]
        \[
        \begin{array}{rcl}
          E_{0}(M) & = & 
          \{ 
          X \in \M{3,1} \ | \ MX = 0_{\M{3}} \}
          \ = \ 
          \{
          \begin{smatrix}
            x \\ 
            y \\
            z
          \end{smatrix}
          \ | \
          x = - 2 y + z
          \}
          \\[.6cm]
          & = & 
          \{
          \begin{smatrix}
            - 2 y + z \\ 
            y \\ 
            z
          \end{smatrix}
          \ | \
          (y, z) \in \R^2
          \}
          \ = \ 
          \{
          y \cdot
          \begin{smatrix}
            -2 \\ 
            1 \\ 
            0
          \end{smatrix}
          +
          z \cdot
          \begin{smatrix}
            1 \\ 
            0 \\ 
            1
          \end{smatrix}
          \ | \ (y, z) \in \R^2
          \}
          \\[.6cm]
          & = & 
          \Vect{
            \begin{smatrix}
              -2 \\
              1 \\ 
              0
            \end{smatrix}
            ,
            \begin{smatrix}
              1 \\
              0 \\ 
              1
            \end{smatrix}
          }
        \end{array}
        \]

      \item Il faut s'habituer à déterminer les ensembles
        $E_{\lambda}(M)$ par lecture de la matrice $M - \lambda \, I_3$.\\
        Ici, on a $\lambda = 0$. On cherche donc les vecteurs $X =
        \begin{smatrix}
          x \\
          y \\
          z
        \end{smatrix}
        $ de $E_{0}(M) $ c'est-à-dire les vecteurs tels que : $M \, X
        = 0_{\M{3,1}}$. Or :\\[-.1cm]
        \[
        \begin{array}{rcl}
          \begin{smatrix}
            0 & 0 & 0 \\
            1 & 2 & -1 \\
            2 & 4 & -2
          \end{smatrix}
          \begin{smatrix}
            x \\
            y \\
            z 
          \end{smatrix}
          & = & x \cdot C_1 + y \cdot C_2 + z \cdot C_3
          \\[-.2cm]
          & = & 
          x \cdot
          \begin{smatrix}
            0 \\
            1 \\
            2
          \end{smatrix}
          + y \cdot
          \begin{smatrix}
            0 \\
            2 \\
            4 
          \end{smatrix}
          + z \cdot
          \begin{smatrix}
            0 \\
            -1 \\
            -2
          \end{smatrix}
        \end{array}
        \]~\\[-.6cm]
        Pour obtenir le vecteur $
        \begin{smatrix}
          0 \\
          0 \\
          0
        \end{smatrix}
        $ à l'aide de cette combinaison linéaire, plusieurs choix sont
        possibles. Plus précisément :
        \begin{noliste}{$\stimes$}
        \item si l'on choisit $y = 0$, il suffit de prendre $x = z$
          pour obtenir le vecteur nul.\\
          En prenant (par exemple) $z = 1$, on obtient : $x = 1$.
        \item si l'on choisit $z = 0$, il suffit de prendre $x = -2y$
          pour obtenir le vecteur nul.\\
          En prenant (par exemple) $y = 1$, on obtient : $y = -2$.
        \end{noliste}
        On obtient ainsi : $E_0(M) \supset \Vect{
            \begin{smatrix}
              -2 \\
              1 \\ 
              0
            \end{smatrix}
            ,
            \begin{smatrix}
              1 \\
              0 \\ 
              1
            \end{smatrix}
          }$.\\
          Et l'égalité est vérifiée car ces deux espaces vectoriels
          sont de même dimension.
        \end{noliste}      
      \end{remarkL}

  \item Soit $P =
    \begin{smatrix}
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      0 & -2 & 1
    \end{smatrix}
    $. Justifier que la matrice $P$ est inversible et calculer le
    produit $P
    \begin{smatrix}
      0 \\
      1 \\
      2
    \end{smatrix}
    $.

    \begin{proof}~%
      \begin{noliste}{$\sbullet$}
      \item Tout d'abord :
        \[
        \rg(P) \ = \ \rg\left(
          \begin{smatrix}
            0 & 1 & 0 \\
            1 & 0 & 0 \\
            0 & -2 & 1
          \end{smatrix}
        \right)
        \begin{arrayEg}
          L_1 \leftrightarrow L_2
        \end{arrayEg}
        \rg\left(
          \begin{smatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & -2 & 1
          \end{smatrix}
        \right)
        \begin{arrayEg}
          L_3 \leftarrow L_3 + 2 \, L_2
        \end{arrayEg}
        \rg\left(
          \begin{smatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
          \end{smatrix}
        \right) \ = \ 3
        \]
        \conc{On en conclut que la matrice $P$ est inversible.}

      \item Ensuite : 
        \[
        P
        \begin{smatrix}
          0 \\
          1 \\
          2
        \end{smatrix}
        \ = \ 
        \begin{smatrix}
          0 & 1 & 0 \\
          1 & 0 & 0 \\
          0 & -2 & 1
        \end{smatrix}
        \begin{smatrix}
          0 \\
          1 \\
          2
        \end{smatrix}
        \ = \ 
        \begin{smatrix}
          1 \\
          0 \\
          0
        \end{smatrix}
        \]        
      \end{noliste}
      \conc{$P
        \begin{smatrix}
          0 \\
          1 \\
          2
        \end{smatrix}
        \ = \ 
        \begin{smatrix}
          1 \\
          0 \\
          0
        \end{smatrix}$}~\\[-1.2cm]
    \end{proof}

  \item Trouver une matrice inversible $Q$ dont la transposée ${}^tQ$
    vérifie : ${}^tQ
    \begin{smatrix}
      1 \\
      2 \\
      -1
    \end{smatrix}
    =
    \begin{smatrix}
      1 \\
      0 \\
      0
    \end{smatrix}
    $.

    \begin{proof}~%
      \begin{noliste}{$\sbullet$}
      \item Notons ${}^tQ =
        \begin{smatrix}
          x & u & a \\
          y & v & b \\
          z & w & c
        \end{smatrix}
        $ où $(x, y, z, u, v, w, a, b, c) \in \R^9$. Remarquons tout
        d'abord :
        \[
        {}^tQ
        \begin{smatrix}
          1 \\
          2 \\
          -1
        \end{smatrix}
        \ = \ 
        \begin{smatrix}
          x & u & a \\
          y & v & b \\
          z & w & c
        \end{smatrix}
        \begin{smatrix}
          1 \\
          2 \\
          -1
        \end{smatrix}
        \ = \ 1 \cdot
        \begin{smatrix}
          x \\
          y \\
          z
        \end{smatrix}
        + 2 \cdot 
        \begin{smatrix}
          u \\
          v \\
          w
        \end{smatrix}
        - 1 \cdot 
        \begin{smatrix}
          a \\
          b \\
          c
        \end{smatrix}
        \]

      \item Pour obtenir le vecteur $
        \begin{smatrix}
          1 \\
          0 \\
          0
        \end{smatrix}
        $ à l'aide de cette combinaison linéaire, plusieurs choix sont
        possibles. Plus précisément :
        \begin{noliste}{$\stimes$}
        \item pour obtenir : $x + 2 \, u - a = 1$, on peut prendre $x
          = 1$ et $u = a = 0$.

        \item pour obtenir : $y + 2 \, v - b = 0$, on peut prendre $y
          = 2$ et $v = -1$ et $b = 0$.

        \item pour obtenir : $z + 2 \, w - c = 0$, on peut prendre $z
          = -1$ et $w = 1$ et $c = 1$.      
        \end{noliste}
        On construit ainsi la matrice ${}^tQ =
        \begin{smatrix}
          1 & 0 & 0 \\
          2 & -1 & 0 \\
          -1 & 1 & 1 
        \end{smatrix}$ et donc $Q =         
        \begin{smatrix}
          1 & 2 & -1 \\
          0 & -1 & 1 \\
          0 & 0 & 1
        \end{smatrix}$.\\
        Cette matrice est triangulaire supérieure et à coefficients
        diagonaux tous non nuls.\\
        Elle est donc inversible.
      \end{noliste}
      \conc{La matrice $Q = 
        \begin{smatrix}
          1 & 2 & -1 \\
          0 & -1 & 1 \\
          0 & 0 & 1
        \end{smatrix}$ est inversible et vérifie ${}^tQ
        \begin{smatrix}
          1 \\
          2 \\
          -1
        \end{smatrix}
        =
        \begin{smatrix}
          1 \\
          0 \\
          0
        \end{smatrix}
        $.}


      \newpage


      \begin{remarkL}{.98}%
        \begin{noliste}{$\sbullet$}
        \item Remarquons tout d'abord, par propriété de l'application
          transposée :
          \[
          {}^tQ 
          \begin{smatrix}
            1 \\
            2 \\
            -1
          \end{smatrix}
          =
          \begin{smatrix}
            1 \\
            0 \\
            0
          \end{smatrix}          
          \quad \Leftrightarrow \quad
          \begin{smatrix}
            1 & 2 & -1
          \end{smatrix}
          \, Q =
          \begin{smatrix}
            1 & 0 & 0 
          \end{smatrix}
          \]
          L'introduction de la transposée a donc pour but ici de faire
          apparaître un calcul sur des lignes plutôt que sur des
          colonnes. Si on travaille directement sur l'égalité de
          droite, on obtient, avec les notations précédentes :
          \[
          \begin{smatrix}
            1 & 2 & -1
          \end{smatrix}
          \, Q \ = \ 1 \cdot 
          \begin{smatrix}
            x & y & z
          \end{smatrix}
          + 2 \cdot 
          \begin{smatrix}
            u & v & w
          \end{smatrix}
          - 1 \cdot 
          \begin{smatrix}
            a & b & c
          \end{smatrix}
          \]
          On obtient évidemment les mêmes équations que précdemment.

        \item On retiendra qu'en multipliant $Q \in \M{3}$ à droite
          (resp. gauche) par une matrice colonne (resp. ligne), on
          obtient une combinaison linéaire des colonnes (resp. lignes)
          de la matrice $Q$. On peut retenir l'idée développée dans le
          paragraphe par la forme :
          \[
          L \ A \ C
          \]
          qui signifie qu'avec une multiplication à gauche, on
          effectue une opération sur les (L)ignes, tandis qu'avec une
          multiplication à droite, on effectue une multiplication sur
          les (C)olonnes.

        \item D'autres choix étaients possibles pour la matrice
          $Q$. Par exemple, on pouvait choisir :
          \[
          Q =
          \begin{smatrix}
            1 & -1 & 1 \\
            0 & 1 & 0 \\
            0 & 1 & 1
          \end{smatrix}
          \quad \text{ ou encore } \quad 
          Q =
          \begin{smatrix}
            1 & -1 & 0 \\
            0 & 1 & 1 \\
            0 & 1 & 2
          \end{smatrix}
          \]
          Il est à noter qu'il ne suffit pas de résoudre les
          contraintes issues des équations pour exhiber une matrice
          $Q$ satisfaisante. Il est précisé dans l'énoncé que $Q$ est
          une matrice inversible. Cela explique la direction prise par
          la résolution proposée initialement : les choix effectués
          permettent de construire une matrice qui est visiblement
          inversible (triangulaire supérieure et à coefficients
          diagonaux tous non nuls).
        \end{noliste}
      \end{remarkL}~\\[-1.4cm]
    \end{proof}

  \item Pour une telle matrice $Q$, calculer le produit $P \, M \, Q$.

    \begin{proof}~%
      \[
      \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
        P \, M \, Q & = & P \, (CL) \, Q
        \\[.2cm]
        & = & (PC) \, (LQ)
        \\[.2cm]
        & = &
        \begin{smatrix}
          1 \\
          0 \\
          0
        \end{smatrix}
        \, 
        \begin{smatrix}
          1 & 0 & 0
        \end{smatrix}
        & (d'après les calculs effectués en \itbf{1.b)} et en
        \itbf{1.c)})
        \nl
        \nl[-.2cm]
        & = &
        \begin{smatrix}
          1 & 0 & 0 \\
          0 & 0 & 0 \\
          0 & 0 & 0
        \end{smatrix}
        \ = \ E_{1,1}
      \end{array}      
      \]
      \conc{Ainsi : $P \, M \, Q = E_{1,1}$.}~\\[-1.2cm]
    \end{proof}
  \end{noliste}


  \newpage


\item La fonction \Scilab{} suivante permet de multiplier la $\eme{i}$
  ligne $L_i$ d'une matrice $A$ par une réel sans modifier ses autres
  lignes, c'est-à-dire de lui appliquer l'opération élémentaire $L_i
  \leftarrow a \, L_i$ (où $a \neq 0$).
  \begin{scilab}
    & \tcFun{function} \tcVar{B} = \underline{multilig}(\tcVar{a},
    \tcVar{i}, \tcVar{A}) \nl %
    & \qquad [n, p] = size(\tcVar{A}) \nl %
    & \qquad \tcVar{B} = \tcVar{A} \nl %
    & \qquad \tcFor{for} j = 1:p \nl %
    & \qquad \qquad \tcVar{B}(\tcVar{i}, j) = \tcVar{a} \Sfois{}
    \tcVar{B}(\tcVar{i}, j) \nl %
    & \qquad \tcFor{end} \nl %
    & \tcFun{endfunction}
  \end{scilab}
  \begin{remarkL}{.98}%
    \begin{noliste}{$\sbullet$}
    \item Le code de ce programme est plutôt simple à comprendre : 
      \begin{noliste}{$\stimes$}
      \item on crée une copie de la matrice {\tt A} que l'on stocke
        dans la variable {\tt B},
      \item on met à jour la $\eme{\text{\tt i}}$ ligne de {\tt B} en
        modifiant un par un les éléments de cette ligne à l'aide de la
        boucle {\tt for}.        
      \end{noliste}

    \item Pour être plus proche de l'opération élémentaire $L_i
      \leftarrow a \, L_i$, on pouvait opter pour une présentation ne
      nécissitant pas l'utilisation de la boucle {\tt for} :\\[-.2cm]
      \begin{scilab}
        & \tcFun{function} \tcVar{B} = \underline{multilig}(\tcVar{a},
        \tcVar{i}, \tcVar{A}) \nl %
        & \qquad [n, p] = size(\tcVar{A}) \nl %
        & \qquad \tcVar{B} = \tcVar{A} \nl %
        & \qquad \tcVar{B}(\tcVar{i}, :) = \tcVar{a} .\Sfois{}
        \tcVar{B}(i, :) \nl %
        & \qquad \tcFor{end} \nl %
        & \tcFun{endfunction}
      \end{scilab}
      L'appel {\tt B(i, :)} permet d'accéder à la $\eme{\text{\tt i}}$
      ligne de {\tt B}. On peut modifier cette ligne en lui assignant
      une matrice ligne de même taille, ce qu'on fait ici.
    \end{noliste}
  \end{remarkL}

  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm} %
  \item Donner le code \Scilab{} de deux fonctions {\tt adlig}
    (d'arguments {\tt b}, {\tt i}, {\tt j}, {\tt A}) et {\tt echlig}
    (d'arguments {\tt i}, {\tt j}, {\tt A}) permettant d'effectuer
    respectivement les autres opérations sur les lignes d'une matrice
    :
    \[
    Li \leftarrow L_i + b \, L_j \ (i \neq j) \quad \text{ et } \quad
    L_i \leftrightarrow L_j \ (i \neq j)
    \]

    \begin{proof}~%
      \begin{noliste}{$\sbullet$}
      \item On s'inspire de la fonction donnée pour créer {\tt adlig} :
        \begin{scilab}
          & \tcFun{function} \tcVar{B} =
          \underline{adlig}(\tcVar{b}, \tcVar{i}, \tcVar{j}, \tcVar{A}) \nl %
          & \qquad [n, p] = size(\tcVar{A}) \nl %
          & \qquad \tcVar{B} = \tcVar{A} \nl %
          & \qquad \tcFor{for} k = 1:p \nl %
          & \qquad \qquad \tcVar{B}(\tcVar{i}, k) =
          \tcVar{B}(\tcVar{i}, k) + \tcVar{b} \Sfois{}
          \tcVar{B}(\tcVar{j}, k) \nl %
          & \qquad \tcFor{end} \nl %
          & \tcFun{endfunction}
        \end{scilab}
        \begin{remark}%
          On note que la variable {\tt j} est ici une variable
          d'entrée du programme (on l'utilise pour désigner la ligne
          ajoutée dans l'opération élémentaire $Li \leftarrow L_i + b
          \, L_j \ (i \neq j)$). Cela oblige à renommer la variable
          d'itération du programme {\tt multilig}.
        \end{remark}


        \newpage


      \item La fonction {\tt echlig} est créée suivant le même
        principe :
        \begin{scilab}
          & \tcFun{function} \tcVar{B} =
          \underline{echlig}(\tcVar{i}, \tcVar{j}, \tcVar{A}) \nl %
          & \qquad [n, p] = size(\tcVar{A}) \nl %
          & \qquad \tcVar{B} = \tcVar{A} \nl %
          & \qquad \tcVar{aux} = 0 \nl %
          & \qquad \tcFor{for} k = 1:p \nl %
          & \qquad \qquad \tcVar{aux} = \tcVar{B}(\tcVar{i}, k) \nl %
          & \qquad \qquad \tcVar{B}(\tcVar{i}, k) =
          \tcVar{B}(\tcVar{j}, k) \nl % 
          & \qquad \qquad \tcVar{B}(\tcVar{j}, k) = aux \nl % 
          & \qquad \tcFor{end} \nl %
          & \tcFun{endfunction}
        \end{scilab}
      \end{noliste}
      \begin{remarkL}{.98}
        \begin{noliste}{$\sbullet$}
        \item On a introduit ici une variable auxiliaire appelée {\tt
            aux}. Le but de cette variable est de ne pas perdre
          d'information lors de l'échange des valeurs des deux
          coefficients de la même colonne. Plus précisément :
          \begin{noliste}{$\stimes$}
          \item l'instruction de la ligne \ligne{6} permet de stocker
            la valeur de {\tt B(i, k)}.
          \item en ligne \ligne{7}, on écrase la valeur du coefficient
            $B(i, k)$ en lui affectant la valeur $B(j, k)$.
          \item enfin, en ligne \ligne{8}, on affecte à {\tt B(i, k)}
            la valeur de {\tt aux}, c'est-à-dire la valeur {\bf
              initiale} (et pas la nouvelle valeur) du coefficient
            $B(i, k)$.
          \end{noliste}

        \item On pouvait aussi tirer parti du fait que l'on travaille
          sur une copie {\tt B} de la matrice {\tt A} d'entrée (jamais
          modifiée) pour ne pas introduire de variable auxiliaire {\tt
          aux}.\\[-.2cm]
          \begin{scilab}
            & \tcFun{function} \tcVar{B} =
            \underline{echlig}(\tcVar{i}, \tcVar{j}, \tcVar{A}) \nl %
            & \qquad [n, p] = size(\tcVar{A}) \nl %
            & \qquad \tcVar{B} = \tcVar{A} \nl %
            & \qquad \tcFor{for} k = 1:p \nl %
            & \qquad \qquad \tcVar{B}(\tcVar{i}, k) =
            \tcVar{A}(\tcVar{j}, k) \nl %
            & \qquad \qquad \tcVar{B}(\tcVar{j}, k) =
            \tcVar{A}(\tcVar{i}, k) \nl %
            & \qquad \tcFor{end} \nl %
            & \tcFun{endfunction}
          \end{scilab}
        \end{noliste}
      \end{remarkL}~\\[-1.5cm]
    \end{proof}

  \item Expliquer pourquoi la fonction {\tt multligmat} suivante
    retourne le même résultat {\tt B} que la fonction {\tt multlig}.
    \begin{scilab}
      & \tcFun{function} \tcVar{B} =
      \underline{multiligmat}(\tcVar{a}, \tcVar{i}, \tcVar{A}) \nl %
      & \qquad [n, p] = size(\tcVar{A}) \nl %
      & \qquad D = eye(n, n) \nl %
      & \qquad D(\tcVar{i}, \tcVar{i}) = a \nl %
      & \qquad \tcVar{B} = D \Sfois{} \tcVar{A} \nl %
      & \tcFun{endfunction}
    \end{scilab}    


    \newpage


    \begin{proof}~\\%
      Nommons $A$, $B$, $i$, $a$, $n$ et $p$ les éléments codés par
      les variables du programme correspondantes.
      \begin{noliste}{$\sbullet$}
      \item L'instruction en ligne \ligne{3} : {\tt D = eye(n, n)}
        permet de créer la matrice identité $I_{n}$.\\
        {\it (le nom {\tt eye} provient d'un jeu sur les sonorités :
          on crée à l'aide de cette instruction la matrice identité
          qui en anglais se dit \og identity matrix \fg{} qu'il faut
          lire {\tt eye}-dentity matrix)}

      \item L'instruction en ligne \ligne{4} : {\tt D(i, i) = a}
        permet de remplacer le coefficient $D_{i,i}$ par la valeur
        $a$. On crée ainsi une matrice $D$ diagonale carrée d'ordre
        $n$ dont :
        \begin{noliste}{$\stimes$}
        \item le $\eme{i}$ coefficient diagonal est la valeur $a$,
        \item les autres coefficients diagonaux ont tous la même
          valeur $1$.
        \end{noliste}

      \item L'instruction en ligne \ligne{5} : {\tt B = D \Sfois{} A}
        permet de stocker, dans la variable {\tt B}, le résultat de la
        multiplication $D \times A$. Détaillons ce calcul.\\[.2cm]
        Soit $(i', j) \in \llb 1, n \rrb \times \llb 1, p \rrb$. Par
        la formule de multiplication matricielle, on a :
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          B_{i', j} & = & \Sum{k = 1}{n} D_{i', k} \times A_{k, j}
          %\\[.2cm]
          \ = \ D_{i', i'} \times A_{i', j}
          & (car $D_{i', k} = 0$ si $k \neq i'$)
        \end{array}
        \]
        Deux cas se présentent alors :
        \begin{noliste}{$\stimes$}
        \item \dashuline{si $i' = i$} alors $D_{i', i'} = D_{i, i} =
          \text{a}$ et ainsi : $B_{i, j} = a \times A_{i, j}$.%
          \conc{La $\eme{i}$ ligne de $B$ est obtenue en multipliant
            la $\eme{i}$ ligne de $A$ par $a$.}
        \item \dashuline{si $i' \neq i$} alors $D_{i', i'} = 1$ et
          ainsi : $B_{i', j} = 1 \times A_{i', j} = A_{i', j}$.%
          \conc{Les autres lignes de $B$ sont des copies des lignes
            correspondantes de la matrice $A$.}
        \end{noliste}
        \concL{On en conclut que la fonction {\tt multiligmat} permet
          de calculer la matrice obtenue en appliquant à $A$
          l'opération élémentaire $L_i \leftarrow a \, L_i$. Cela
          correspond bien au calcul effectué par la fonction {\tt
            multilig}.}{12.4}
      \end{noliste}
      \begin{remark}%
        \begin{noliste}{$\sbullet$}
        \item La matrice $D \in \M{n}$ décrite dans cette question est
          une matrice {\bf de dilatation}. L'opération élémentaire
          $L_i \leftarrow a \, L_i$ (resp. $C_i \leftarrow a \, C_i$)
          se traduit par la multiplication matricielle à gauche
          (resp. à droite) de la matrice initiale $A$ par la matrice
          $D$.

        \item Illustrons de point par un exemple simple. Considérons
          $D =
          \begin{smatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 3
          \end{smatrix}
          $. Alors :
          \[
          \begin{array}{C{1cm}rcl}
            & D \, M & = & 
            \begin{smatrix}
              1 & 0 & 0 \\
              0 & 1 & 0 \\
              0 & 0 & 5
            \end{smatrix}
            \begin{smatrix}
              0 & 0 & 0 \\
              1 & 2 & -1 \\
              2 & 4 & -2
            \end{smatrix}
            \ = \ 
            \begin{smatrix}
              0 & 0 & 0 \\
              1 & 2 & -1 \\
              10 & 20 & -10
            \end{smatrix}
            \\[.6cm]
            & & & \text{\it (on multiplie la $\eme{3}$ ligne par $5$)}
            \\[.2cm]
            et & M \, D & = & 
            \begin{smatrix}
              0 & 0 & 0 \\
              1 & 2 & -1 \\
              2 & 4 & -2
            \end{smatrix}
            \begin{smatrix}
              1 & 0 & 0 \\
              0 & 1 & 0 \\
              0 & 0 & 5
            \end{smatrix}
            \ = \ 
            \begin{smatrix}
              0 & 0 & 0 \\
              1 & 2 & -5 \\
              2 & 4 & -10
            \end{smatrix}
            \\[.6cm]
            & & & \text{\it (on multiplie la $\eme{3}$ colonne par $5$)}
          \end{array}
          \]          
        \end{noliste}
      \end{remark}~\\[-1.4cm]
    \end{proof}
  \end{noliste}


  \newpage


\item Dans cette question, on note $n$ un entier supérieur ou égal à
  $2$ et $M$ une matrice de $\M{n}$ de rang $1$. Pour tout couple $(i,
  j) \in \llb 1, n \rrb^2$, on note $E_{i, j}$ la matrice de $\M{n}$
  dont tous les coefficients sont nuls sauf celui situé à
  l'intersection de sa $\eme{i}$ ligne et de sa $\eme{j}$ colonne, et
  qui vaut $1$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm} %
  \item
    \begin{nonoliste}{(i)}
      \setlength{\itemsep}{2mm} %
    \item Justifier l'existence d'une matrice colonne non nulle $C =
      \begin{smatrix}
        c_1 \\
        \vdots \\
        c_n
      \end{smatrix}
      \in \M{n, 1}$ et d'une matrice ligne non nulle $
      L_1 = 
      \begin{smatrix}
        \ell_1 & \ldots & \ell_n
      \end{smatrix}
      \in \M{1, n}
      $ telles que $M = C \, L$.

      \begin{proof}~%
        \begin{noliste}{$\sbullet$}
        \item Remarquons tout d'abord :
          \[
          \begin{array}{rcl@{\qquad}>{\it}R{4cm}}
            C \, L & = &
            \begin{smatrix}
              c_{1} \, \ell_1 & c_{1} \, \ell_2 & \ldots & c_{1} \,
              \ell_{n-1} & c_{1} \ell_n
              \\
              c_{2} \, \ell_1 & c_{2} \, \ell_2 & \ldots & c_{2} \,
              \ell_{n-1} & c_{2} \ell_n
              \\
              \vdots & \vdots & & \vdots & \vdots
              \\
              c_{n-1} \, \ell_1 & c_{n-1} \, \ell_2 & \ldots & c_{n-1}
              \, \ell_{n-1} & c_{n-1} \, \ell_n
              \\
              c_{n} \, \ell_1 & c_{n} \, \ell_2 & \ldots & c_{n} \,
              \ell_{n-1} & c_{n} \ell_n
            \end{smatrix}
            \\[1.2cm]
            & = & 
            \left( 
              \ell_1 \, 
              \begin{smatrix}
                c_{1} \\
                c_2 \\
                \vdots \\
                c_{n-1} \\
                c_{n}
              \end{smatrix}
              \quad
              \ell_2 \, 
              \begin{smatrix}
                c_{1} \\
                c_2 \\
                \vdots \\
                c_{n-1} \\
                c_{n}
              \end{smatrix}
              \quad \ldots \quad
              \ell_{n-1} \, 
              \begin{smatrix}
                c_{1} \\
                c_2 \\
                \vdots \\
                c_{n-1} \\
                c_{n}
              \end{smatrix}
              \quad
              \ell_{n} \, 
              \begin{smatrix}
                c_{1} \\
                c_2 \\
                \vdots \\
                c_{n-1} \\
                c_{n}
              \end{smatrix}
            \right)  
            \ = \ 
            \begin{smatrix}
              \ell_1 \, C & \ldots & \ell_n \, C
            \end{smatrix}
          \end{array}      
          \]
          Il s'agit donc de démontrer que toute matrice de rang $1$
          apparaît comme concaténation de colonnes colinéaires à une
          matrice colonne $C$ non nulle.\\
          Ce résultat se montre en deux étapes.

        \item Tout d'abord, comme la matrice $M$ est de rang $1$ elle
          est forcément non nulle (si c'était le cas, elle serait de
          rang $0$). Ainsi, $M$ admet (au moins) une colonne non
          nulle. On note $C$ la première colonne non nulle de $M$.

        \item Démontrons maintenant que toutes les colonnes de $M$
          sont colinéaires à $C$.\\
          On procède par l'absurde.\\[.1cm]
          Supposons que le matrice $M$ possède une colonne non
          colinéaire à $C$.\\
          Notons $k \in \llb 1, n \rrb$ l'indice de cette
          colonne. Alors :
          \[
          \begin{array}{rcl@{\qquad}>{\it}R{4cm}}
            \rg( M ) & = & \rg\big( C_1(M), \ldots, C_n(M) \big)
            \\[.2cm]
            & \geq & \rg\big( C_{k}(M), C \big)
            & (car $C$ et $C_k(M)$ sont des colonnes de $M$)
          \end{array}
          \]
          La famille $\big( C_{k}(M), C \big)$ est libre car
          constituée de deux vecteurs non colinéaires.\\
          On en déduit :
          \[
          \rg( M ) \geq \rg\big( C_{k}(M), C \big) = 2
          \]
          Absurde ! %
          \conc{Ainsi, toute colonne de $M$ est colinéaire à $C$} %

        \item Il existe donc un $n$-uplet $(\ell_1, \ldots, \ell_n)
          \in \R^n$ tel que :
          \[
          M \ = \ 
          \begin{smatrix}
            \ell_1 \, C & \ldots & \ell_n \, C
          \end{smatrix}
          \]
          Notons que ce $n$-uplet est forcément différent du $n$-uplet
          $(0, \ldots, 0)$ (si c'était le cas, la matrice $M$ serait
          nulle).
        \end{noliste}~\\[-1cm]
        \concL{Ainsi, si $M \in \M{n}$ est de rang $1$, il existe un
          matrice colonne $C \in \M{n, 1}$ et une matrice ligne non
          nulle $L \in \M{1, n}$ telles que : $M = C \,
          L$.}{14.4}~\\[-1cm]
        % \begin{remarkL}{.98}
        %   Les énoncés de type {\tt HEC} / {\tt ESSEC} se distinguent
        %   des énoncés {\tt EML} / {\tt EDHEC} par un découpage plus
        %   faible des questions qui oblige à prendre plus
        %   d'initiatives. Ici, la formulation de la question \og En
        %   déduire que \ldots \fg{} doit aider à comprendre qu'il
        %   s'agit de se servir du résultat précédent. En question
        %   précédente, on exhibe $(n-1)$ vecteurs de $\im(f)$. Il
        %   s'agit alors de tester si la famille constituée de ces
        %   vecteurs est une base de $\im(f)$.
        % \end{remarkL}~\\[-1.4cm]        
      \end{proof}


      \newpage


    \item Calculer la matrice $M \, C$ et en déduire une valeur propre
      de $M$.

      \begin{proof}~%
        \[
        \begin{array}{rcl@{\qquad}>{\it}R{5cm}}
          M \times C & = & (C \, L) \times C
          \\[.4cm]
          & = & C \times (L \, C)
          & (par associativité)
          \nl
          \nl[-.2cm]
          & = & (LC) \cdot C 
          & (car comme $L \in \M{1, 3}$ et $C \in \M{3, 1}$, $LC$ est
          un réel)        
          \nl
          & = & \left( \Sum{i=1}{n} \ell_i \, c_i \right) \cdot C
        \end{array}
        \]
        \concL{Comme $C \neq 0_{\M{n,1}}$, le vecteur $C$ est un
          vecteur propre de $M$ associé à la valeur propre
          $\Sum{i=1}{n} \ell_i \, c_i$.}{12.4}~\\[-1cm]
      \end{proof}

    \item Montrer que si le réel $\Sum{i=1}{n} c_i \, \ell_i$ est
      différent de $0$, alors la matrice $M$ est diagonalisable.

      \begin{proof}~%
        \begin{noliste}{$\sbullet$}
        \item Par définition :
          \[
          \rg(M) = 1 \neq n = \dim\big( \M{n, 1} \big)
          \]
          En effet, il est précisé dans l'énoncé : $n \geq 2$. %
          \conc{On en déduit que $M$ n'est pas inversible. Ainsi, le
            réel $0$ est valeur propre de $M$.}

        \item Notons $\B$ la base canonique de $\R^n$. Considérons
          l'endomorphisme $f \in \LL{\R^n}$ dont la représentation
          dans la base $\B$ est $M$. Par le théorème du rang :
          \[
          \begin{array}{ccccc}
            \dim\big( \R^n \big) & = & \dim\big( \kr(f) \big) & + &
            \dim\big( \im(f) \big)
            \\[.2cm]
            \shortparallel & & \shortparallel & & \shortparallel
            \\[.2cm]
            n & & \dim\big(  E_0(f) \big) & & \rg(f)
            \\[.2cm]
            \shortparallel & & \shortparallel & & \shortparallel
            \\[.2cm]
            \dim\big( \M{n,1} \big) & = & \dim\big( E_0(M) \big) & + & 
            \rg(M)
          \end{array}
          \]
          \conc{Ainsi : $\dim\big( E_0(M) \big) = \dim\big( \M{n,1}
            \big) - \rg(M) = n - 1$.} %

        \item Notons $\alpha = \Sum{i=1}{n} c_i \, l_i$. On suppose
          $\alpha$ non nul.\\
          D'après la question précédente, $\alpha$ est valeur propre
          de $M$. On en déduit : $\dim\big( E_{\alpha}(M) \big) \geq
          1$. Ainsi :
          \[
          \dim\big( E_{0}(M) \big) + \dim\big( E_{\alpha}(M) \big)
          \geq (n-1) + 1 \geq n
          \]
          Et comme on a forcément : $\dim\big( E_{0}(M) \big) +
          \dim\big( E_{\alpha}(M) \big) \leq n = \dim\big( \M{n,1}
          \big)$, on en conclut :
          \[
          \dim\big( E_{0}(M) \big) + \dim\big( E_{\alpha}(M) \big) = n
          \]
        \end{noliste}
        \conc{Ainsi, si $\alpha \neq 0$, la matrice $M$ possède deux
          valeurs propres distinctes $0$ et $\alpha$.\\
          Comme : $\dim\big( E_{0}(M) \big) + \dim\big( E_{\alpha}(M)
          \big) = \dim\big( \M{n,1} \big)$, la matrice $M$ est
          diagonalisable.}~\\[-1cm]
      \end{proof}
    \end{nonoliste}


    \newpage


  \item
    \begin{nonoliste}{(i)}
      \setlength{\itemsep}{2mm} %
    \item À l'aide de l'égalité $M = C \, L$, établir l'existence de
      deux matrices inversibles $P$ et $Q$ telles que $P \, M \, Q =
      E_{1, 1}$.

      \begin{proof}~%
        \begin{noliste}{$\sbullet$}
        \item D'après un calcul analogue à celui fait en
          \itbf{3.a)(i)} :$
          \begin{smatrix}
            1 \\
            0 \\
            \vdots \\
            0
          \end{smatrix}
          \begin{smatrix}
            1 & 0 & \ldots & 0 
          \end{smatrix}
          \ = \ E_{1, 1}
          $.

        \item Pour résoudre la question, il suffit donc de trouver
          deux matrices inversibles $P$ et $Q$ de $\M{n}$ telles que :
          \[
          P \, C \ = \           
          \begin{smatrix}
            1 \\
            0 \\
            \vdots \\
            0
          \end{smatrix}
          \qquad \text{ et } \qquad
          L \, Q \ = \           
          \begin{smatrix}
            1 & 0 & \ldots & 0 
          \end{smatrix}
          \]
          En effet, si c'est le cas, on a :
          \[
          \begin{array}{R{2cm}rcl@{\qquad}>{\it}R{5cm}}
            & P \, M \, Q & = & P \, (CL) \, Q
            \\[.2cm]
            & & = & (PC) \, (LQ)
            & (par associativité)
            \nl
            \nl[-.2cm]
            & & = & 
            \begin{smatrix}
              1 \\
              0 \\
              \vdots \\
              0
            \end{smatrix}
            \begin{smatrix}
              1 & 0 & \ldots & 0 
            \end{smatrix}
            \ = \ E_{1, 1}
          \end{array}
          \]

        \item Il reste à démontrer l'existence des matrices $P$ et
          $Q$. Remarquons dans un premier temps :
          \[
          \begin{array}{ccccR{8cm}}
            PC =         
            \begin{smatrix}
              1 \\
              0 \\
              \vdots \\
              0
            \end{smatrix}
            & \Leftrightarrow & 
            P^{-1}           
            \begin{smatrix}
              1 \\
              0 \\
              \vdots \\
              0
            \end{smatrix}
            = C
            & \Leftrightarrow &
            La première colonne de $P^{-1}$ est le vecteur $C$
          \end{array}
          \]
          Il s'agit donc de construire une matrice {\bf inversible}
          dont la première colonne est $C$. \\
          La première colonne étant fixée, il reste à choisir les
          suivantes.

        \item Construisons une telle matrice. Deux cas se présentent :
        \end{noliste}
          \begin{liste}{$\stimes$}
          \item \dashuline{si $c_1 \neq 0$}, on pose :
            \[
            P^{-1} \ = \
            \begin{smatrix}
              c_1 & 0 & \ldots & \ldots & 0 \\
              c_2 & 1 & \ddots & & \vdots \\
              \vdots & 0 & \ddots & \ddots & \vdots \\
              \vdots & \vdots & \ddots & \ddots & 0 \\
              c_2 & 0 & \ldots & 0 & 1
            \end{smatrix}
            \]
            Cette matrice est bien inversible car elle est
            trinagulaire inférieure et de coefficients diagonaux tous
            non nuls.
            
          \item \dashuline{si $c_1 = 0$}, on note $i_0$ l'indice du
            premier coefficient non nul du vecteur $C$. On pose alors
            :
            \[
            P^{-1} \ = \
            \begin{smatrix}
              0 & 0 & \ldots & 0 & 1 & 0 & \ldots & 0 \\
              \vdots & 1 & \ddots & \vdots & 0 & \vdots & & \vdots \\
              \vdots & 0 & \ddots & 0 & \vdots & \vdots & & \vdots \\
              0 & \vdots & \ddots & 1 & \vdots & \vdots & & \vdots \\ 
              c_{i_0} & \vdots & & \ddots & 0 & 0 & & \vdots \\
              \vdots & \vdots & & & \ddots & 1 & \ddots & \vdots \\
              \vdots & \vdots & & & & \ddots & \ddots & 0 \\
              c_n & 0 & \ldots & \ldots & \ldots & \ldots & 0 & 1 
            \end{smatrix}
            \]
            Démontrons que cette matrice est inversible :
            \[
            \rg\left( P^{-1} \right)
            \ 
            \begin{arrayEg}
              C_1 \leftrightarrow C_{i_0}
            \end{arrayEg}
            \ 
            \rg\left( 
              \begin{smatrix}
                1 & 0 & \ldots & \ldots & \ldots & \ldots & & 0 \\
                0 & \ddots & \ddots & & & & & \vdots \\
                \vdots & \ddots & 1 & \ddots & & & & \vdots \\
                \vdots & & 0 & c_{i_0} & \ddots & & & \vdots \\
                \vdots & & \vdots & \vdots & 1 & \ddots & & \vdots \\
                \vdots & & \vdots & \vdots & 0 & \ddots & \ddots & \vdots \\
                \vdots & & \vdots & \vdots & \vdots & \ddots & \ddots & 0 \\
                0 & \ldots & 0 & c_n & 0 & \ldots & 0 & 1 
              \end{smatrix}
            \right)
            \]
            La réduite obtenue est triangulaire inférieure et à
            coefficients diagonaux tous non nuls.\\
            Elle est donc inversible et il en est de même de la
            matrice initiale $P^{-1}$.
          \end{liste}
          \begin{noliste}{$\sbullet$}
          \item Il reste alors à construire $Q$. Remarquons : 
          \[
          \begin{array}{ccccccR{3.8cm}}
            LQ =         
            \begin{smatrix}
              1 & 0 & \ldots & 0 
            \end{smatrix}
            & \Leftrightarrow & 
            {}^tQ \ {}^t L = 
            \begin{smatrix}
              1 \\
              0 \\
              \vdots \\
              0
            \end{smatrix}
            & \Leftrightarrow &
            \big({}^tQ \big)^{-1}
            \begin{smatrix}
              1 \\
              0 \\
              \vdots \\
              0
            \end{smatrix}
            = {}^tL
            & \Leftrightarrow &
            La première colonne de $\big({}^tQ \big)^{-1} = {}^t\big(Q
            \big)^{-1}$ est le vecteur ${}^tL$ 
          \end{array}
          \]       
          On construit alors ${}^t\big(Q \big)^{-1}$ par la méthode
          ayant permis la construction de $P^{-1}$. 
        \end{noliste}
        \conc{On a bien démontré l'existence de deux matrices
          inversibles $P$ et $Q$ telles que $P \, M \, Q =
          E_{1,1}$.}~\\[-1cm] 
      \end{proof}
      \begin{remarkL}{1.05}%
        Pour construire $P^{-1}$, on peut aussi faire appel au
        théorème dit de la base incomplète qui stipule :
        \[
        \Boxed{
          \begin{array}{C{12cm}}
            Toute famille libre d'un espace vectoriel $E$ de dimension
            finie $n$ peut être complétée en une base de $E$ 
          \end{array}
        }
        \]
        Ici, comme le vecteur $C$ est non nul, la famille $(C)$ est
        une famille libre de $\M{n, 1}$. \\
        On peut donc compléter cette famille en une base de $\M{n,
          1}$. Autrement dit, il existe des vecteurs colonnes $C_2$,
        \ldots $C_n$ tels que la famille $\big( C, C_2, \ldots, C_n
        \big)$ est une base de $\M{n,1}$.\\
        On peut alors construire la matrice $P^{-1}$ par concaténation
        de ces vecteurs :
        \[
        P^{-1} \ = \
        \begin{smatrix}
          C & C_2 & \ldots & C_n
        \end{smatrix}
        \]
        Cette matrice est bien inversible. En effet :
        \[
        \rg\left(
          \begin{smatrix}
            C & C_2 & \ldots & C_n
          \end{smatrix}
        \right) \ = \ \rg \big( C, C_2, \ldots, C_n \big) \ = \ n
        \quad \text{\it (car $\big( C, C_2, \ldots, C_n \big)$ est
          une famille libre)}
        \]
      \end{remarkL}


      \newpage


    \item En déduire que pour tout couple $(i, j) \in \llb 1,
      n\rrb^2$, il existe deux matrices inversibles $P_i$ et $Q_j$
      telles que $P_i \, M \, Q_j = E_{i, j}$.

      \begin{proof}~\\%
        Soit $(i, j) \in \llb 1, n\rrb^2$. On raisonne comme
        précédemment.
        \begin{noliste}{$\sbullet$}
        \item Remarquons tout d'abord que $E_{i,j}$ peut s'obtenir
          comme produit :
        \end{noliste}
        \begin{liste}{$\stimes$}
        \item de la matrice colonne contenant uniquement des $0$
          sauf en ligne $i$ où il contient un $1$.
        \item de la matrice ligne contenant uniquement des $0$ sauf
          en colonne $j$ où il contient un $1$.
        \end{liste}
        \begin{noliste}{$\sbullet$}
        \item Pour résoudre la question, il suffit donc de trouver
          deux matrices inversibles $P$ et $Q$ de $\M{n}$ telles que :
          \[
          P_i \, C \ = \
          \begin{smatrix}
            0 \\
            \vdots \\
            0 \\
            1 \\
            0 \\
            \vdots \\
            0
          \end{smatrix}
          \qquad \text{ et } \qquad L \, Q_j \ = \
          \begin{smatrix}
            0 & \ldots & 0 & 1 & 0 & \ldots & 0
          \end{smatrix}
          \]
        
        \item Pour un raisonnement analogue à celui de la question
          précédente, il suffit alors de trouver :
        \end{noliste}
        \begin{liste}{$\stimes$}
        \item une matrice $P_i^{-1}$ inversible dont la $\eme{i}$
          colonne est $C$.

        \item une matrice ${}^t \big( Q_j \big)^{-1}$ dont la $\eme{j}$
          colonne est ${}^tL$.         
        \end{liste}
        On obtient ces deux matrices par une construction similaire à
        la précédente. %
        \concL{Ainsi, pour tout couple $(i, j) \in \llb 1, n\rrb^2$, il
          existe deux matrices inversibles $P_i$ et $Q_j$ telles que
          $P_i \, M \, Q_j = E_{i, j}$.}{15.4}~\\[-1.2cm]
      \end{proof}
    \end{nonoliste}
    
  \end{noliste}
\end{noliste}


\newpage


\section*{Problème}

\noindent %
{\it Dans ce problème, on définit et on étudie les fonctions
  génératrices des cumulants de variables aléatoires discrètes ou à
  densité.\\
  Les cumulants d'ordre $3$ et $4$ permettent de définir des
  paramètres d'asymétrie et d'aplatissement qui viennent compléter la
  description usuelle d'une loi de probabilité par son espérance
  (paramètre de position) et sa variance (paramètre de dispersion) ;
  ces cumulants sont notamment utilisés pour l'évaluation des risques
  financiers.}\\[.2cm]
{\bf Dans tout le problème} :
\begin{noliste}{$\sbullet$}
\item on note $(\Omega, \A, \Prob)$ un espace probabilisé et toutes
  les variables aléatoires introduites dans l'énoncé sont des
  variables aléatoires réelles définies sur $(\Omega, \A)$ ;
  
\item sous réserve d'existence, l'espérance et la variance d'une
  variable aléatoire $X$ sont respectivement notées $\E(X)$ et $\V(X)$
  ;
  
\item pour tout variable aléatoire $X$ et pour tout réel $t$ pour
  lesquels la variable aléatoire $\ee^{t \, X}$ admet une espérance,
  on pose :
  \[
    M_X(t) = \E\left(\ee^{t \, X}\right) \quad \text{et} \quad K_X(t)
    = \ln\big(M_X(t)\big) ;
  \]
  (les fonctions $M_X$ et $K_X$ sont respectivement appelées la {\it
    fonction génératrice des moments} et la {\it fonction génératrice
    des cumulants} de $X$)
  
\item lorsque, pour un entier $p \in \N^*$, la fonction $K_X$ est de
  classe $\Cont{p}$ sur un intervalle ouvert contenant l'origine, on
  appelle {\it cumulant d'ordre $p$ de $X$}, noté $Q_p(X)$, la valeur
  de la dérivée $\eme{p}$ de $K_X$ en $0$ :
  \[
    Q_p(X) \ = \ K_X^{(p)}(0).
  \]
\end{noliste}

\begin{remark}
  \begin{noliste}{$\sbullet$}
  \item La fonction génératrice des moments d'une \var $X$ est un
    objet classique en probabilités. Comme son nom l'indique, cette
    fonction permet de retrouver les moments de $X$ (sous réserve
    d'existence). Plus précisément, si la \var $X$ admet un moment
    d'ordre $n$, alors :
    \[
      \E(X^n) \ = \ M_X^{(n)}(0)
    \]
    
  \item On ne confondra pas cette fonction avec une autre fonction
    classique en probabilités : la fonction génératrice (des
    probabilités). Cette dernière n'est définie que pour des \var $X$
    à valeurs entières et positives par la formule :
    \[
      \forall s \in [0,1], \quad G_X(s) \ = \ \E(s^X) \ = \
      \Sum{k=0}{+\infty} s^k \, \Prob(\Ev{X = k})
    \]
    Cette fonction caractérise quant à elle, non pas les moments de
    $X$, mais sa loi.\\
    Plus précisément :
    \[
      \forall n \in \N, \quad \Prob(\Ev{X = n}) \ = \
      \dfrac{G_X^{(n)}(0)}{n!}
    \]
    
  \item L'énoncé propose de démontrer quelques propriétés de la
    fonction génératrice des moments pour des \var particulières. Par
    exemple, dans le cas d'une \var $X$ à valeurs dans $\llb -n, n \rrb$ :
    \begin{noliste}{$\stimes$}
    \item $\forall p \in \N^*$, $M_X^{(p)}(0) = \E(X^p)$,
      
    \item $\forall t \in \R$, $G_X(\ee^t) = \ee^{nt} \, M_X(t)$.\\
      {\it (lien entre la fonction génératrice et la fonction génératrice
      des moments)}
    \end{noliste}
  \end{noliste}
\end{remark}


\newpage


\subsection*{Partie I. Fonction génératrice des moments de variables
  aléatoires discrètes}

\noindent
{\it Dans toute cette partie} :
\begin{noliste}{$\sbullet$}
\item on note $n$ un entier supérieur ou égal à $2$ ;
  
\item toutes les variables aléatoires considérées sont discrètes à
  valeurs entières ;
  
\item on note $S$ une variable aléatoire à valeurs dans $\{-1,1\}$
  dont la loi est donnée par :
  \[
    \Prob(\Ev{S = -1}) \ = \ \Prob(\Ev{S = +1}) \ = \ \dfrac{1}{2}.
  \]
\end{noliste}


% \newpage


\begin{noliste}{1.}
  \setlength{\itemsep}{4mm}
\item Soit $X$ une variable aléatoire à valeurs dans $\llb -n, n\rrb$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Pour tout $t\in \R$, écrire $M_X(t)$ sous la forme d'une somme
    et en déduire que la fonction $M_X$ est de classe $\Cont{\infty}$
    sur $\R$.
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Soit $t \in \R$.\\
        La \var $X$ est une \var finie. Ainsi, la \var $\ee^{t \,
          X}$ est également une \var finie. Elle admet donc des
        moments à tout ordre, en particulier une espérance.
        \conc{On en déduit que, pour tout $t \in \R$, le réel
          $M_X(t)$ est bien défini.}
        \begin{remark}
          On rappelle que, d'après l'énoncé : $X(\Omega) \subset \llb
          -n, n \rrb = \{k \ | \ k \in \llb -n, n \rrb\}$.\\
          On en déduit, pour tout $t \in \R$ :
          \[
            \left(\ee^{t \, X}\right)(\Omega) \ = \ \ee^{t \,
              X(\Omega)} \ = \ \{\ee^{t \, k} \ | \ k \in \llb -n, n
            \rrb \}
          \]
          On retrouve bien que la \var $\ee^{t \, X}$ est finie.
        \end{remark}
        
      \item Soit $t\in \R$. Comme $X$ est une \var discrète, par
        théorème de transfert :
        \[
          M_X(t) \ = \ \E\left(\ee^{t \, X}\right) \ = \ \Sum{k=-n}{n}
          \ee^{t \, k} \, \Prob(\Ev{X = k})
        \]
        \conc{$\forall t \in \R$, $M_X(t) = \Sum{k=-n}{n} \ee^{t \, k}
          \, \Prob(\Ev{X = k})$}
        
      \item La fonction $M_X$ est de classe $\Cont{\infty}$ sur $\R$
        en tant que somme, pour tout $k \in \llb -n,n \rrb$ des
        fonctions $t \mapsto \ee^{t \, k} \, \Prob(\Ev{X=k})$ de
        classe $\Cont{\infty}$ sur $\R$.
        \conc{La fonction $M_X$ est de classe $\Cont{\infty}$ sur
          $\R$.}
      \end{noliste}
      \begin{remark}
        Soit $k \in \llb -n, n \rrb$.\\
        Il faut noter que l'expression $\Prob(\Ev{X = k})$ est une
        constante par rapport à $t$. Ainsi, pour étudier la régularité de
        la fonction $t \mapsto \ee^{t \, k} \, \Prob(\Ev{X = k})$
        on peut se contenter d'étudier la régularité de la fonction $t \mapsto
        \ee^{t \, k}$ (qui, elle, est trivialement de classe
        $\Cont{\infty}$ sur $\R$).
      \end{remark}~\\[-1.4cm]
    \end{proof}


    \newpage
    
    
  \item Justifier pour tout $p \in \N^*$, l'égalité : $M_X^{(p)}(0) =
    \E(X^p)$.
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item D'après la question précédente, la fonction $M_X$ est de
        classe $\Cont{\infty}$, donc, pour tout $p \in \N^*$, la
        fonction $M_X^{(p)}$ existe.
        
      \item Commençons par déterminer, pour tout $p \in \N^*$,
        l'expression de $M_X^{(p)}$.\\
        Démontrons par récurrence : $\forall p \in \N^*$, $\PP{p}$\\ 
        où \quad $\PP{p}$ : $\forall t \in \R$, $M_X^{(p)}(t) =
        \Sum{k=-n}{n} k^p \, \ee^{k \, t} \, \Prob(\Ev{X = k})$.
        \begin{noliste}{\fitem}
        \item {\bf Initialisation} :\\
          D'après la question précédente :
          \[
            M_X : t \mapsto \Sum{k=-n}{n} \ee^{k \, t} \, \Prob(\Ev{X
              = k})
          \]
          Ainsi, pour tout $t \in \R$.
          \[
            M_X^{(1)}(t) \ = \ M_X'(t) \ = \ \Sum{k=-n}{n} k \, \ee^{k
              \, t} \, \Prob(\Ev{X = k})
          \]
          D'où $\PP{1}$.
          
        \item {\bf Hérédité} : soit $p \in \N^* $.\\
          Supposons $\PP{p}$ et démontrons $\PP{p+1}$ (\ie $\forall t
          \in \R$, $M_X^{(p+1)}(t) = \Sum{k=-n}{n} k^{p+1} \, \ee^{k
            \, t} \, \Prob(\Ev{X = k})$).\\
          Par hypothèse de récurrence :
          \[
            M_X^{(p)} : t \mapsto \Sum{k=-n}{n} k^p \, \ee^{k \, t} \,
            \Prob(\Ev{X = k})
          \]
          Ainsi, pour tout $t \in \R$ :
          \[
            M_X^{(p+1)}(t) \ = \ \left(M_X^{(p)}\right)'(t) \ = \
            \Sum{k=-n}{n} k^p \, \left(k \, \ee^{k \, t}\right) \,
            \Prob(\Ev{X = k}) \ = \ \Sum{k=-n}{n} k^{p+1} \, \ee^{k \,
              t} \, \Prob(\Ev{X = k})
          \]
          D'où $\PP{p+1}$.
        \end{noliste}
        Par principe de récurrence : $\forall t \in \R$, $M_X^{(p)}(t)
        = \Sum{k=-n}{n} k^p \, \ee^{k \, t} \, \Prob(\Ev{X = k})$.
        
      \item Soit $p \in \N^*$. On en déduit :
        \[
          M_X^{(p)}(0) \ = \ \Sum{k=-n}{n} k^p \, \ee^{k \times 0} \,
          \Prob(\Ev{X = k}) \ = \ \Sum{k=-n}{n} k^p \, \Prob(\Ev{X =
            k}) \ = \ \E(X^p)
        \]
        où la dernière égalité est obtenue par théorème de transfert.
        \conc{Finalement, pour tout $p \in \N^*$ : $M_X^{(p)}(0) =
          \E(X^p)$.}
      \end{noliste}


      \newpage

      
      \begin{remark}
        L'obtention de l'expression de $M_X^{(p)}$ s'effectue au
        brouillon :
        \begin{noliste}{\scriptsize 1)}
        \item on commence par déterminer les premières dérivées
          successives de la fonction $M_X$ :
          \[
            \begin{array}{lccl}
              \forall t \in \R, &  M_X'(t) & = & \Sum{k=-n}{n} k \,
              \ee^{k \, t} \, \Prob(\Ev{X = k})
              \\[.4cm]
              \forall t \in \R, &  M_X''(t) & = & \Sum{k=-n}{n} k \left(
              k \, \ee^{k \, t}\right) \, \Prob(\Ev{X = k}) \ = \
              \Sum{k=-n}{n} k^2 \ee^{k \, t} \, \Prob(\Ev{X = k})
              \\[.4cm]
              \forall t \in \R, &  M_X^{(3)}(t) & = & \Sum{k=-n}{n} k^2
              \left(k \, \ee^{k \, t}\right) \, \Prob(\Ev{X = k}) \ = \
              \Sum{k=-n}{n} k^3 \ee^{k \, t} \, \Prob(\Ev{X = k})
            \end{array}
          \]
          
        \item on en déduit une formule générale :
          \[
            \forall t \in \R, \quad
            M_X^{(p)}(t) \ = \ \Sum{k=-n}{n} k^p \, \ee^{k \, t} \,
            \Prob(\Ev{X = k})
          \]
        \end{noliste}
      \end{remark}~\\[-1.4cm]
    \end{proof}
    
  \item Soit $Y$ une variable aléatoire à valeurs dans $\llb -n,n
    \rrb$ dont la fonction génératrice des moments $M_Y$ est la même
    que celle de $X$.\\
    On note $G_X$ et $G_Y$ les deux polynômes définis par :
    \[
      \forall x \in \R, \ \left\{
        \begin{array}{l}
          G_X(x) \ = \ \Sum{k=0}{2n} \Prob(\Ev{X = k-n}) \, x^k
          \\[.4cm]
          G_Y(x) \ = \ \Sum{k=0}{2n} \Prob(\Ev{Y = k-n}) \, x^k
        \end{array}
      \right.
    \]
    \begin{nonoliste}{(i)}
      \setlength{\itemsep}{2mm}
    \item Vérifier pour tout $t\in \R$, l'égalité : $G_X(\ee^t) =
      \ee^{nt} \, M_X(t)$.
      \begin{proof}~\\
        Soit $t \in \R$.
        \[
          \begin{array}{rcl@{\quad}>{\it}R{4cm}}
            G_X\left(\ee^t\right)
            & = & \Sum{k=0}{2n} \Prob(\Ev{X = k-n}) \,
                  \left(\ee^t\right)^k
            \\[.6cm]
            & = & \Sum{k=0}{2n} \Prob(\Ev{X = k-n}) \, \ee^{k \, t}
            \\[.6cm]
            & = & \Sum{k=0-n}{2n-n} \Prob(\Ev{X = k}) \, \ee^{(k+n) \, t}
            & (par décalage d'indice)
            \nl
            \nl[-.2cm]
            & = & \Sum{k=-n}{n} \left(\Prob(\Ev{X = k}) \, \ee^{k \, t} \,
                  \ee^{n \, t}\right)
            \\[.6cm]
            & = & \ee^{n \, t} \ \Sum{k=-n}{n} \Prob(\Ev{X = k}) \,
                  \ee^{k \, t}
            \\[.6cm]
            & = & \ee^{n \, t} \, M_X(t)
          \end{array}
        \]
        \conc{$\forall t \in \R$, $G_X\left(\ee^t\right) = \ee^{nt} \,
          M_X(t)$}~\\[-1cm]
      \end{proof}


      \newpage
      
      
    \item Justifier la relation : $\forall t \in \R$, $G_X(\ee^t) =
      G_Y(\ee^t)$.
      \begin{proof}~\\
        Soit $t \in \R$.
        \[
          \begin{array}{rcl@{\quad}>{\it}R{5cm}}
            G_X\left(\ee^t\right)
            & = & \ee^{n \, t} \, M_X(t)
            & (d'après la question précédente)
            \nl
            \nl[-.2cm]
            & = & \ee^{n \, t} \, M_Y(t)
            & (car, d'après l'énoncé : $M_X = M_Y$)
            \nl
            \nl[-.2cm]
            & = & G_Y\left(\ee^t\right)
            & (avec le même raisonnement qu'en question précédente)
          \end{array}
        \]
        \conc{$\forall t \in \R$, $G_X\left(\ee^t\right) = G_Y\left(
              \ee^t\right)$}~\\[-1cm]
      \end{proof}
      
    \item En déduire que la variable aléatoire $Y$ suit la même loi
      que $X$.
      \begin{proof}~
        \begin{noliste}{$\sbullet$}
        \item Tout d'abord : $X(\Omega) \subset \llb -n,n \rrb$ et
          $Y(\Omega) \subset \llb -n,n \rrb$.
          
        \item Ensuite, d'après la question précédente :
          \[
            \forall t \in \R, \ G_X\left(\ee^t\right) =
            G_Y\left(\ee^t\right)
          \]
          Ainsi, pour tout $x \in \ ]0,+\infty[$ :
          \[
            \begin{array}{ccc}
              G_X\left(\ee^{\ln(x)}\right) & =
              & G_Y\left(\ee^{\ln(x)}\right)
              \\[.2cm]
              \shortparallel & & \shortparallel
              \\[.2cm]
              G_X(x) & & G_Y(x)
            \end{array}
          \]
          
        \item Soit $x \in \ ]0,+\infty[$, on en déduit :
          $(G_X-G_Y)(x) = 0$.\\
          Or :
          \[
            \begin{array}{rcl}
              (G_X-G_Y)(x)
              & = &  G_X(x) - G_Y(x)
              \\[.2cm]
              & = & \Sum{k=0}{2n} \Prob(\Ev{X = k-n}) \,
                    x^k - \Sum{k=0}{2n} \Prob(\Ev{Y = k-n}) \, x^k
              \\[.6cm]
              & = & \Sum{k=0}{2n} \left(\Prob(\Ev{X = k-n}) \, x^k -
                    \Prob(\Ev{Y = k-n}) \, x^k \right)
              \\[.6cm]
              & = & \Sum{k=0}{2n} \big( \Prob(\Ev{X = k-n}) -
                    \Prob(\Ev{Y = k-n})\big) \, x^k
            \end{array}
          \]
          Ainsi, le polynôme $G_X-G_Y$ admet une infinité de
          racines. C'est donc le polynôme nul, \ie :
          \[
            \forall k \in \llb 0,2n \rrb, \ \Prob(\Ev{X = k-n}) -
            \Prob(\Ev{Y = k-n}) =0
          \]
          D'où :
          \[
            \forall k \in \llb -n,n \rrb, \ \Prob(\Ev{X = k}) -
            \Prob(\Ev{Y = k}) =0
          \]
          Ainsi :
          \[
            \forall k \in \llb -n,n \rrb, \ \Prob(\Ev{X = k}) =
            \Prob(\Ev{Y = k})
          \]
          \conc{Finalement, les \var $X$ et $Y$ suivent la même
            loi.}~\\[-1.4cm]
        \end{noliste}
      \end{proof}
    \end{nonoliste}
  \end{noliste}


  \newpage
  
  
\item Dans cette question, on note $X_2$ une variable aléatoire qui
  suit la loi binomiale $\Bin{2}{\dfrac{1}{2}}$.\\
  On suppose que les variables aléatoires $X_2$ et $S$ sont
  indépendantes et on pose $Y_2 = S \, X_2$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item
    \begin{nonoliste}{(i)}
      \setlength{\itemsep}{2mm}
    \item Préciser l'ensemble des valeurs possibles de la variable
      aléatoire $Y_2$.
      \begin{proof}~
        \begin{noliste}{$\sbullet$}
        \item Comme $X_2 \suit \Bin{2}{\dfrac{1}{2}}$, on a : $X_2(\Omega) =
          \llb 0,2 \rrb = \{0,1,2\}$.
          
        \item De plus : $S(\Omega) = \{-1,1\}$.
        \end{noliste}
        \conc{On en déduit : $Y_2(\Omega) = (S \, X_2)(\Omega) \subset
          \{-2,-1,0,1,2\} = \llb -2, 2\rrb$.}~\\[-1cm]
      \end{proof}
      
    \item Calculer les probabilités $\Prob(\Ev{Y_2 = y})$ attachées
      aux diverses valeurs possibles $y$ de $Y_2$.
      \begin{proof}~
        \begin{noliste}{$\sbullet$}
        \item Tout d'abord, on a l'égalité entre événements :
          \[
            \Ev{Y_2 = -2} \ = \ \Ev{S \, X_2 = -2} \ = \ \Ev{S = -1}
            \cap \Ev{X_2 = 2}
          \]
          On en déduit :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{4cm}}
              \Prob(\Ev{Y_2 = -2})
              & = & \Prob(\Ev{S = -1} \cap \Ev{X_2 = 2})
              \\[.2cm]
              & = & \Prob(\Ev{S = -1}) \ \Prob(\Ev{X_2 = 2})
              & (car $S$ et $X_2$ sont indépendantes)
              \nl
              \nl[-.2cm]
              & = & \dfrac{1}{2} \times \dbinom{2}{2}
                    \left(\dfrac{1}{2}\right)^2 \
                    \left(\dfrac{1}{2}\right)^{2-2}
              \\[.6cm]
              & = & \dfrac{1}{2} \times 1 \times \dfrac{1}{4}
            \end{array}
          \]
          \conc{$\Prob(\Ev{Y_2 = -2}) = \dfrac{1}{8}$}
          
        \item Ensuite :
          \[
            \Ev{Y_2 = -1} \ = \ \Ev{S = -1} \cap \Ev{X_2 = 1}
          \]
          On obtient donc :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{4cm}}
              \Prob(\Ev{Y_2 = -1})
              & = & \Prob(\Ev{S = -1} \cap \Ev{X_2 = 1})
              \\[.2cm]
              & = & \Prob(\Ev{S = -1}) \ \Prob(\Ev{X_2 = 1})
              & (car $S$ et $X_2$ sont indépendantes)
              \nl
              \nl[-.2cm]
              & = & \dfrac{1}{2} \times \dbinom{2}{1}
                    \left(\dfrac{1}{2}\right)^1 \
                    \left(\dfrac{1}{2}\right)^{2-1}
              \\[.6cm]
              & = & \dfrac{1}{2} \times 2 \times \dfrac{1}{4}
            \end{array}
          \]
          \conc{$\Prob(\Ev{Y_2 = -1}) = \dfrac{1}{4}$}


          \newpage

          
        \item De même :
          \[
            \Ev{Y_2 = 1} = \Ev{S = 1} \cap \Ev{X_2 = 1} \quad
            \text{et} \quad \Ev{Y_2 = 2} = \Ev{S = 1} \cap \Ev{X_2 =
              2}
          \]
          \conc{Ainsi : $\Prob(\Ev{Y_2 = 1}) = \dfrac{1}{4}$ \quad et
            \quad $\Prob(\Ev{Y_2 = 2}) = \dfrac{1}{8}$.}
          
        \item Enfin : $\Ev{Y_2 = 0} = \Ev{X_2 = 0}$. Ainsi :
          \[
            \Prob(\Ev{Y_2  = 0}) \ = \ \Prob(\Ev{X_2 = 0}) \ = \
            \dbinom{2}{0} \left(\dfrac{1}{2}\right)^0 \,
            \left(\dfrac{1}{2}\right)^2 \ = \ \dfrac{1}{4}
          \]
          \conc{$\Prob(\Ev{Y_2 = 0}) = \dfrac{1}{4}$}
        \end{noliste}
        \begin{remark}
          On pouvait aussi remarquer que la famille $(\Ev{Y_2 = k})_{k
            \in \llb -2, 2\rrb}$ est un système complet
          d'événements. D'où :
          \[
            \begin{array}{rcl}
              \Prob(\Ev{Y_2 = 0})
              & = & 1-\Big(\Prob(\Ev{Y_2 = -2}) +
                    \Prob(\Ev{Y_2 = -1}) + \Prob(\Ev{Y_2 = 1}) +
                    \Prob(\Ev{Y_2  = 2})\Big)
              \\[.4cm]
              & = & 1 - \left(\dfrac{1}{8} + \dfrac{1}{4} +
                    \dfrac{1}{4} + \dfrac{1}{8}\right) \ = \ \dfrac{1}{4}
            \end{array}
          \]
        \end{remark}~\\[-1.4cm]
      \end{proof}
    \end{nonoliste}
    
  \item Vérifier que la variable aléatoire $X_2 - (S+1)$ suit la même
    loi que $Y_2$.
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Tout d'abord, comme $S(\Omega) = \{-1,1\}$, on a :
        $(S+1)(\Omega) = \{0,2\}$.\\
        De plus $X_2(\Omega) = \{0,1,2\}$. D'où, en notant $T_2 = X_2
        - (S+1)$ : $T_2(\Omega) \subset \{-2,-1,0,1,2\}$.
        \conc{$T_2(\Omega) \subset \llb -2,2 \rrb$ (on rappelle :
          $Y_2(\Omega) \subset \llb -2, 2 \rrb$ d'après \itbf{2.a)(i)})}
        
      \item Soit $k \in \llb -2,2 \rrb$.\\
        La famille $(\Ev{S = -1}, \Ev{S = 1})$ forme un système
        complet d'événements.\\
        Ainsi, par formule des probabilités totales :
        \[
          \begin{array}{cl@{\quad}>{\it}R{4cm}}
            & \Prob(\Ev{T_2 = k})
            \\[.4cm]
            = & \Prob(\Ev{T_2 = k} \cap \Ev{S = -1}) + \Prob(\Ev{T_2
                  = k} \cap \Ev{S = 1})
            \\[.4cm]
            = & \Prob(\Ev{X_2 - (S+1) = k} \cap \Ev{S = -1}) +
                  \Prob(\Ev{X_2 - (S+1) = k} \cap \Ev{S = 1})
            \\[.4cm]
            = & \Prob(\Ev{X_2 - (-1+1) = k} \cap \Ev{S = -1}) +
                  \Prob(\Ev{X_2 - (1+1) = k} \cap \Ev{S = 1})
            \\[.4cm]
            = & \Prob(\Ev{X_2 = k} \cap \Ev{S = -1}) +
                  \Prob(\Ev{X_2 = k+2} \cap \Ev{S = 1})
            \\[.4cm]
            = & \Prob(\Ev{X_2 = k}) \ \Prob(\Ev{S = -1}) +
                  \Prob(\Ev{X_2 = k+2}) \ \Prob(\Ev{S = 1})
            & (car $X_2$ et $S$ sont indépendantes)
            \nl
            \nl[-.2cm]
            = & \dfrac{1}{2} \ \Prob(\Ev{X_2 = k}) + \dfrac{1}{2} \
                \Prob(\Ev{X_2 = k+2})
          \end{array}
        \]


        \newpage

        
      \item On obtient ainsi :
        \[
          \begin{array}{rcl}
            \Prob(\Ev{T_2 = -2}) & = & \dfrac{1}{2} \ \Prob(\Ev{X_2 = -2})
            + \dfrac{1}{2} \ \Prob(\Ev{X_2 = 0}) \ = \ \bcancel{\dfrac{1}{2} \
            \Prob(\emptyset)} + \dfrac{1}{2} \times \dfrac{1}{4} \ = \
            \dfrac{1}{8} \ = \ \Prob(\Ev{Y_2 = -2})
            \\[.6cm]
            \Prob(\Ev{T_2 = -1}) & = & \dfrac{1}{2} \ \Prob(\Ev{X_2 = -1})
            + \dfrac{1}{2} \ \Prob(\Ev{X_2 = 1}) \ = \ \bcancel{\dfrac{1}{2} \
            \Prob(\emptyset)} + \dfrac{1}{2} \times \dfrac{1}{2} \ = \
            \dfrac{1}{4} \ = \ \Prob(\Ev{Y_2 = -1})
            \\[.6cm]
            \Prob(\Ev{T_2 = 0}) & = & \dfrac{1}{2} \ \Prob(\Ev{X_2 = 0})
            + \dfrac{1}{2} \ \Prob(\Ev{X_2 = 2}) \ = \ \dfrac{1}{2} \times
            \dfrac{1}{4} + \dfrac{1}{2} \times \dfrac{1}{4} \ = \
            \dfrac{1}{4} \ = \ \Prob(\Ev{Y_2 = 0})
            \\[.6cm]                          
            \Prob(\Ev{T_2 = 1}) & = & \dfrac{1}{2} \ \Prob(\Ev{X_2 = 1})
            + \dfrac{1}{2} \ \Prob(\Ev{X_2 = 3}) \ = \ \dfrac{1}{2} \times
            \dfrac{1}{2} + \bcancel{\dfrac{1}{2} \ \Prob(\emptyset)} \ = \
            \dfrac{1}{4} \ = \ \Prob(\Ev{Y_2 = 1})
            \\[.6cm]
            \Prob(\Ev{T_2 = 2}) & = & \dfrac{1}{2} \ \Prob(\Ev{X_2 = 2})
            + \dfrac{1}{2} \ \Prob(\Ev{X_2 = 4}) \ = \ \dfrac{1}{2} \times
            \dfrac{1}{4} + \bcancel{\dfrac{1}{2} \ \Prob(\emptyset)} \ = \
            \dfrac{1}{8} \ = \ \Prob(\Ev{Y_2 = 2})
          \end{array}
        \]
        \conc{On en déduit que $T_2 = X_2 - (S+1)$ et $Y_2$ suivent la
          même loi.}
      \end{noliste}
      \begin{remark}
        \begin{noliste}{$\sbullet$}
        \item Lors d'une rédaction classique de détermination de la loi
          d'une somme, après obtention de la relation :
          \[
            \Prob(\Ev{T_2 = k}) \ = \ \dfrac{1}{2} \ \Prob(\Ev{X_2 = k})
            + \dfrac{1}{2} \ \Prob(\Ev{X_2 = k+2})
          \]
          On cherche à déterminer si les événements $\Ev{X_2 = k}$ et/ou
          $\Ev{X_2 = k+2}$ sont l'événement impossible.
          
        \item Ici, cela varie suivant les valeurs de $k$. C'est pour
          cela qu'on effectue le calcul direct des probabilités
          $\Prob(\Ev{T_2 = k})$ après l'obtention de la formule précédente.
        \end{noliste}
      \end{remark}~\\[-1.4cm]
    \end{proof}
  \end{noliste}
  
\item Le script \Scilab{} suivant permet d'effectuer des simulations
  de la variable aléatoire $Y_2$ définie dans la question précédente.
  \begin{scilab}
    & n = 10 \nl %
    & X = grand(n,2,\ttq{}bin\ttq{},2,0.5) \nl %
    & B = grand(n,2,\ttq{}bin\ttq{},1,0.5) \nl %
    & S = 2 \Sfois{} B - ones(n,2) \nl %
    & Z1 = [S(1:n,1) .\Sfois{} X(1:n,1) , X(1:n,1) - S(1:n,1) -
    ones(n,1)] \nl %
    & Z2 = [S(1:n,1) .\Sfois{} X(1:n,1) , X(1:n,2) - S(1:n,2) -
    ones(n,1)]
  \end{scilab}
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Que contiennent les variables {\tt X} et {\tt S} après
    l'exécution des quatre premières instructions ?
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item
        % La variable {\tt X} est définie par la commande :
        % \begin{scilabC}{1}
        %   & X = grand(n,2,\ttq{}bin\ttq{},2,0.5)
        % \end{scilabC}
        L'instruction {\tt X = grand(n,2,\ttq{}bin\ttq{},2,0.5)}
        permet de stocker dans la variable {\tt X} une matrice
        à {\tt n} lignes
        et $2$ colonnes où chaque colonne contient une observation
        d'un {\tt n}-échantillon de loi $\Bin{2}{\dfrac{1}{2}}$,
        c'est-à-dire un {\tt n}-échantillon de la \var $X$. Plus précisément,
        la première colonne de {\tt X} contient une observation $(c_1,
        \ldots, c_n)$ d'un {\tt n}-échantillon $(C_1, \ldots, C_n)$ de
        $X$, et la deuxième colonne de {\tt X} contient une
        observation $(c_1', \ldots, c_n')$ d'un {\tt n}-échantillon
        $(c_1', \ldots, C_n')$ de {\tt X}.\\
        {\it (les \var $C_i$ et $C_i'$ sont indépendantes et ont même
          loi que la \var $X$)}
        

        \newpage
        
        
      \item
        % La variable {\tt B} est définie par la commande :
        % \begin{scilabC}{2}
        %   & B = grand(n,2,\ttq{}bin\ttq{},1,0.5)
        % \end{scilabC}
        L'instruction {\tt B = grand(n,2,\ttq{}bin\ttq{},1,0.5)}
        permet de stocker dans la variable {\tt B} une matrice à {\tt n}
        lignes et $2$ colonnes où la première colonne contient une
        observation $(b_1, \ldots, b_n)$ d'un {\tt n}-échantillon
        $(B_1, \ldots, B_n)$ de loi $\Bern{\dfrac{1}{2}}$, et la
        deuxième colonne contient une observation $(b_1', \ldots,
        b_n')$ d'un {\tt n}-échantillon $(B_1', \ldots, B_n')$ de loi
        $\Bern{\dfrac{1}{2}}$.\\
        {\it (les \var $B_i$ et $B_i'$ sont indépendantes et ont même
          loi $\Bern{\dfrac{1}{2}}$)}
        
      \item On commence par rappeler que l'instruction {\tt ones(n,2)}
        permet d'obtenir une matrice à {\tt n} lignes et $2$ colonnes
        dont tous les coefficients sont égaux à $1$.\\
        On en déduit que l'instruction {\tt S = 2 \Sfois{} B -
          ones(n,2)} permet de stocker dans la variable {\tt S} une
        matrice à {\tt n} lignes et $2$ colonnes.\\
        La première colonne contient l'observation
        $(s_1, \ldots, s_n) = (2 \, b_1 -1, \ldots, 2 \, b_n -1)$ du
        {\tt n}-échantillon $(2 \, B_1 - 1, \ldots, 2 \, B_n -1)$.\\
        La deuxième colonne contient l'observation $(s_1', \ldots,
        s_n') = (2 \, b_1' -1, \ldots, 2 \, b_n' -1)$ du {\tt
          n}-échantillon $(2 \, B_1' -1, \ldots, 2 \, B_n' -1)$.
        
      \item Les \var $2 \, B_i - 1$ et $2 \, B_i' -1$ sont
        indépendantes et de même loi. Déterminons cette loi : on note
        $B$ une \var de loi $\Bern{\dfrac{1}{2}}$ et on cherche la loi
        de $V = 2 \, B -1$.
        \begin{noliste}{$\stimes$}
        \item Tout d'abord, comme $B \suit \Bern{\dfrac{1}{2}}$, on a :
          $B(\Omega) = \{0,1\}$.\\
          On en déduit : $V(\Omega) = \{2 \times 0 -1, 2 \times 1 -
          1\} = \{-1,1\}$.
          
        \item De plus :
          \[
            \Ev{V = 1} \ = \ \Ev{2 \, B -1 = 1} \ = \ \Ev{2 \, B =
              2} \ = \ \Ev{B = 1}
          \]
          D'où : $\Prob(\Ev{V = 1}) = \Prob(\Ev{B = 1}) =
          \dfrac{1}{2}$
            
        \item Enfin, comme la famille $(\Ev{V = -1}, \Ev{V = 1})$
          est un système complet d'événements :
          \[
            \Prob(\Ev{V = -1}) \ = \ 1 - \Prob(\Ev{V = 1}) \ = \ 1 -
            \dfrac{1}{2} \ = \ \dfrac{1}{2}
          \]
        \end{noliste}
        On en déduit que la \var $V = 2 \, B -1$ suit la même loi que
        la \var $S$.
        
      \item La variable {\tt S} contient donc une
        matrice à {\tt n} lignes et $2$ colonnes, où
        la première colonne contient une observation
        $(s_1, \ldots, s_n)$ d'un
        {\tt n}-échantillon $(S_1, \ldots, S_n)$ de la \var $S$, et la
        deuxième colonne contient une observation $(s_1', \ldots,
        s_n')$ du {\tt n}-échantillon $(S_1', \ldots, S_n')$ de la
        \var $S$.\\
        {\it (les \var $S_i$ et $S_i'$ sont indépendantes et ont même
          loi que la \var $S$)}
      \end{noliste}
      \begin{remark}
        \begin{noliste}{$\sbullet$}
        \item Comme souvent dans les sujets HEC, l'une des difficultés
          provient de la prise d'initiatives nécessaires pour répondre à
          une question.
          
        \item Dans cette question par exemple, déterminer la loi de la
          \var $V = 2 \, B-1$ n'est pas difficile. C'est prendre
          l'initiative de déterminer cette loi qui constitue la difficulté.
        \end{noliste}
      \end{remark}~\\[-1.4cm]
    \end{proof}


    \newpage
    
    
  \item Expliquer pourquoi, après l'exécution des six instructions,
    chacun des coefficients des matrices {\tt Z1} et {\tt Z2} contient
    une simulation de la variable aléatoire $Y_2$.
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item L'instruction {\tt  Z1 = [S(1:n,1) .\Sfois{} X(1:n,1) ,
          X(1:n,1) - S(1:n,1) - ones(n,1)]} permet de stocker dans la
        variable {\tt Z1} une matrice à {\tt n} lignes et $2$
        colonnes.\\
        Le contenu de la première colonne est donné par la commande
        {\tt S(1:n,1) .\Sfois{} X(1:n,1)} et celui de la deuxième
        colonne par {\tt  X(1:n,1) - S(1:n,1) - ones(n,1)}.

      \item L'instruction {\tt S(1:n,1) .\Sfois{} X(1:n,1)}
        permet d'obtenir une matrice colonne à {\tt n} lignes
        contenant l'observation $(d_1, \ldots, d_n) =
        (s_1 \times c_1, \ldots , s_n \times c_n)$
        du {\tt n}-échantillon ($S_1 \, C_1, \ldots, S_n \ C_n)$.\\
        Or, les $S_i$ suivent la même loi que $S$ et les $C_i$ suivent
        la même loi que $X_2$. Ainsi, les $S_i \, C_i$ suivent la même
        loi que $Y_2$.\\
        La première colonne de {\tt Z1} contient donc une observation $(d_1,
        \ldots, d_n)$ d'un {\tt n}-échantillon $(D_1, \ldots, D_n)$ de
        $Y_2$.\\
        {\it (les \var $D_i$ sont indépendantes et ont même
          loi que la \var $Y_2$)}


        % \newpage


        
      \item L'instruction {\tt X(1:n,1) - S(1:n,1) - ones(n,1)}
        permet d'obtenir une matrice colonne à {\tt n} lignes
        contenant l'observation $(d_1', \ldots, d_n') =
        (c_1 - s_1 - 1, \ldots , c_n - s_n -1)$
        du {\tt n}-échantillon ($C_1 - S_1-1, \ldots, C_n- S_n -1)$.\\
        Or, les $S_i$ suivent la même loi que $S$ et les $C_i$ suivent
        la même loi que $X_2$. Ainsi, les $C_i-S_i-1$ suivent la même
        loi que $X_2 - S-1$. De plus, d'après la question \itbf{2.b)},
        la \var $X_2-S-1$ suit la même loi que $Y_2$.\\ 
        La deuxième colonne de {\tt Z1} contient donc une observation $(d_1',
        \ldots, d_n')$ d'un {\tt n}-échantillon $(D_1', \ldots, D_n')$ de
        $Y_2$.\\
        {\it (les \var $D_i'$ sont indépendantes et ont même
          loi que la \var $Y_2$)}
        
      \item De même, l'instruction {\tt  Z2 = [S(1:n,1) .\Sfois{} X(1:n,1) ,
          X(1:n,2) - S(1:n,2) - ones(n,1)]} permet de stocker dans la
        variable {\tt Z2} une matrice à {\tt n} lignes et $2$
        colonnes.\\
        Le contenu de la première colonne est donné par la commande
        {\tt S(1:n,1) .\Sfois{} X(1:n,1)} et celui de la deuxième
        colonne par {\tt  X(1:n,2) - S(1:n,2) - ones(n,1)}.
        
      \item La première colonne de {\tt Z2} est identique à celle de
        {\tt Z1}, donc la première colonne de {\tt Z2} contient
        l'observation $(d_1, \ldots, d_n)$ du {\tt n}-échantillon
        $(D_1, \ldots, D_n)$ de $Y_2$.
        
      \item L'instruction {\tt X(1:n,2) - S(1:n,2) - ones(n,1)}
        permet d'obtenir une matrice colonne à {\tt n} lignes
        contenant l'observation $(d_1'', \ldots, d_n'') =
        (c_1' - s_1' - 1, \ldots , c_n' - s_n' -1)$
        du {\tt n}-échantillon ($C_1' - S_1'-1, \ldots, C_n'- S_n' -1)$.\\
        Or, les $S_i'$ suivent la même loi que $S$ et les $C_i'$ suivent
        la même loi que $X_2$. Ainsi, les $C_i'-S_i'-1$ suivent la même
        loi que $X_2 - S-1$, donc que $Y_2$.\\ 
        La deuxième colonne de {\tt Z2} contient donc une observation $(d_1'',
        \ldots, d_n'')$ d'un {\tt n}-échantillon $(D_1'', \ldots, D_n'')$ de
        $Y_2$.\\
        {\it (les \var $D_i''$ sont indépendantes et ont même
          loi que la \var $Y_2$)}
      \end{noliste}
      \conc{Finalement, chacun des coefficients des matrices {\tt Z1}
        et {\tt Z2} contient une simulation de la \var $Y_2$.}
      \begin{remarkL}{.95}
        On aurait pu utiliser davantage les commandes Scilab{}. En
        effet, l'appel classique permettant d'extraire la première
        colonne de la matrice {\tt S} est plutôt {\tt S(:,1)}
        (que {\tt S(1:n,1)}).
      \end{remarkL}~\\[-1.4cm]
    \end{proof}


    \newpage
    
    
  \item On modifie la première ligne du script précédent en affectant
    à {\tt n} une valeur beaucoup plus grande que $10$ (par exemple,
    $100000$) et en lui adjoignant les deux instructions \ligne{7} et
    \ligne{8} suivantes :
    \begin{scilabC}{6}
      & p1 = length(find(Z1(1:n,1) == Z1(1:n,2))) / n \nl %
      & p2 = length(find(Z2(1:n,1) == Z2(1:n,2))) / n
    \end{scilabC}
    Quelles valeurs numériques approchées la loi faible des grands
    nombres permet-elle de fournir pour {\tt p1} et {\tt p2} après
    l'exécution des huit lignes du nouveau script ?


    % \newpage


    \noindent
    Dans le langage \Scilab{}, la fonction {\tt length} fournit la \og
    longueur \fg{} d'un vecteur ou d'une matrice et la fonction {\tt
      find} calcule les positions des coefficients d'une matrice pour
    lesquels une propriété est vraie, comme l'illustre le script
    suivant :
    \[
      \begin{console}
        \lInv{A = [1 ; 2 ; 0 ; 4]} \nl %
        \lInv{B = [2 ; 2 ; 4 ; 3]} \nl %
        \lInv{length(A)} \nl %
        \lDisp{\qquad ans = 4.} \nl %
        \lInv{length([A , B])} \nl %
        \lDisp{\qquad ans = 8.} \nl %
        \lInv{find(A < B)} \nl %
        \lDisp{\qquad ans = 1. 3. // \textit{car $1<2$ et $0<4$, alors
            que $2 \geq 2$ et $4 \geq 3$}}
      \end{console}
    \]


    % \newpage

    
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Commençons par commenter l'instruction :
        \begin{scilabC}{6}
          & p1 = length(find(Z1(1:n,1) == Z1(1:n,2))) / n
        \end{scilabC}
        \begin{noliste}{$\stimes$}
        \item L'instruction {\tt find(Z1(1:n,1) == Z1(1:n,2))} permet
          d'obtenir une matrice ligne contenant les positions des
          coefficients des matrices {\tt Z1(1:n,1)} et {\tt Z1(1:n,2)}
          égaux. Autrement dit, on obtient une matrice ligne contenant
          les indices $i$ tels que $d_i = d_i'$.\\
          {\it (on rappelle que $(d_1, \ldots, d_n)$ est une
            observation d'un {\tt n}-échantillon $(D_1, \ldots, D_n)$
            de $S \, X_2$ et $(d_1', \ldots, d_n')$ est une
            observation d'un {\tt n}-échantillon $(D_1', \ldots, D_n')$
            de $X_2 - S-1$)}
          
        \item L'instruction {\tt length(find(Z1(1:n,1) == Z1(1:n,2)))}
          permet d'obtenir la longueur de la matrice
          précédente. Ainsi, on obtient le nombre de fois où $d_i =
          d_i'$, pour $i \in \llb 1,{\tt n} \rrb$.
          
        \item Enfin, on divise ce nombre par la taille {\tt n} de
          l'observation.\\
          Or, par loi faible des grands nombres (LfGN) :
          \[
            \dfrac{\text{nombre de fois où $d_i = d_i'$}}{\text{taille
                de l'observation}} \ \simeq \ \Prob(\Ev{S \, X_2 = X_2
              - S-1})
          \]
        \end{noliste}
        \conc{La variable {\tt p1} contient une valeur approchée de
          $\Prob(\Ev{S \, X_2 = X_2 -S-1})$.}
        
      \item Commentons ensuite l'instruction :
        \begin{scilabC}{7}
          & p2 = length(find(Z2(1:n,1) == Z2(1:n,2))) / n
        \end{scilabC}
        \begin{noliste}{$\stimes$}
        \item L'instruction {\tt find(Z2(1:n,1) == Z2(1:n,2))} permet
          d'obtenir une matrice ligne contenant les positions des
          coefficients des matrices {\tt Z1(2:n,1)} et {\tt Z1(2:n,2)}
          égaux. Autrement dit, on obtient une matrice ligne contenant
          les indices $i$ tels que $d_i = d_i''$.\\
          {\it (on rappelle que $(d_1, \ldots, d_n)$ est une
            observation d'un {\tt n}-échantillon $(D_1, \ldots, D_n)$
            de $S \, X_2$ et $(d_1'', \ldots, d_n'')$ est une
            observation d'un {\tt n}-échantillon $(D_1'', \ldots, D_n'')$
            de $X_2' - S-1$, où la \var $X_2'$ suit la même loi que $X_2$)}
          
        \item L'instruction {\tt length(find(Z2(1:n,1) == Z2(1:n,2)))}
          permet d'obtenir la longueur de la matrice
          précédente. Ainsi, on obtient le nombre de fois où $d_i =
          d_i''$, pour $i \in \llb 1,{\tt n} \rrb$.

          
          \newpage
          
          
        \item Enfin, on divise ce nombre par la taille {\tt n} de
          l'observation.\\
          Or, par loi faible des grands nombres (LfGN) :
          \[
            \dfrac{\text{nombre de fois où $d_i = d_i''$}}{\text{taille
                de l'observation}} \ \simeq \ \Prob(\Ev{S \, X_2 = X_2'
              - S-1})
          \]
        \end{noliste}
        \conc{La variable {\tt p2} contient une valeur approchée de
          $\Prob(\Ev{S \, X_2 = X_2' -S-1})$\\
          où la \var $X_2'$ suit la même loi que $X_2$.}
      \end{noliste}
      \begin{remarkL}{.95}
        \begin{noliste}{$\sbullet$}
        \item Le programme proposé par l'énoncé n'est ici rien d'autre
          qu'une illustration de l'idée naturelle pour obtenir une
          approximation de $\Prob(\Ev{S \, X_2 = X_2 - S-1})$ :
          \begin{noliste}{$\stimes$}
          \item simuler un grand nombre de fois ({\tt n} $= 100000$
            est ce grand nombre) les \var $S \, X_2$ et $X_2 - S-1$.\\
            Formellement, on souhaite obtenir une observation $(d_1,
            \ldots, d_n)$ d'un {\tt n}-échantillon
            $(D_1, \ldots, D_n)$ de la \var $S \, X_2$, et une
            observation $(d_1', \ldots, d_n')$ d'un {\tt
              n}-échantillon $(D_1', \ldots, D_n')$ de la \var $X_2 -S-1$. 
            
          \item de compter le nombre de fois où $d_i = d_i'$, pour $i
            \in \llb 1, {\tt n} \rrb$.
          \end{noliste}
          
        \item L'objectif de cette question \Scilab{} est de revenir sur
          un point crucial en probabilités :
          \[
            X = Y \quad \Rightarrow \quad \text{$X$ et $Y$ ont
            même loi}
          \]
          Mais :
          \[
            \text{$X$ et $Y$ ont même loi} \quad \bcancel{\Rightarrow}
            \quad X=Y
          \]
          Ainsi, ici :
          \begin{noliste}{$\stimes$}
          \item les \var $S \, X_2$ et $X_2 -S-1$ ont même loi (la même
            que $Y_2$), mais : $\Prob(\Ev{S \, X_2 = X_2 -S-1}) \neq 1$,
            
          \item les \var $X_2 -S-1$ et $X_2'-S-1$ ont même loi
            (toujours la même que $Y_2$), mais : $\Prob(\Ev{S \, X_2 =
              X_2' -S-1}) \neq \Prob(\Ev{S \, X_2 = X_2-S-1})$.
          \end{noliste}
          
        \item On peut avoir la confirmation du point précédent en
          exécutant le programme \Scilab{}. On obtient :
          \[
            \begin{console}
              \lDisp{\qquad p1 =} \nl %
              \lDisp{\qquad \qquad 0.1261} \nl %
              \lDisp{\qquad p2 =} \nl %
              \lDisp{\qquad \qquad 0.2179}
            \end{console}
          \]
          On constate qu'on a bien ${\tt p1} \neq 1$, ${\tt p2} \neq
          1$ et ${\tt p1} \neq {\tt p2}$. 
        \end{noliste}
      \end{remarkL}~\\[-1.4cm]
    \end{proof}
  \end{noliste}
  
\item Dans cette question, on note $X_n$ une variable aléatoire qui
  suit la loi binomiale $\Bin{n}{\dfrac{1}{2}}$.\\
  On suppose que les variables aléatoires $X_n$ et $S$ sont
  indépendantes et on pose $Y_n = S \, X_n$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier que la fonction $M_{X_n}$ est définie sur $\R$ et
    calculer $M_{X_n}(t)$ pour tout $t\in \R$.
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item  Soit $t \in \R$.\\
        La \var $X_n$ est une \var finie. Ainsi, la \var $\ee^{t \,
          X_n}$ est également une \var finie. Elle admet donc des
        moments à tout ordre, en particulier une espérance.
        \conc{On en déduit que la fonction $M_{X_n}$ défini sur $\R$.}


        \newpage
        
        
      \item Soit $t \in \R$.
        \[
          \begin{array}{rcl@{\quad}>{\it}R{5cm}}
            M_{X_n}(t)
            & = & \E\left(\ee^{t \, X_n}\right)
            \\[.2cm]
            & = & \Sum{k=0}{n} \ee^{k \, t} \ \Prob(\Ev{X_n = k})
            & (par théorème de transfert ($X_n(\Omega) = \llb 0,n \rrb$))
            \nl
            \nl[-.2cm]
            & = & \Sum{k=0}{n} \ee^{k \, t} \dbinom{n}{k}
                  \left(\dfrac{1}{2}\right)^k
                  \left(\dfrac{1}{2}\right)^{n-k}
            & (car $X_n \suit \Bin{n}{\dfrac{1}{2}}$)
            \nl
            \nl[-.2cm]
            & = & \Sum{k=0}{n} \dbinom{n}{k} \ee^{k \, t}
                  \left(\dfrac{1}{2}\right)^n
            \\[.6cm]
            & = & \dfrac{1}{2^n} \ \Sum{k=0}{n} \dbinom{n}{k} \left(
                  \ee^t \right)^k \ 1^{n-k}
            \\[.6cm]
            & = & \dfrac{1}{2^n} \left(\ee^t +1\right)^n
            & (d'après la formule du binôme de Newton)
          \end{array}
        \]
        \conc{$\forall t \in \R$, $M_{X_n}(t) = \dfrac{1}{2^n}
          \left(1+ \ee^t\right)^n$}~\\[-1.4cm]
      \end{noliste}
    \end{proof}
    
  \item Montrer que la fonction $M_{Y_n}$ est donnée par : $\forall t
    \in \R$, $M_{Y_n}(t) = \dfrac{1}{2^{n+1}} \ \big((1+\ee^t)^n + (1+
    \ee^{-t})^n\big)$.
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Soit $t \in \R$.\\
        La \var $Y_n$ est une \var finie (car les \var $X_n$ et
        $S$ le sont). Ainsi, la \var $\ee^{t \,
          Y_n}$ est également une \var finie. Elle admet donc des
        moments à tout ordre, en particulier une espérance.
        \conc{On en déduit que la fonction $M_{Y_n}$ défini sur $\R$.}
        
      \item Déterminons la loi de $Y_n$.
        \begin{noliste}{$\stimes$}
        \item Tout d'abord, comme $X_n(\Omega) = \llb 0,n \rrb$ et
          $S(\Omega) = \{-1,1\}$, on a : $Y_n(\Omega) \subset \llb -n,
          n \rrb$.
          
        \item Soit $k \in \llb -n, n \rrb$.\\
          La famille $(\Ev{S=-1}, \Ev{S=1})$ forme un système complet
          d'événements.\\
          Ainsi, par formule des probabilités totales :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{4cm}}
              \Prob(\Ev{Y_n = k})
              & = & \Prob(\Ev{S = -1} \cap \Ev{Y_n = k}) + \Prob(\Ev{S
                    = 1} \cap \Ev{Y_n = k})
              \\[.4cm]
              & = & \Prob(\Ev{S = -1} \cap \Ev{S \, X_n = k}) +
                    \Prob(\Ev{S = 1} \cap \Ev{S \, X_n = k})
              \\[.4cm]
              & = & \Prob(\Ev{S = -1} \cap \Ev{-X_n = k}) +
                    \Prob(\Ev{S = 1} \cap \Ev{X_n = k})
              \\[.4cm]
              & = & \Prob(\Ev{S = -1}) \ \Prob(\Ev{X_n = -k}) +
                    \Prob(\Ev{S = 1}) \ \Prob(\Ev{X_n = k})
              & (car les \var $S$ et $X_n$ sont indépendantes)
              \nl
              \nl[-.2cm]
              & = & \dfrac{1}{2} \ \Prob(\Ev{X_n = -k}) + \dfrac{1}{2}
                    \ \Prob(\Ev{X_n = k})
            \end{array}
          \]
          Trois cas se présentent alors :
        \end{noliste}
        \begin{liste}{-}
        \item \dashuline{si $k \in \llb -n, 0 \llb$}, alors $\Ev{X_n = k} =
          \emptyset$. Donc :
          \[
            \Prob(\Ev{Y_n = k}) \ = \ \dfrac{1}{2} \ \Prob(\Ev{X_n =
              -k}) + \bcancel{\dfrac{1}{2} \ \Prob(\emptyset)} \ = \
            \dfrac{1}{2} \ \Prob(\Ev{X_n = -k})
          \]


          \newpage
          
          
        \item \dashuline{si $k=0$}, alors :
          \[
            \begin{array}{rcl}
              \Prob(\Ev{Y_n = 0})
              & = & \dfrac{1}{2} \ \Prob(\Ev{X_n = 0}) + \dfrac{1}{2}
                    \ \Prob(\Ev{X_n = 0})
              \\[.6cm]
              & = & \Prob(\Ev{X_n = 0})
              \\[.4cm]
              & = & \dbinom{n}{0} \left(\dfrac{1}{2}\right)^0
                    \left(\dfrac{1}{2}\right)^{n-0}
              \\[.6cm]
              & = & \dfrac{1}{2^n}
            \end{array}
          \]
          
        \item \dashuline{si $k \in \rrb 0, n \rrb$}, alors $\Ev{X_n = -k} =
          \emptyset$. Donc :
          \[
            \Prob(\Ev{Y_n = k}) \ = \ \bcancel{\dfrac{1}{2} \ \Prob(\emptyset)}
            + \dfrac{1}{2} \ \Prob(\Ev{X_n = k}) \ = \
            \dfrac{1}{2} \ \Prob(\Ev{X_n = k})
          \]
        \end{liste}
        
      \item Soit $t \in \R$.
        \[
          \begin{array}{rcl@{\quad}>{\it}R{4cm}}
            M_{Y_n}(t)
            & = & \E\left(\ee^{t \, Y_n}\right)
            \\[.2cm]
            & = & \Sum{k=-n}{n} \ee^{k \, t} \ \Prob(\Ev{Y_n = k})
            \\[.6cm]
            & = & \Sum{k=-n}{-1} \left(\ee^{k \, t} \ \Prob(\Ev{Y_n =
                  k})\right) + \ee^{0 \, t} \ \Prob(\Ev{Y_n = 0}) +
                  \Sum{k=1}{n} \left(\ee^{k \, t} \ \Prob(\Ev{Y_n =
                  k})\right)
            \\[.6cm]
            & = & \dfrac{1}{2} \ \Sum{k=-n}{-1} \left(\ee^{k \, t} \
                  \Prob(\Ev{X_n = -k})\right) + \dfrac{1}{2^n} +
                  \dfrac{1}{2} \ \Sum{k=1}{n} \left(\ee^{k \, t} \
                  \Prob(\Ev{X_n = k})\right)
          \end{array}
        \]
        \begin{noliste}{$\stimes$}
        \item Tout d'abord :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{4cm}}
              \Sum{k=-n}{-1} \ee^{k \, t} \ \Prob(\Ev{X_n = -k})
              & = & \Sum{j=1}{n} \ee^{-j \, t} \ \Prob(\Ev{X_n = j})
              & (par changement d'indice $j=-k$)
              \nl
              \nl[-.2cm]
              & = &\Sum{j=1}{n} \ee^{-j \, t} \ \dbinom{n}{j}
                    \left(\dfrac{1}{2}\right)^j
                    \left(\dfrac{1}{2}\right)^{n-j}
              \\[.6cm]
              & = & \dfrac{1}{2^n} \ \Sum{j=1}{n} \dbinom{n}{j} \
                    \left(\ee^{-t}\right)^j \ 1^{n-j}
              \\[.6cm]
              & = & \dfrac{1}{2^n} \ \left(\Sum{j=0}{n} \left(\dbinom{n}{j} \
                    \left(\ee^{-t}\right)^j \ 1^{n-j}\right) -1\right)
              \\[.6cm]
              & = & \dfrac{1}{2^n} \left((\ee^{-t}+1)^n -1\right)
              & (d'après la formule du binôme de Newton)
            \end{array}
          \]

        \item De même :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{4cm}}
              \Sum{k=1}{n} \ee^{k \, t} \ \Prob(\Ev{X_n = k})
              & = &\Sum{k=1}{n} \ee^{k \, t} \ \dbinom{n}{k}
                    \left(\dfrac{1}{2}\right)^k
                    \left(\dfrac{1}{2}\right)^{n-k}
              \\[.6cm]
              & = & \dfrac{1}{2^n} \ \Sum{k=1}{n} \dbinom{n}{k} \
                    \left(\ee^{t}\right)^k \ 1^{n-k}
              \\[.6cm]
              & = & \dfrac{1}{2^n} \ \left(\Sum{k=0}{n} \left(\dbinom{n}{k} \
                    \left(\ee^{t}\right)^k \ 1^{n-k}\right) -1\right)
              \\[.6cm]
              & = & \dfrac{1}{2^n} \left((\ee^{t}+1)^n -1\right)
              & (d'après la formule du binôme de Newton)
            \end{array}
          \]
        \end{noliste}


        \newpage
        
        
      \item On obtient alors :
        \[
          \begin{array}{rcl@{\quad}>{\it}R{4cm}}
            M_{Y_n}(t)
            & = & \dfrac{1}{2} \times \dfrac{1}{2^n}
                  \left((1+\ee^{-t})^n -1\right) + \dfrac{1}{2^n} +
                  \dfrac{1}{2} \times \dfrac{1}{2^n} \left((1+
                  \ee^t)^n -1\right)
            \\[.6cm]
            & = & \dfrac{1}{2^{n+1}} (1+ \ee^{-t})^n -
                  \dfrac{1}{2^{n+1}} + \dfrac{1}{2^n} +
                  \dfrac{1}{2^{n+1}} (1+ \ee^t)^n - \dfrac{1}{2^{n+1}}
            \\[.6cm]
            & = & \dfrac{1}{2^{n+1}} \left((1+ \ee^{-t})^n + (1+
                  \ee^t)^n \right) - \dfrac{\bcancel{2}}{2^{n +
                  \bcancel{1}}} + \dfrac{1}{2^n}
            \\[.6cm]
            & = & \dfrac{1}{2^{n+1}} \left((1+ \ee^{-t})^n + (1+
                  \ee^t)^n \right) - \bcancel{\dfrac{1}{2^n}} +
                  \bcancel{\dfrac{1}{2^n}}
          \end{array}
        \]
        \conc{$\forall t \in \R$, $M_{Y_n}(t) = \dfrac{1}{2^{n+1}} \
          \left((1+ \ee^t)^n + (1+ \ee^{-t})^n\right)$}~\\[-1.4cm]
      \end{noliste}
    \end{proof}
    
  \item En utilisant l'égalité $(1+\ee^{-t})^n = \ee^{-nt} \,
    (1+\ee^t)^n$, montrer que $Y_n$ suit la même loi que la différence
    $X_n - H_n$, où $H_n$ est une variable aléatoire indépendante de
    $X_n$ dont on précisera la loi.
    \begin{proof}~
      \begin{noliste}{$\sbullet$}
      \item Comme $Y_n(\Omega) \subset \llb -n,n \rrb$, on souhaite
        appliquer la question \itbf{1.c)}, \ie on souhaite trouver
        une \var $H_n$ indépendante de $X_n$ telle que :
        \[
          (X_n - H_n)(\Omega) \subset \llb -n,n \rrb \quad \text{et}
          \quad M_{Y_n} = M_{X_n - H_n}
        \]
        Avec la question \itbf{1.c)}, on pourra alors conclure que
        $Y_n$ et $X_n - H_n$ suivent la même loi.
        
      \item Supposons qu'il existe une telle \var $H_n$. Alors, soit
        $t \in \R$ :
        \begin{noliste}{$\stimes$}
        \item d'une part :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{7cm}}
              M_{X_n - H_n}(t)
              & = & \E\left( \ee^{t \, (X_n - H_n)}\right)
              \\[.4cm]
              & = & \E\left(\ee^{t \, X_n} \ \ee^{-t \, H_n}\right)
              \\[.4cm]
              & = & \E\left(\ee^{t \, X_n}\right) \ \E\left(\ee^{-t \,
                    H_n}\right)
              & (car, par lemme des coalitions, les \var $\ee^{t \,
                X_n}$ et $\ee^{-t \, H_n}$ sont indépendantes)
              \nl
              \nl[-.2cm]
              & = & M_{X_n}(t) \ \E\left(\ee^{-t \, H_n}\right)
              \\[.4cm]
              & = & \dfrac{1}{2^n} \ (1+\ee^t)^n \ \E\left(\ee^{-t \,
                    H_n}\right)
              & (d'après la question \itbf{4.a)})
            \end{array}
          \]
          
        \item d'autre part :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{4cm}}
              M_{Y_n}(t)
              & = & \dfrac{1}{2^{n+1}} \ \big((1+ \ee^t)^n + (1+
                    \ee^{-t})^n\big)
              & (d'après la question précédente)
              \nl
              \nl[-.2cm]
              & = & \dfrac{1}{2^{n+1}} \big((1+\ee^t)^n + \ee^{-nt}
                    (1+ \ee^t)^n\big)
              & (d'après l'indication de l'énoncé)
              \nl
              \nl[-.2cm]
              & = & \dfrac{1}{2^{n+1}} \ (1+ \ee^t)^n \left(1 +
                    \ee^{-nt}\right)
              \\[.6cm]
              & = & \dfrac{1}{2^n} \ (1+ \ee^t)^n \times \dfrac{1}{2}
                    \ \left(1+ \ee^{-t}\right)^n
            \end{array}
          \]
        \end{noliste}
        On en déduit :
        \[
          \E\left(\ee^{-t \, H_n}\right) \ = \ \dfrac{1}{2} \left(1+
            \ee^{-nt}\right) \ = \ \ee^{0 \, t} \ \dfrac{1}{2} +
          \ee^{-nt} \ \dfrac{1}{2}
        \]


        \newpage
        
        
      \item On considère alors une \var $H_n$, indépendante de $X_n$
        telle que :
        \begin{noliste}{$\stimes$}
        \item $H_n(\Omega) = \{0,n\}$,
          
        \item $\Prob(\Ev{H_n = 0}) = \Prob(\Ev{H_n = n}) = \dfrac{1}{2}$.
        \end{noliste}
        
      \item Vérifions qu'avec cette \var $H_n$, nous sommes dans
        le cadre d'application de la question \itbf{1.c)}.
        \begin{noliste}{$\stimes$}
        \item Tout d'abord, comme $X_n(\Omega) = \llb 0,n \rrb$ et
          $H_n(\Omega) = \{0,n \}$, alors : $(X_n - H_n)(\Omega)
          \subset \llb -n,n \rrb$.
          
        \item Ensuite, la \var $\ee^{-t \, H_n}$ admet une espérance
          en tant que \var finie (car $H_n$ est une \var finie).
          Ainsi, par théorème de transfert, soit $t \in \R$ :
          \[
            \begin{array}{rcl}
              \E\left( \ee^{-t \, H_n}\right)
              & = & \ee^{0 \, t} \ \Prob(\Ev{H_n = 0}) + \ee^{-nt} \
                    \Prob(\Ev{H_n = n})
              \\[.4cm]
              & = & \dfrac{1}{2} + \ee^{-nt} \ \dfrac{1}{2}
              \\[.6cm]
              & = & \dfrac{1}{2} \left(1+ \ee^{-nt}\right)
            \end{array}
          \]
          Ainsi :
          \[
            \begin{array}{rcl@{\quad}>{\it}R{5cm}}
              M_{Y_n}(t)
              & = & \dfrac{1}{2^n} \ (1+ \ee^t)^n \times \dfrac{1}{2} \
                    (1+ \ee^{-t})^n
              \\[.6cm]
              & = & M_{X_n}(t) \times \E(\ee^{-t \, H_n})
              \\[.4cm]
              & = & \E(\ee^{t \, X_n}) \ \E(\ee^{-t \, H_n})
              \\[.4cm]
              & = & \E(\ee^{t \, X_n} \ \ee^{-t \, H_n})
              & (car les \var $\ee^{t \, X_n}$ et $\ee^{-t \, H_n}$
                sont indépendantes)
              \nl
              \nl[-.2cm]
              & = & M_{X_n-H_n}(t)
            \end{array}
          \]
          D'où : $M_{Y_n} = M_{X_n - H_n}$
        \end{noliste}
        \conc{D'après \itbf{1.c)}, on en déduit que les \var $Y_n$ et
          $X_n - H_n$ suivent la même loi.}
      \end{noliste}

      \begin{remark}
        Démontrons l'égalité : $(1+ \ee^{-t})^n = \ee^{-n} (1+
        \ee^t)^n$.
        \[
          \begin{array}{rcl@{\quad}>{\it}R{4cm}}
            \ee^{-n \, t} \ (1+ \ee^t)^n
            & = & \left(\ee^{-t}\right)^n \ (1+\ee^t)^n
            \\[.2cm]
            & = & \big(\ee^{-t}(1+\ee^t)\big)^n
            \\[.2cm]
            & = & (\ee^{-t} + 1)^n
          \end{array}
        \]
      \end{remark}~\\[-1.4cm]
    \end{proof}
  \end{noliste}
\end{noliste}


\subsection*{Partie II. Propriétés générales des fonctions
  génératrices des cumulants et quelques exemples}

\begin{noliste}{1.}
  \setlength{\itemsep}{4mm}
  \setcounter{enumi}{4}
\item Soit $X$ une variable aléatoire et ${\cal D}_X$ le domaine de
  définition de la fonction $K_X$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Donner la valeur de $K_X(0)$.
    
  \item Soit $(a,b) \in \R^2$ et $Y= a \, X +b$. Justifier pour tout
    réel $t$ pour lequel $a \, t$ appartient à ${\cal D}_X$, l'égalité
    :
    \[
      K_Y(t) \ = \ b \, t + K_X(a \, t)
    \]
    
  \item On suppose ici que les variables aléatoires $X$ et $-X$
    suivent la même loi.\\
    Que peut-on dire dans ce cas des cumulants d'ordre impair de la
    variables aléatoire $X$ ?
  \end{noliste}
  
\item Soit $X$ et $Y$ deux variables aléatoires indépendantes et
  ${\cal D}_X$ et ${\cal D}_Y$ les domaines de définition respectifs
  des fonctions $K_X$ et $K_Y$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Monter que pour tout réel $t$ appartenant à la fois à ${\cal
      D}_X$ et ${\cal D}_Y$, on a : $K_{X+Y}(t) = K_X(t) + K_Y(t)$.
    
  \item En déduire une relation entre les cumulants des variables
    aléatoires $X$, $Y$ et $X+Y$.
  \end{noliste}
  
\item Soit $U$ une variable aléatoire suivant la loi uniforme sur
  l'intervalle $[0,1]$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Montrer que la fonction $M_U$ est définie sur $\R$ et donnée
    par : $\forall t \in \R$, $M_U(t) = \left\{
      \begin{array}{cR{1.5cm}}
        \dfrac{\ee^t - 1}{t} & si $t \neq 0$
        \nl
        \nl[-.2cm]
        1 & si $t=0$
      \end{array}
    \right.$.
    
  \item Calculer la dérivée de la fonction $M_U$ en tout point $t \neq
    0$.
    
  \item Trouver la limite du quotient $\dfrac{M_U(t) -1}{t}$ lorsque
    $t$ tend vers $0$.
    
  \item Montrer que la fonction $M_U$ est de classe $\Cont{1}$ sur $\R$.
  \end{noliste}


  % \newpage
  
  
\item Soit $\alpha$ et $\beta$ deux réels tels que $\alpha < \beta$.\\
  Dans cette question, on note $X$ une variable aléatoire qui suit la
  loi uniforme sur l'intervalle $[\alpha, \beta]$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Exprimer $K_X$ en fonction de $M_U$, où la variable aléatoire
    $U$ a été définie dans la question \itbf{7.}
    
  \item Justifier que la fonction $K_X$ est de classe $\Cont{1}$ sur
    $\R$ et établir l'égalité : $Q_1(X) = \E(X)$.
  \end{noliste}
  
\item Soit un réel $\lambda >0$ et soit $T$ une variable aléatoire qui
  suit la loi de Poisson de paramètre $\lambda$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Déterminer les fonctions $M_T$ et $K_T$.
    
  \item En déduire les cumulants de $T$.
  \end{noliste}
  
\item Soit $Z$ une variable aléatoire qui suit la loi normale centrée
  réduite.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier pour tout $t \in \R$, la convergence de l'intégrale
    $\dint{-\infty}{+\infty} \exp\left(t \, x - \dfrac{x^2}{2}\right)
    \dx$.
    
  \item Montrer que la fonction $M_Z$ est définie sur $\R$ et donnée
    par : $\forall t \in \R$, $M_Z(t) =
    \exp\left(\dfrac{t^2}{2}\right)$.
    
  \item EN déduire la valeur de tous les cumulants d'une variable
    aléatoire qui suit une loi normale d'espérance $\mu \in \R$ et
    d'écart-type $\sigma \in \R_+^*$.
  \end{noliste}
  
\item Soit $(T_n)_{n \in \N^*}$ une suite de variables aléatoires
  telles que, pour tout $n \in \N^*$, la variable aléatoire $T_n$ suit
  la loi de Poisson de paramètre $n$. Pour tout $n \in \N^*$, on pose
  : $W_n = \dfrac{T_n - n}{\sqrt{n}}$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Justifier la convergence en loi de la suite de variables
    aléatoires $(W_n)_{n \in \N^*}$ vers une variable aléatoire $W$.
    
  \item Déterminer la fonction $K_{W_n}$.
    
  \item Montrer que pour tout $t \in \R$, on a : $\dlim{n\to +\infty}
    K_{W_n}(t) = K_W(t)$.
  \end{noliste}
\end{noliste}


\subsection*{Partie III. Cumulant d'ordre 4}

\noindent
Dans cette partie, on considère une variable aléatoire $X$ telle que
$M_X$ est de classe $\Cont{4}$ sur un intervalle ouvert $I$ contenant
l'origine.\\
{\it On admet} alors que $X$ possède des moments jusqu'à l'ordre $4$
qui coïncident avec les dérivées successives de la fonction $M_X$ en
$0$. Autrement dit, pour tout $k \in \llb 1,4 \rrb$, on a :
$M_X^{(k)}(0) = \E(X^k)$.\\[.1cm]
De plus, on pose : $\mu_4(X) = \E\left( \big(X - \E(X)\big)^4\right)$.
\begin{noliste}{1.}
  \setlength{\itemsep}{4mm}
  \setcounter{enumi}{11}
\item Justifier les égalités : $Q_1(X) = \E(X)$ et $Q_2(X) = \V(X)$.
  
\item Soit $X_1$ et $X_2$ deux variables aléatoires indépendantes et
  de même loi que $X$. On pose : $S = X_1 - X_2$.
  \begin{noliste}{a)}
    \setlength{\itemsep}{2mm}
  \item Montrer que la variable aléatoire $S$ possède un moment
    d'ordre $4$ et établir l'égalité :
    \[
      \E(S^4) \ = \ 2 \, \mu_4(X) + 6 \, \big(\V(X)\big)^2
    \]
    
  \item Montrer que les fonctions $M_S$ et $K_S$ sont de classe
    $\Cont{4}$ sur $I$ et que pour tout $t \in I$, on a :
    \[
      M_S^{(4)}(t) \ = \ K_S^{(4)}(t) \, M_S(t) + 3 \, K_S^{(3)}(t) \,
      M_S'(t) + 3 \, K_S''(t) \, M_S''(t) + K_S'(t) \, M_S^{(3)}(t)
    \]
    
  \item En déduire l'égalité : $\E(S^4) \ = \ Q_4(S) + 3 \, \big(\V(S)\big)^2$.
  \end{noliste}
  
\item Justifier que le cumulant d'ordre $4$ de $X$ est donné par la
  relation : $Q_4(X) \ = \ \mu_4(X) - 3 \, \big(\V(X)\big)^2$.
\end{noliste}


\end{document}